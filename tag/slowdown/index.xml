<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Slowdown | Nubificus</title><link>/tag/slowdown/</link><atom:link href="/tag/slowdown/index.xml" rel="self" type="application/rss+xml"/><description>Slowdown</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Fri, 11 Jul 2025 08:02:51 +0100</lastBuildDate><image><url>/media/logo_hu_866fdf07312224c.png</url><title>Slowdown</title><link>/tag/slowdown/</link></image><item><title>When More Cores Means Less Speed: Debugging PyTorch with Valgrind on ARM</title><link>/blog/torch-arm-debug/</link><pubDate>Fri, 11 Jul 2025 08:02:51 +0100</pubDate><guid>/blog/torch-arm-debug/</guid><description>&lt;p&gt;If you’ve ever tried to debug a PyTorch program on an ARM64 system using
&lt;a href="https://valgrind.org/" target="_blank" rel="noopener"&gt;&lt;code&gt;Valgrind&lt;/code&gt;&lt;/a&gt;, you might have stumbled on something really
odd: &amp;ldquo;Why does it take so long?&amp;rdquo;. And if you&amp;rsquo;re like us, you would probably try
to run it locally, on a Raspberry pi, to see what&amp;rsquo;s going on&amp;hellip; And the madness
begins!&lt;/p&gt;
&lt;p&gt;TL;DR, as you probably figured out from the title of this post, it’s a
counter-intuitive experience: the more cores your machine has, the slower your
(torch) code seems to run under &lt;code&gt;Valgrind&lt;/code&gt;. Shouldn’t more cores mean more speed?
Let’s dive into why that’s not always the case ;)&lt;/p&gt;
&lt;h2 id="the-background"&gt;The background&lt;/h2&gt;
&lt;p&gt;In an effort to improve our testing infrastructure for
&lt;a href="https://github.com/nubificus/vaccel" target="_blank" rel="noopener"&gt;vAccel&lt;/a&gt; and make it more robust, we
started cleaning up our examples, unifying the build &amp;amp; test scripts and started
adding more elaborate test cases for both the library and the plugins.
&lt;code&gt;Valgrind&lt;/code&gt; provides a quite decent experience for this, especially to catch
multi-arch errors, memory leaks and dangling pointers (something quite common
when writing in C :D).&lt;/p&gt;
&lt;h2 id="the-issue"&gt;The issue&lt;/h2&gt;
&lt;p&gt;While adding the &lt;code&gt;Valgrind&lt;/code&gt; mode of execution in our tests for the vAccel
plugins, we noticed something really weird in the
&lt;a href="https://docs.vaccel.org/latest/plugins/available-plugins/acceleration-plugins/torch-plugin/" target="_blank" rel="noopener"&gt;Torch&lt;/a&gt;
case. The test was taking forever!&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-figure-1-build--test-run-on-amd64"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/images/torch_run_amd64.png" alt="Figure 1: Build &amp; Test run on amd64" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 1: Build &amp;amp; Test run on amd64
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Specifically, while the equivalent &lt;code&gt;amd64&lt;/code&gt; was taking roughly 4 and a half
minutes (Figure 1), the &lt;code&gt;arm64&lt;/code&gt; run was taking nearly an hour (53 minutes) &amp;ndash;
see Figure 2.&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-figure-2-why-is-it-taking-sooo-long"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/images/torch_run_arm64.png" alt="Figure 2: Why is it taking sooo long?" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 2: Why is it taking sooo long?
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id="debugging"&gt;Debugging&lt;/h2&gt;
&lt;p&gt;The first thing that came to mind was that there&amp;rsquo;s something wrong with our
infrastructure. We run self-hosted Github runners, with custom container images
that support the relevant software components we need for each plugin/case. We
run those on our infra, a set of VMs running on top of diverse low-end
bare-metal machines, both &lt;code&gt;amd64&lt;/code&gt; and &lt;code&gt;arm64&lt;/code&gt;. The &lt;code&gt;arm64&lt;/code&gt; runners run on a
couple of &lt;code&gt;Jetson&lt;/code&gt; &lt;code&gt;AGX&lt;/code&gt; &lt;code&gt;Orins&lt;/code&gt;, with 8 cores and 32GB of RAM.&lt;/p&gt;
&lt;p&gt;And what&amp;rsquo;s the first thing to try (especially when debugging on &lt;code&gt;arm64&lt;/code&gt;? A
Raspberry Pi of course!&lt;/p&gt;
&lt;p&gt;So getting the runner container image on a Raspberry Pi 5, with 8GB of RAM,
spinning up the container, building the library and the plugin, all took
roughly 10 minutes. And we&amp;rsquo;re ready for the test:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;#&lt;/span&gt; ninja run-examples-valgrind -C build-container
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;ninja: Entering directory `build-container&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[0/1] Running external command run-examples-valgrind (wrapped by meson to set env)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Arch is 64bit : true
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Running examples with plugin &amp;#39;libvaccel-torch.so&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;+ valgrind --leak-check=full --show-leak-kinds=all --track-origins=yes --errors-for-leak-kinds=all --max-stackframe=3150000 --keep-debuginfo=yes --error-exitcode=1 --suppressions=/home/ananos/develop/vaccel-plugin-torch/scripts/common/config/valgrind.supp --main-stacksize=33554432 --max-stackframe=4000000 --suppressions=/home/ananos/develop/vaccel-plugin-torch/scripts/config/valgrind.supp /home/runner/artifacts/bin/torch_inference /home/runner/artifacts/share/vaccel/images/example.jpg https://s3.nbfc.io/torch/mobilenet.pt /home/runner/artifacts/share/vaccel/labels/imagenet.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== Memcheck, a memory error detector
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== Copyright (C) 2002-2024, and GNU GPL&amp;#39;d, by Julian Seward et al.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== Using Valgrind-3.25.1 and LibVEX; rerun with -h for copyright info
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== Command: /home/runner/artifacts/bin/torch_inference /home/runner/artifacts/share/vaccel/images/example.jpg https://s3.nbfc.io/torch/mobilenet.pt /home/runner/artifacts/share/vaccel/labels/imagenet.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:01.91 - &amp;lt;debug&amp;gt; Initializing vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:01.93 - &amp;lt;info&amp;gt; vAccel 0.7.1-9-b175578f
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:01.93 - &amp;lt;debug&amp;gt; Config:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:01.93 - &amp;lt;debug&amp;gt; plugins = libvaccel-torch.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:01.93 - &amp;lt;debug&amp;gt; log_level = debug
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:01.93 - &amp;lt;debug&amp;gt; log_file = (null)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:01.93 - &amp;lt;debug&amp;gt; profiling_enabled = false
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:01.93 - &amp;lt;debug&amp;gt; version_ignore = false
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:01.94 - &amp;lt;debug&amp;gt; Created top-level rundir: /run/user/0/vaccel/ZpNkGT
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:47.87 - &amp;lt;info&amp;gt; Registered plugin torch 0.2.1-3-0b1978fb
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:48.07 - &amp;lt;debug&amp;gt; Downloading https://s3.nbfc.io/torch/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:53.18 - &amp;lt;debug&amp;gt; Downloaded: 2.4 KB of 13.7 MB (17.2%) | Speed: 474.96 KB/sec
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:54.93 - &amp;lt;debug&amp;gt; Downloaded: 13.7 MB of 13.7 MB (100.0%) | Speed: 2.01 MB/sec
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:54.95 - &amp;lt;debug&amp;gt; Download completed successfully
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:55.04 - &amp;lt;debug&amp;gt; session:1 Registered resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:56.37 - &amp;lt;debug&amp;gt; session:1 Looking for plugin implementing torch_jitload_forward operation
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:56.37 - &amp;lt;debug&amp;gt; Returning func from hint plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;CUDA not available, running in CPU mode
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Success!
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Result Tensor :
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Output tensor =&amp;gt; type:7 nr_dims:2
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;size: 4000 B
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Prediction: banana
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== HEAP SUMMARY:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== in use at exit: 339,636 bytes in 3,300 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== total heap usage: 1,779,929 allocs, 1,776,629 frees, 405,074,676 bytes allocated
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== LEAK SUMMARY:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== definitely lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== indirectly lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== possibly lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== still reachable: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== suppressed: 339,636 bytes in 3,300 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== For lists of detected and suppressed errors, rerun with: -s
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 3160 from 3160)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;+ valgrind --leak-check=full --show-leak-kinds=all --track-origins=yes --errors-for-leak-kinds=all --max-stackframe=3150000 --keep-debuginfo=yes --error-exitcode=1 --suppressions=/home/ananos/develop/vaccel-plugin-torch/scripts/common/config/valgrind.supp --main-stacksize=33554432 --max-stackframe=4000000 --suppressions=/home/ananos/develop/vaccel-plugin-torch/scripts/config/valgrind.supp /home/runner/artifacts/bin/classify /home/runner/artifacts/share/vaccel/images/example.jpg 1 https://s3.nbfc.io/torch/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== Memcheck, a memory error detector
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== Copyright (C) 2002-2024, and GNU GPL&amp;#39;d, by Julian Seward et al.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== Using Valgrind-3.25.1 and LibVEX; rerun with -h for copyright info
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== Command: /home/runner/artifacts/bin/classify /home/runner/artifacts/share/vaccel/images/example.jpg 1 https://s3.nbfc.io/torch/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:54:37.78 - &amp;lt;debug&amp;gt; Initializing vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:54:37.80 - &amp;lt;info&amp;gt; vAccel 0.7.1-9-b175578f
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:54:37.80 - &amp;lt;debug&amp;gt; Config:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:54:37.80 - &amp;lt;debug&amp;gt; plugins = libvaccel-torch.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:54:37.80 - &amp;lt;debug&amp;gt; log_level = debug
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:54:37.80 - &amp;lt;debug&amp;gt; log_file = (null)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:55:30.78 - &amp;lt;debug&amp;gt; Found implementation in torch plugin
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:55:30.78 - &amp;lt;debug&amp;gt; [torch] Loading model from /run/user/0/vaccel/zazTtc/resource.1/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;CUDA not available, running in CPU mode
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-21:01:14.77 - &amp;lt;debug&amp;gt; [torch] Prediction: banana
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;classification tags: banana
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-21:01:23.92 - &amp;lt;debug&amp;gt; Unregistered plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== HEAP SUMMARY:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== in use at exit: 341,280 bytes in 3,304 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== total heap usage: 3,167,523 allocs, 3,164,219 frees, 534,094,402 bytes allocated
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== LEAK SUMMARY:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== definitely lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== indirectly lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== possibly lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== still reachable: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== suppressed: 341,280 bytes in 3,304 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== For lists of detected and suppressed errors, rerun with: -s
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 3161 from 3161)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;+ set +x
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;Note: We&amp;rsquo;ll talk about the suppressions a bit later&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The test took roughly 13 minutes. At this point, we were scratching our heads.
Why would a high-end Jetson Orin, with way more cores and RAM, perform so much
worse under &lt;code&gt;Valgrind&lt;/code&gt; than a humble Raspberry Pi? Time to dig deeper into what’s
really going on under the hood&amp;hellip;&lt;/p&gt;
&lt;h2 id="the-surprise"&gt;The Surprise&lt;/h2&gt;
&lt;p&gt;When the results came in, the numbers were still striking: the same
&lt;code&gt;Valgrind&lt;/code&gt;-wrapped Torch test that took almost an hour on our Jetson Orin
finished in just 13 minutes on the Raspberry Pi. The Pi, with far less RAM and
CPU muscle, still managed to outperform the Orin by a wide margin under these
specific conditions.&lt;/p&gt;
&lt;p&gt;This result was the definition of counter-intuitive. Everything we know about
hardware says the Orin should wipe the floor with the Pi. Yet, here we were,
staring at the Pi’s prompt, wondering if we’d missed something obvious.&lt;/p&gt;
&lt;h2 id="digging-deeper-whats-really-happening"&gt;Digging Deeper: What’s Really Happening?&lt;/h2&gt;
&lt;p&gt;So, what’s going on? Why does a high-end, multi-core ARM system get crushed by
a humble Pi in this scenario? The answer lies at the intersection of &lt;code&gt;Valgrind&lt;/code&gt;,
multi-threaded workloads, and the quirks of the ARM64 ecosystem.&lt;/p&gt;
&lt;h3 id="thread-count-the-double-edged-sword"&gt;Thread Count: The Double-Edged Sword&lt;/h3&gt;
&lt;p&gt;Modern CPUs, especially high-end ARM chips like the Orin, have lots of cores,
and frameworks like PyTorch are eager to use them all. By default, PyTorch will
spawn as many threads as it thinks your system can handle, aiming for maximum
parallelism.&lt;/p&gt;
&lt;p&gt;But &lt;code&gt;Valgrind&lt;/code&gt;, which works by instrumenting every memory access and
synchronizing thread activity to catch bugs, doesn’t scale gracefully with
thread count. In fact:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each additional thread multiplies &lt;code&gt;Valgrind&lt;/code&gt;’s overhead. More threads mean more
context switches, more synchronization, and more internal bookkeeping.&lt;/li&gt;
&lt;li&gt;On platforms where &lt;code&gt;Valgrind&lt;/code&gt;’s threading support is less mature (like
aarch64), this overhead can balloon out of control.&lt;/li&gt;
&lt;li&gt;On the Raspberry Pi, with its modest core count, PyTorch only spawns a
handful of threads. But on the Orin, with many more cores, PyTorch ramps up
the thread count—and &lt;code&gt;Valgrind&lt;/code&gt;’s overhead explodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="the-arm64-valgrind-quirk"&gt;The ARM64 &lt;code&gt;Valgrind&lt;/code&gt; Quirk&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;arm64&lt;/code&gt; port of &lt;code&gt;Valgrind&lt;/code&gt; is still catching up to its &lt;code&gt;amd64&lt;/code&gt; sibling in
terms of optimizations and robustness. Some operations, especially those
involving threads and memory, are simply slower to emulate and track on &lt;code&gt;arm64&lt;/code&gt;.
This compounds the thread explosion problem, making high-core-count systems
paradoxically slower under &lt;code&gt;Valgrind&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id="dealing-with-library-suppressions-on-arm64-with-valgrind"&gt;Dealing with library suppressions on arm64 with &lt;code&gt;Valgrind&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;For instance, when running applications that rely on specific libraries under
&lt;code&gt;Valgrind&lt;/code&gt; on &lt;code&gt;arm64&lt;/code&gt; systems, developers frequently encounter a barrage of
memory-related warnings and errors. Many of these issues are not actual bugs in
your code, but rather artifacts of how these libraries manage memory
internally, or limitations in &lt;code&gt;Valgrind&lt;/code&gt;’s emulation on such architectures.&lt;/p&gt;
&lt;p&gt;For instance, OpenSSL is known for its custom memory management strategies. It
often allocates memory statically or uses platform-specific tricks, which can
confuse &lt;code&gt;Valgrind&lt;/code&gt;’s memory checker. For example, you might see reports of
&amp;ldquo;still reachable&amp;rdquo; memory or even &amp;ldquo;definitely lost&amp;rdquo; memory at program exit.&lt;/p&gt;
&lt;p&gt;In reality, much of this memory is intentionally held for the lifetime of the
process—such as global tables or the state for the random number generator.
These are not leaks in the conventional sense, but &lt;code&gt;Valgrind&lt;/code&gt; will still flag
them, especially if you run with strict leak checking enabled.&lt;/p&gt;
&lt;p&gt;On &lt;code&gt;arm64&lt;/code&gt; platforms, the situation can be further complicated. &lt;code&gt;Valgrind&lt;/code&gt; may
not fully emulate every instruction used by the specific library. This can lead
to false positives, such as uninitialized value warnings, or even more dramatic
errors like &lt;code&gt;SIGILL&lt;/code&gt; (illegal instruction) if &lt;code&gt;Valgrind&lt;/code&gt; encounters an
unsupported operation.&lt;/p&gt;
&lt;p&gt;It’s not uncommon to see a flood of warnings that are, in practice, harmless or
simply not actionable unless you’re developing for that specific library itself.&lt;/p&gt;
&lt;p&gt;To manage this noise and focus on real issues in our application, we use
&lt;code&gt;Valgrind&lt;/code&gt;’s suppression mechanism. Suppression files allow us
to tell &lt;code&gt;Valgrind&lt;/code&gt; to ignore specific known issues, so we can zero in on genuine
bugs in our own code.&lt;/p&gt;
&lt;p&gt;Suppression entries are typically matched by library object names, so on
&lt;code&gt;arm64&lt;/code&gt; we use patterns like &lt;code&gt;/usr/lib/aarch64-linux-gnu/libssh.so*&lt;/code&gt; or
&lt;code&gt;obj:*libc10*.so*&lt;/code&gt;, &lt;code&gt;obj:*libtorch*.so*&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;An example suppression snippet (&lt;code&gt;valgrind.supp&lt;/code&gt;) looks like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;[...]
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; suppress_libtorch_leaks
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; Memcheck:Leak
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; match-leak-kinds: reachable,possible
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ...
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; obj:*libtorch*.so*
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; suppress_libtorch_ovelaps
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; Memcheck:Overlap
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ...
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; obj:*libtorch*.so*
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;[...]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It’s important to note that not all problems can be suppressed away. For
example, if &lt;code&gt;Valgrind&lt;/code&gt; encounters a truly unsupported instruction and throws a
&lt;code&gt;SIGILL&lt;/code&gt;, a suppression file won’t help; you may need to update &lt;code&gt;Valgrind&lt;/code&gt; or avoid
that code path. Still, for the majority of benign memory warnings from OpenSSL
or Torch, well-crafted suppressions keeps our &lt;code&gt;Valgrind&lt;/code&gt; output manageable
and meaningful.&lt;/p&gt;
&lt;h3 id="debug-symbol-overhead"&gt;Debug Symbol Overhead&lt;/h3&gt;
&lt;p&gt;Another factor: large binaries with lots of debug symbols (common in deep
learning stacks) can cause &lt;code&gt;Valgrind&lt;/code&gt; to spend an inordinate amount of time just
parsing and managing symbol information. The more complex the binary and its
dependencies, the longer the startup and runtime overhead. Again, amplified on
&lt;code&gt;arm64&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="lessons-learned-and-what-you-can-do"&gt;Lessons Learned (and What You Can Do)&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Limit Thread Count&lt;/em&gt;: When running under &lt;code&gt;Valgrind&lt;/code&gt;, explicitly set PyTorch to
use a single thread &lt;code&gt;OMP_NUM_THREADS=1&lt;/code&gt;. This alone can make a world of
difference.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Test Small&lt;/em&gt;: Use the smallest possible model and dataset for &lt;code&gt;Valgrind&lt;/code&gt; runs.
Save the big workloads for native or lighter-weight profiling tools.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Expect the Unexpected&lt;/em&gt;: Don’t assume that “bigger is better” when debugging
with &lt;code&gt;Valgrind&lt;/code&gt; &amp;ndash; sometimes, less really is more!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Profile Performance Separately&lt;/em&gt;: Use &lt;code&gt;Valgrind&lt;/code&gt; for correctness and bug-hunting,
not for benchmarking or performance profiling.&lt;/p&gt;
&lt;p&gt;And here&amp;rsquo;s the full snippet of the test, on a runner VM on the Jetson Orin,
taking less than 6 minutes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; ninja run-examples-valgrind -C build
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;ninja: Entering directory `build&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[0/1] Running external command run-examples-valgrind (wrapped by meson to set env)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Arch is 64bit : true
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Default config dir : /home/ananos/vaccel-plugin-torch/scripts/common/config
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Package : vaccel-torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Package config dir : /home/ananos/vaccel-plugin-torch/scripts/config
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Package lib dir : /home/ananos/vaccel-plugin-torch/build/src
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;vAccel prefix : /home/runner/artifacts
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;vAccel lib dir : /home/runner/artifacts/lib/aarch64-linux-gnu
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;vAccel bin dir : /home/runner/artifacts/bin
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;vAccel share dir : /home/runner/artifacts/share/vaccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Running examples with plugin &amp;#39;libvaccel-torch.so&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;+ eval valgrind --leak-check=full --show-leak-kinds=all --track-origins=yes --errors-for-leak-kinds=all --max-stackframe=3150000 --keep-debuginfo=yes --error-exitcode=1 --suppressions=/home/ananos/vaccel-plugin-torch/scripts/common/config/valgrind.supp --main-stacksize=33554432 --max-stackframe=4000000 --fair-sched=no --suppressions=/home/ananos/vaccel-plugin-torch/scripts/config/valgrind.supp /home/runner/artifacts/bin/torch_inference /home/runner/artifacts/share/vaccel/images/example.jpg https://s3.nbfc.io/torch/mobilenet.pt /home/runner/artifacts/share/vaccel/labels/imagenet.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;+ valgrind --leak-check=full --show-leak-kinds=all --track-origins=yes --errors-for-leak-kinds=all --max-stackframe=3150000 --keep-debuginfo=yes --error-exitcode=1 --suppressions=/home/ananos/vaccel-plugin-torch/scripts/common/config/valgrind.supp --main-stacksize=33554432 --max-stackframe=4000000 --fair-sched=no --suppressions=/home/ananos/vaccel-plugin-torch/scripts/config/valgrind.supp /home/runner/artifacts/bin/torch_inference /home/runner/artifacts/share/vaccel/images/example.jpg https://s3.nbfc.io/torch/mobilenet.pt /home/runner/artifacts/share/vaccel/labels/imagenet.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== Memcheck, a memory error detector
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== Copyright (C) 2002-2024, and GNU GPL&amp;#39;d, by Julian Seward et al.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== Using Valgrind-3.25.1 and LibVEX; rerun with -h for copyright info
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== Command: /home/runner/artifacts/bin/torch_inference /home/runner/artifacts/share/vaccel/images/example.jpg https://s3.nbfc.io/torch/mobilenet.pt /home/runner/artifacts/share/vaccel/labels/imagenet.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:06:28.83 - &amp;lt;debug&amp;gt; Initializing vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:06:28.85 - &amp;lt;info&amp;gt; vAccel 0.7.1-9-b175578f
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:06:28.86 - &amp;lt;debug&amp;gt; Config:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:06:28.86 - &amp;lt;debug&amp;gt; plugins = libvaccel-torch.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:06:28.86 - &amp;lt;debug&amp;gt; log_level = debug
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:06:28.86 - &amp;lt;debug&amp;gt; log_file = (null)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:06:28.86 - &amp;lt;debug&amp;gt; profiling_enabled = false
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:06:28.86 - &amp;lt;debug&amp;gt; version_ignore = false
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:06:28.87 - &amp;lt;debug&amp;gt; Created top-level rundir: /run/user/1000/vaccel/P01ae4
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.35 - &amp;lt;info&amp;gt; Registered plugin torch 0.2.1-3-0b1978fb
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.35 - &amp;lt;debug&amp;gt; Registered op torch_jitload_forward from plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.35 - &amp;lt;debug&amp;gt; Registered op torch_sgemm from plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.35 - &amp;lt;debug&amp;gt; Registered op image_classify from plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.35 - &amp;lt;debug&amp;gt; Loaded plugin torch from libvaccel-torch.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.39 - &amp;lt;debug&amp;gt; Initialized resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Initialized model resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.39 - &amp;lt;debug&amp;gt; New rundir for session 1: /run/user/1000/vaccel/P01ae4/session.1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.39 - &amp;lt;debug&amp;gt; Initialized session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Initialized vAccel session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.40 - &amp;lt;debug&amp;gt; New rundir for resource 1: /run/user/1000/vaccel/P01ae4/resource.1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.62 - &amp;lt;debug&amp;gt; Downloading https://s3.nbfc.io/torch/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:33.90 - &amp;lt;debug&amp;gt; Downloaded: 555.7 KB of 13.7 MB (4.0%) | Speed: 88.84 KB/sec
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:36.78 - &amp;lt;debug&amp;gt; Downloaded: 13.7 MB of 13.7 MB (100.0%) | Speed: 1.50 MB/sec
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:36.80 - &amp;lt;debug&amp;gt; Download completed successfully
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:36.94 - &amp;lt;debug&amp;gt; session:1 Registered resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:38.16 - &amp;lt;debug&amp;gt; session:1 Looking for plugin implementing torch_jitload_forward operation
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:38.16 - &amp;lt;debug&amp;gt; Returning func from hint plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:38.16 - &amp;lt;debug&amp;gt; Found implementation in torch plugin
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:38.16 - &amp;lt;debug&amp;gt; [torch] session:1 Jitload &amp;amp; Forward Process
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:38.16 - &amp;lt;debug&amp;gt; [torch] Model: /run/user/1000/vaccel/P01ae4/resource.1/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:38.17 - &amp;lt;debug&amp;gt; [torch] Loading model from /run/user/1000/vaccel/P01ae4/resource.1/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;CUDA not available, running in CPU mode
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Success!
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Result Tensor :
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Output tensor =&amp;gt; type:7 nr_dims:2
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;size: 4000 B
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Prediction: banana
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:39.93 - &amp;lt;debug&amp;gt; session:1 Unregistered resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:39.94 - &amp;lt;debug&amp;gt; Released session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:39.94 - &amp;lt;debug&amp;gt; Removing file /run/user/1000/vaccel/P01ae4/resource.1/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:39.95 - &amp;lt;debug&amp;gt; Released resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:48.91 - &amp;lt;debug&amp;gt; Cleaning up vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:48.91 - &amp;lt;debug&amp;gt; Cleaning up sessions
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:48.91 - &amp;lt;debug&amp;gt; Cleaning up resources
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:48.91 - &amp;lt;debug&amp;gt; Cleaning up plugins
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:48.92 - &amp;lt;debug&amp;gt; Unregistered plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== HEAP SUMMARY:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== in use at exit: 304,924 bytes in 3,290 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== total heap usage: 1,780,098 allocs, 1,776,808 frees, 406,800,553 bytes allocated
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== LEAK SUMMARY:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== definitely lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== indirectly lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== possibly lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== still reachable: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== suppressed: 304,924 bytes in 3,290 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== For lists of detected and suppressed errors, rerun with: -s
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 3153 from 3153)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;+ [ 1 = 1 ]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;+ eval valgrind --leak-check=full --show-leak-kinds=all --track-origins=yes --errors-for-leak-kinds=all --max-stackframe=3150000 --keep-debuginfo=yes --error-exitcode=1 --suppressions=/home/ananos/vaccel-plugin-torch/scripts/common/config/valgrind.supp --main-stacksize=33554432 --max-stackframe=4000000 --fair-sched=no --suppressions=/home/ananos/vaccel-plugin-torch/scripts/config/valgrind.supp /home/runner/artifacts/bin/classify /home/runner/artifacts/share/vaccel/images/example.jpg 1 https://s3.nbfc.io/torch/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;+ valgrind --leak-check=full --show-leak-kinds=all --track-origins=yes --errors-for-leak-kinds=all --max-stackframe=3150000 --keep-debuginfo=yes --error-exitcode=1 --suppressions=/home/ananos/vaccel-plugin-torch/scripts/common/config/valgrind.supp --main-stacksize=33554432 --max-stackframe=4000000 --fair-sched=no --suppressions=/home/ananos/vaccel-plugin-torch/scripts/config/valgrind.supp /home/runner/artifacts/bin/classify /home/runner/artifacts/share/vaccel/images/example.jpg 1 https://s3.nbfc.io/torch/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== Memcheck, a memory error detector
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== Copyright (C) 2002-2024, and GNU GPL&amp;#39;d, by Julian Seward et al.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== Using Valgrind-3.25.1 and LibVEX; rerun with -h for copyright info
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== Command: /home/runner/artifacts/bin/classify /home/runner/artifacts/share/vaccel/images/example.jpg 1 https://s3.nbfc.io/torch/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:50.40 - &amp;lt;debug&amp;gt; Initializing vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:50.42 - &amp;lt;info&amp;gt; vAccel 0.7.1-9-b175578f
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:50.42 - &amp;lt;debug&amp;gt; Config:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:50.42 - &amp;lt;debug&amp;gt; plugins = libvaccel-torch.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:50.42 - &amp;lt;debug&amp;gt; log_level = debug
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:50.42 - &amp;lt;debug&amp;gt; log_file = (null)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:50.42 - &amp;lt;debug&amp;gt; profiling_enabled = false
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:50.42 - &amp;lt;debug&amp;gt; version_ignore = false
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:50.43 - &amp;lt;debug&amp;gt; Created top-level rundir: /run/user/1000/vaccel/73XJNT
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:48.93 - &amp;lt;info&amp;gt; Registered plugin torch 0.2.1-3-0b1978fb
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:48.93 - &amp;lt;debug&amp;gt; Registered op torch_jitload_forward from plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:48.93 - &amp;lt;debug&amp;gt; Registered op torch_sgemm from plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:48.93 - &amp;lt;debug&amp;gt; Registered op image_classify from plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:48.93 - &amp;lt;debug&amp;gt; Loaded plugin torch from libvaccel-torch.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:48.94 - &amp;lt;debug&amp;gt; New rundir for session 1: /run/user/1000/vaccel/73XJNT/session.1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:48.95 - &amp;lt;debug&amp;gt; Initialized session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Initialized session with id: 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:48.97 - &amp;lt;debug&amp;gt; Initialized resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:48.98 - &amp;lt;debug&amp;gt; New rundir for resource 1: /run/user/1000/vaccel/73XJNT/resource.1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:49.19 - &amp;lt;debug&amp;gt; Downloading https://s3.nbfc.io/torch/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:55.17 - &amp;lt;debug&amp;gt; Downloaded: 816.6 KB of 13.7 MB (5.8%) | Speed: 137.30 KB/sec
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:57.71 - &amp;lt;debug&amp;gt; Downloaded: 13.7 MB of 13.7 MB (100.0%) | Speed: 1.62 MB/sec
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:57.73 - &amp;lt;debug&amp;gt; Download completed successfully
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:57.87 - &amp;lt;debug&amp;gt; session:1 Registered resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:57.88 - &amp;lt;debug&amp;gt; session:1 Looking for plugin implementing VACCEL_OP_IMAGE_CLASSIFY
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:57.88 - &amp;lt;debug&amp;gt; Returning func from hint plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:57.88 - &amp;lt;debug&amp;gt; Found implementation in torch plugin
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:57.88 - &amp;lt;debug&amp;gt; [torch] Loading model from /run/user/1000/vaccel/73XJNT/resource.1/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;CUDA not available, running in CPU mode
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:31.42 - &amp;lt;debug&amp;gt; [torch] Prediction: banana
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;classification tags: banana
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;classification imagename: PLACEHOLDER
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:31.93 - &amp;lt;debug&amp;gt; session:1 Unregistered resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:31.93 - &amp;lt;debug&amp;gt; Removing file /run/user/1000/vaccel/73XJNT/resource.1/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:31.94 - &amp;lt;debug&amp;gt; Released resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:31.95 - &amp;lt;debug&amp;gt; Released session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:44.12 - &amp;lt;debug&amp;gt; Cleaning up vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:44.12 - &amp;lt;debug&amp;gt; Cleaning up sessions
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:44.12 - &amp;lt;debug&amp;gt; Cleaning up resources
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:44.12 - &amp;lt;debug&amp;gt; Cleaning up plugins
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:44.12 - &amp;lt;debug&amp;gt; Unregistered plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== HEAP SUMMARY:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== in use at exit: 306,616 bytes in 3,294 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== total heap usage: 3,167,511 allocs, 3,164,217 frees, 533,893,229 bytes allocated
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== LEAK SUMMARY:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== definitely lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== indirectly lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== possibly lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== still reachable: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== suppressed: 306,616 bytes in 3,294 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== For lists of detected and suppressed errors, rerun with: -s
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 3153 from 3153)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;+ set +x
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and the actual test in Figure 3, taking 8 minutes, almost 7 times faster than
the original execution:&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-figure-3-fixed-arm64-valgrind-test"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/images/torch_run_arm64-fixed.png" alt="Figure 3: Fixed arm64 valgrind test" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 3: Fixed arm64 valgrind test
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id="wrapping-up"&gt;Wrapping Up&lt;/h2&gt;
&lt;p&gt;This experience was a great reminder that debugging tools and parallel
workloads don’t always play nicely, especially on less mature platforms.
Sometimes, the humble Raspberry Pi will leave a high-end chip in the dust,
at least when &lt;code&gt;Valgrind&lt;/code&gt; is in the mix.&lt;/p&gt;
&lt;p&gt;So next time you’re staring at a progress bar that refuses to budge, remember:
more cores might just mean more waiting. And don’t be afraid to try your tests
on the &amp;ldquo;little guy&amp;rdquo; &amp;ndash; you might be surprised by what you find.&lt;/p&gt;</description></item></channel></rss>