<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>VirtIO | Nubificus</title><link>/tag/virtio/</link><atom:link href="/tag/virtio/index.xml" rel="self" type="application/rss+xml"/><description>VirtIO</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Fri, 04 Dec 2020 14:14:04 +0100</lastBuildDate><image><url>/media/logo_hu_866fdf07312224c.png</url><title>VirtIO</title><link>/tag/virtio/</link></image><item><title>Hardware acceleration in the Age of Functions (vol II)</title><link>/blog/vaccel_v2/</link><pubDate>Fri, 04 Dec 2020 14:14:04 +0100</pubDate><guid>/blog/vaccel_v2/</guid><description>&lt;p&gt;In our &lt;a href="https://blog.cloudkernels.net/posts/vaccel/" target="_blank" rel="noopener"&gt;previous post&lt;/a&gt; we spoke about the potential solutions for
deploying serverless offerings with hardware acceleration support. With the
increasing adoption of the serverless and FaaS paradigms, providers will need
to offer some form of hardware acceleration semantics.&lt;/p&gt;
&lt;p&gt;For some time now, Amazon has &lt;a href="https://github.com/firecracker-microvm/firecracker/issues/1179" target="_blank" rel="noopener"&gt;identified&lt;/a&gt; this as a &amp;ldquo;compelling use
case&amp;rdquo; for their AWS Firecracker hypervisor which powers the Amazon Lambda
service. What is more, they identify traditional techniques for GPU support in
VMs such as GPU passthrough comes with limitations and significantly increases
the attack surface of the hypervisor.&lt;/p&gt;
&lt;p&gt;As an alternative to passing through the accelerator device inside the guest,
paravirtual interfaces can expose hardware acceleration capabilities inside
the VM with minimal overhead and offering a simple user interface for
offloading code to the host for acceleration.&lt;/p&gt;
&lt;p&gt;In fact, such interfaces already exist. &lt;code&gt;virtio-crypto&lt;/code&gt; is an example, where
the guest VM uses a simple crypto API while the actual computation is
offloaded, through the paravirtual driver, to the host.&lt;/p&gt;
&lt;p&gt;We believe that the same paradigm can be applied to any kind of computation
that can benefit from acceleration. Whether that is crypto operations, Machine
Learning or linear algebra operators, the workflow from the point of view of
the developer these days is the same; You will use a framework such as
cryptodev, Jetson Inference or the BLAS library, to write your applications and
you will not deal with the low-level complexities of the actual accelerator.
Moreover, that workflow should not be different whether your application runs
on baremetal or inside a VM.&lt;/p&gt;
&lt;p&gt;In the rest of this post we present &lt;em&gt;vAccel&lt;/em&gt;, an acceleration framework that
enables &lt;strong&gt;portable&lt;/strong&gt; and &lt;strong&gt;hardware agnostic&lt;/strong&gt; acceleration for cloud
and edge applications.&lt;/p&gt;
&lt;h2 id="vaccel-design"&gt;vAccel design&lt;/h2&gt;
&lt;p&gt;In simple terms, vAccel is an acceleration API. It offers support for a set of
operators that commonly use hardware acceleration to increase performance,
such as machine learning and linear algebra operators.&lt;/p&gt;
&lt;p&gt;The API is implemented by &lt;em&gt;VaccelRT&lt;/em&gt; a thin and efficient runtime system that links
against the user application and is responsible for dispatching operations to
the relevant hardware accelerators. The interaction with the hardware itself is
mediated by plugins which implement the API for the specific hardware
accelerator.&lt;/p&gt;
&lt;p&gt;This design is driven by our requirements for high degree of portability, an
application that consumes the vAccel API can run without modification or even
re-compilation to any platform for which there is suitable back-end plugin.&lt;/p&gt;
&lt;p&gt;In fact, this opens up the way to enable the vAccel API inside a VM guest. The missing
bits are a virtio driver that implements the vAccel API and a backend plugin that
speaks with the virtio device. Once you have this components in place, you can
run your existing vAccel application inside a VM, just by using the virtio-plugin at
runtime.&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-vaccelrt"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel_v2/vaccelrt.png#center" alt="vAccel runtime" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
VaccelRT
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id="vaccel-support-in-aws-firecracker"&gt;vAccel support in AWS Firecracker&lt;/h2&gt;
&lt;p&gt;Once we implemented the frontend vAccel-virtio driver and virtio plugin for VaccelRT,
we need a hypervisor to test this on. We already showed, in the previous post some
initial vAccel results with QEMU as the target hypervisor. In this post, we will focus
on AWS Firecracker.&lt;/p&gt;
&lt;p&gt;AWS Firecracker has been designed having in mind really small boot times and small attack
surface, which makes it a compelling choice for cloud and edge deployments.
Moreover, it powers up Lambda, Amazon&amp;rsquo;s serverless platform, which we see as a
paradigm for which vAccel&amp;rsquo;s hardware abstraction level is a perfect fit.&lt;/p&gt;
&lt;p&gt;AWS Firecracker already implements virtio backend drivers for net, block and vsock. That
was good news for us, we have all the required virtio machinery. All we had to do, was
add a new device for vAccel and link the hypervisor with VaccelRT.&lt;/p&gt;
&lt;p&gt;The last bit required us to create the necessary Rust bindings for calling C from AWS Firecracker
which is written in Rust. This was actually a good exercise for us, since we plan to anyway
provide bindings for the vAccel API in more high-level languages.&lt;/p&gt;
&lt;p&gt;With all the components in place our stack looks like this:&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-vaccel-e2e"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel_v2/vaccele2e.png#center" alt="vAccel VM execution" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
vaccel-e2e
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The user application is consumes the vAccel API and links against VaccelRT. Inside the VM
the application uses the vAccel-virtio backend plugin. When a vAccel function is called, the
plugin will offload the request to &lt;code&gt;/dev/accel&lt;/code&gt; device which is the interface of the virtio
driver. Next, the virtio driver will forward the request to the vAccel-enabled AWS Firecracker
instance which will the host-residing VaccelRT. Finally, in the host side, VaccelRT will use
one of the available plugins, to execute the operation on the hardware accelerator.&lt;/p&gt;
&lt;p&gt;But how does this perform?&lt;/p&gt;
&lt;p&gt;We first grabbed a copy of jetson-inference, a rich repo full of ML inference models and example applications based on TensorRT. We patched it to be able to run on an x86 GPU (we had an NVIDIA RTX 2060 SUPER handy), and we built the vAccelRT backend for an image classification operation. To compare vAccel on AWS firecracker we patched the example imagenet-console application to properly calculate the time of execution and to account for more than 1 iteration. The average execution time for image classification on two sets of Image files (set &amp;ldquo;*_0.jpg&amp;rdquo; and &amp;ldquo;*_1.jpg) are shown below:&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-vaccel-bf-0"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel_v2/vaccel_bf_0.png#center" alt="vAccel VM execution" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
vaccel-bf-0
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The set above is sorted by the overhead percentage (GUEST vs HOST), while the set below, is sorted by Image size (in KB). One thing to note is that on all cases, the overhead of running an image classification operation in the guest compared to the host is less than 5%.&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-vaccel-bf-1"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel_v2/vaccel_bf_1.png#center" alt="vAccel VM execution" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
vaccel-bf-1
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id="putting-it-all-together"&gt;Putting it all together&lt;/h2&gt;
&lt;p&gt;So, are you brave (or curious) enough to try it out yourself ? Full disclosure, vAccel is WiP, in terms of software development terms, the project should be considered in a pre-alpha phase. However, since we think the idea is useful, there might be someone willing to try it out. So here we go:&lt;/p&gt;
&lt;p&gt;TL;DR
The easy way to try vAccel on AWS Firecracker is to run the docker container bundled with all necessary libraries/tools etc. The only prerequisite is [Docker][https://github.com/NVIDIA/nvidia-container-runtime] &amp;amp; nvidia-container-runtime[https://github.com/NVIDIA/nvidia-container-runtime] installed.&lt;/p&gt;
&lt;p&gt;To fire a VM up try running:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;docker run -e LD_LIBRARY_PATH=/usr/local/lib -e VACCEL_BACKENDS=/usr/local/lib/libvaccel-jetson.so --rm -it --gpus all --privileged nubificus/jetson-runtime
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now what the above command does is the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sets up a couple of env vars to let the container know where to find the necessary libraries&lt;/li&gt;
&lt;li&gt;runs in a privileged mode so that /dev/kvm is available to the container instance (we need to boot a VM in there ;))&lt;/li&gt;
&lt;li&gt;provides access to the GPU from the container.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The entrypoint for the above container image (&lt;code&gt;nubificus/jetson-runtime&lt;/code&gt;) simply starts a firecracker VM with a pre-built kernel &amp;amp; rootfs.img, available &lt;a href="https://github.com/nubificus/fc-x86-guest-build" target="_blank" rel="noopener"&gt;here&lt;/a&gt;. This repository contains the dockerfile from which these binaries have been produced. You can download ready-made binaries from the &lt;a href="https://github.com/nubificus/fc-x86-guest-build/releases/latest" target="_blank" rel="noopener"&gt;releases&lt;/a&gt; page.&lt;/p&gt;
&lt;p&gt;If (for any reason) you want to try out jetson-inference, without the AWS Firecracker VM boot, then just run the container with /bin/bash as an entrypoint, using the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;docker run --network=host --rm -it --gpus all --privileged -v nubificus/jetson-runtime /bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="host"&gt;Host&lt;/h3&gt;
&lt;p&gt;The current available version of vAccelRT supports the jetson-inference plugin. Adding a new plugin is as easy as linking with vAccelRT and writing the glue code in the plugin directory &amp;ndash; more information should be available in the coming weeks!&lt;/p&gt;
&lt;p&gt;To use this plugin, the Host machine should have an NVIDIA GPU (supporting CUDA), the relevant drivers installed, as well as &lt;a href="https://github.com/dusty-nv/jetson-inference" target="_blank" rel="noopener"&gt;jetson-inference&lt;/a&gt; installed.&lt;/p&gt;
&lt;p&gt;The next step is to build &amp;amp; install &lt;a href="https://github.com/cloudkernels/vaccelrt" target="_blank" rel="noopener"&gt;vAccelRT&lt;/a&gt;, the glue that ties everything together. You can build it from source, or just install the binaries available from the &lt;a href="https://github.com/cloudkernels/vaccelrt/releases/latest" target="_blank" rel="noopener"&gt;releases&lt;/a&gt; page. Make sure you specify &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; to the folder where libvaccel.so is located, as well as to choose the necessary backend by setting &lt;code&gt;VACCEL_BACKENDS&lt;/code&gt; to &lt;code&gt;libvaccel-jetson.so&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-gdscript3" data-lang="gdscript3"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="n"&gt;LD_LIBRARY_PATH&lt;/span&gt;&lt;span class="o"&gt;=/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="n"&gt;VACCEL_BACKENDS&lt;/span&gt;&lt;span class="o"&gt;=/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;libvaccel&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;jetson&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;so&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="vm"&gt;VM&lt;/h3&gt;
&lt;p&gt;To run a vAccel-enabled VM, we need four basic components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the AWS firecracker VMM (with the vAccel backend patch) &lt;a href="https://github.com/cloudkernels/firecracker" target="_blank" rel="noopener"&gt;github&lt;/a&gt; &lt;a href="https://github.com/cloudkernels/firecracker/releases/latest" target="_blank" rel="noopener"&gt;releases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;a firecracker guest Linux kernel supporting modules + the virtio-accel module &lt;a href="https://github.com/nubificus/fc-x86-guest-build" target="_blank" rel="noopener"&gt;github&lt;/a&gt; &lt;a href="https://github.com/nubificus/fc-x86-guest-build/releases/latest" target="_blank" rel="noopener"&gt;releases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;the vAccel runtime system (vAccelRT) for the Host and the guest &lt;a href="https://github.com/cloudkernels/vaccelrt" target="_blank" rel="noopener"&gt;github&lt;/a&gt; &lt;a href="https://github.com/cloudkernels/vaccelrt/releases/latest" target="_blank" rel="noopener"&gt;releases&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To facilitate the process of packing all these software components, we include links to binaries built from the respective github repositories.&lt;/p&gt;
&lt;p&gt;Grab, or build &lt;code&gt;vmlinux&lt;/code&gt; &amp;amp; the &lt;code&gt;rootfs.img&lt;/code&gt; from the links above, and use a template config for firecracker like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;boot-source&amp;#34;: {
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;kernel_image_path&amp;#34;: &amp;#34;vmlinux&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;boot_args&amp;#34;: &amp;#34;console=ttyS0 reboot=k panic=1 pci=off loglevel=0 root=/dev/vda quiet&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; },
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;drives&amp;#34;: [
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;drive_id&amp;#34;: &amp;#34;rootfs&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;path_on_host&amp;#34;: &amp;#34;rootfs.img&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;is_root_device&amp;#34;: true,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;is_read_only&amp;#34;: false
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ],
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;network-interfaces&amp;#34;: [
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;iface_id&amp;#34;: &amp;#34;eth0&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;guest_mac&amp;#34;: &amp;#34;AA:FC:00:00:00:01&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;host_dev_name&amp;#34;: &amp;#34;tap&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ],
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;crypto&amp;#34;: {
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;crypto_dev_id&amp;#34;: &amp;#34;vaccel0&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;host_crypto_dev&amp;#34;: &amp;#34;/dev/vaccel0&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; },
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;machine-config&amp;#34;: {
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;vcpu_count&amp;#34;: 1,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;mem_size_mib&amp;#34;: 1024,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;ht_enabled&amp;#34;: false
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Make sure vmlinux, rootfs.img are in the same directory as the invocation of the firecracker command. Also, ensure that you have set &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; and &lt;code&gt;VACCEL_BACKENDS&lt;/code&gt; correctly, and that you&amp;rsquo;ve downloaded the ML networks needed for inference. This step can be done using &lt;a href="https://github.com/dusty-nv/jetson-inference/blob/master/tools/download-models.sh" target="_blank" rel="noopener"&gt;this script&lt;/a&gt;. Just get this script and run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-gdscript3" data-lang="gdscript3"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="o"&gt;./&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sh&lt;/span&gt; &lt;span class="n"&gt;NO&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and the models will be placed by default at &lt;code&gt;../data/networks&lt;/code&gt;. Change the path in the script if you need to. For the AWS Firecracker backend to work, we need the models in the same directory as the invocation of the firecracker binary, in a folder called networks.&lt;/p&gt;
&lt;p&gt;Now, we&amp;rsquo;re ready to fire up our VM:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;firecracker --api-sock /tmp/fc.sock --config-file config_vaccel.json --seccomp-level 0
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;aaand we get the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;2020-12-04T14:39:02.363414625 [anonymous-instance:WARN:src/devices/src/legacy/i8042.rs:126] Failed to trigger i8042 kbd interrupt (disabled by guest OS)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;2020-12-04T14:39:02.369542154 [anonymous-instance:WARN:src/devices/src/legacy/i8042.rs:126] Failed to trigger i8042 kbd interrupt (disabled by guest OS)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;SELinux: Could not open policy file &amp;lt;= /etc/selinux/targeted/policy/policy.32: No such file or directory
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;[ 0.970330] systemd[1]: Failed to find module &amp;#39;autofs4&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;[UNSUPP] Starting of Arbitrary Execâ€¦Automount Point not supported.
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;Ubuntu 20.04.1 LTS vaccel-guest.nubificus.co.uk ttyS0
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;vaccel-guest login:
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Some harmless messages and a login prompt! Try &lt;code&gt;root&lt;/code&gt; for username (no password).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;vaccel-guest login: root
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 4.20.0 x86_64)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; * Documentation: https://help.ubuntu.com
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; * Management: https://landscape.canonical.com
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; * Support: https://ubuntu.com/advantage
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;This system has been minimized by removing packages and content that are
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;not required on a system that users do not log into.
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;To restore this content, you can run the &amp;#39;unminimize&amp;#39; command.
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;The programs included with the Ubuntu system are free software;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;the exact distribution terms for each program are described in the
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;individual files in /usr/share/doc/*/copyright.
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;applicable law.
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;root@vaccel-guest:~#
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Bear in mind, the rootfs.img is based on docker hub&amp;rsquo;s ubuntu latest.&lt;/p&gt;
&lt;p&gt;Now lets do some image classification! Try running the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;root@vaccel-guest:~# ./classify images/airplane_1.jpg 1
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;Initialized session with id: 1
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;Image size: 115835B
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;classification tags: 21.973% warplane, military plane
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first time the execution might take longer, but any consecutive run will be significantly faster. This is because the acceleration backend (jetson-inference) needs to examine the hardware available, generate and load the necessary components for the model to run correctly.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s it! you just ran an image classification example for a JPG image, in an AWS Firecracker VM, talking vAccelRT, which, in turn, forwards this request to AWS Firecracker vAccel backend, calling vAccelRT, jetson-inference, talking to the GPU and voila!&lt;/p&gt;
&lt;h2 id="future-steps"&gt;Future steps&lt;/h2&gt;
&lt;p&gt;Argh! you made it this far? :D If you enjoyed playing with hardware
acceleration &amp;amp; Firecracker stay tuned to enjoy some more of this on aarch64
devices (yeap, the NVIDIA Jetson Nano!). Last but not least, we are in the
process of integrating vAccel-enabled AWS Firecracker instances to k8s and k3s
to facilitate deployment and scaling.&lt;/p&gt;
&lt;p&gt;Give us a shout at &lt;a href="mailto:team@cloudkernels.net"&gt;team@cloudkernels.net&lt;/a&gt; if you liked it, or visit the &lt;a href="https://vaccel.org" target="_blank" rel="noopener"&gt;vAccel&lt;/a&gt; website and drop us a note at &lt;a href="mailto:vaccel@nubificus.co.uk"&gt;vaccel@nubificus.co.uk&lt;/a&gt; for more info!&lt;/p&gt;</description></item><item><title>Hardware acceleration in the Age of Functions</title><link>/blog/vaccel/</link><pubDate>Mon, 01 Jun 2020 20:29:17 +0000</pubDate><guid>/blog/vaccel/</guid><description>&lt;p&gt;The debate on how to deploy applications, monoliths or micro services, is in
full swing. Part of this discussion relates to how the new paradigm
incorporates support for accessing accelerators, e.g. GPUs, FPGAs. That kind
of support has been made available to traditional programming models the last
couple of decades and its tooling has evolved to be stable and standardized.&lt;/p&gt;
&lt;p&gt;On the other hand, what does it mean for a serverless setup to access an
accelerator? Should the function invoked to classify an image, for instance,
link against the whole acceleration runtime and program the hardware device
itself? It seems quite counter-intuitive to create such bloated functions.&lt;/p&gt;
&lt;p&gt;Things get more complicated when we consider the low-level layers of the
service architecture. How does the system itself get access to the
acceleration hardware? Docker allows exposing a GPU device inside a container
for some time now, so serverless systems based on top of it can expose GPU
devices to running functions. Virtual Machine-based setups rely on the
monitor, e.g. QEMU or &lt;a href="https://firecracker-microvm.github.io/" target="_blank" rel="noopener"&gt;Firecracker&lt;/a&gt;, to expose acceleration devices to the
guest.&lt;/p&gt;
&lt;p&gt;There are several techniques used to expose a device from the host to a guest
VM. Passthrough mode exposes the hardware accelerator as is inside the guest.
This mode provides native performance using the accelerator from inside the VM,
however it does cause issues with sharing the device across multiple VMs. API
remoting, e.g. &lt;a href="http://rcuda.net/" target="_blank" rel="noopener"&gt;rCUDA&lt;/a&gt;, is another option, where requests are being
forwarded to the accelerator device over the network. Finally, there is the
option of paravirtual interfaces where the monitor exposes a generic device to
the guest, with a very simple API. Applications in the guest send requests to
the paravirtual device which are then passed to the hypervisor and dispatched
by the latter to an accelerator device on the host.&lt;/p&gt;
&lt;p&gt;VirtIO drivers are an example of such paravirtualized frameworks. VirtIO
exposes simple front-end device drivers to the guest, rather than emulating
complex devices and offloads the complexity of interacting with the hardware
to the back-end that lives in the Virtual Machine Monitor (VMM).&lt;/p&gt;
&lt;h3 id="virtio-crypto"&gt;virtio-crypto&lt;/h3&gt;
&lt;p&gt;One of the devices described in the VirtIO spec is the &lt;a href="https://github.com/gongleiarei/virtio" target="_blank" rel="noopener"&gt;virtio-crypto&lt;/a&gt;
&lt;a href="https://github.com/gongleiarei/virtio-crypto-linux-driver" target="_blank" rel="noopener"&gt;device&lt;/a&gt;. The guest chooses the cryptographic operation to perform and
passes a pointer to the data that will be manipulated. The actual operation is
offloaded through the VMM to the host crypto acceleration device.&lt;/p&gt;
&lt;p&gt;A VM is able to use a crypto device by using a combination of
&lt;em&gt;cryptodev&lt;/em&gt; and &lt;em&gt;virtio-crypto&lt;/em&gt;. Requests for encryption /
decryption originating from the VM, get forwarded to the backend, get injected
to the &lt;em&gt;cryptodev&lt;/em&gt; device and end up being handled by the host Linux
kernel. Figure 1 presents an overview of the &lt;em&gt;virtio-crypto&lt;/em&gt;
architecture.&lt;/p&gt;
&lt;figure id="figure-figure-1-virtio-crypto-architecture-overview"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel/virtio-crypto.png#center" alt="Figure 1: VirtIO-crypto architecture overview" loading="lazy" data-zoomable width="60%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 1: VirtIO-crypto architecture overview
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;In the context of micro-services (FaaS/Serverless) cryptographic operations are
quite common, presented to the user as language/library abstractions.
Integrating an off-loading mechanism of these CPU-intensive operations seems
like an interesting optimization. To showcase the potential of paravirtual
accelerated devices, we implemented a &lt;em&gt;virtio-crypto&lt;/em&gt; backend driver
for AWS Firecracker. Since &lt;em&gt;virtio-crypto&lt;/em&gt;&amp;rsquo;s frontend is already present
in the Linux kernel, all we had to do is implement the corresponding back-end
in the Firecracker code base. This effort was relatively straight-forward since
Firecracker already provides a number of VirtIO devices, e.g. net and block,
which means that all the machinery for communication with the guest was in
place.&lt;/p&gt;
&lt;p&gt;Figure 2 shows the performance our &lt;em&gt;virtio-crypto&lt;/em&gt; driver achieves (light
bars) compared to running the computation in the guest kernel using the
&lt;em&gt;cryptodev-linux&lt;/em&gt; driver (dark bars), when running the AES-CBC cipher.
Unfortunately, we have not been able to get our hands on a crypto acceleration
device, so &lt;em&gt;virtio-crypto&lt;/em&gt; is using the same &lt;em&gt;cryptodev-linux&lt;/em&gt;
device in the host (the CPU). This means that we do not actually accelerate the
operation, but our experiment is quite useful to see the VirtIO overhead of
offloading the operation to the host. As expected, the larger the block size of
the blob we are encrypting, the better we are able to hide the cost of moving
data from the userland of the guest to the kernel of the host.&lt;/p&gt;
&lt;figure id="figure-figure-2-host-and-guest-throughput-for-aes-cbc-128-vs-chunk-size"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel/aes_results.png#center" alt="Figure 2: Host and Guest Throughput for AES-CBC-128 vs chunk size" loading="lazy" data-zoomable width="80%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 2: Host and Guest Throughput for AES-CBC-128 vs chunk size
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;This is encouraging; once there is a hardware accelerator for computation,
acceleration capabilities are automatically exposed inside a Firecracker VM in
a secure way with reasonably low overhead. Which inevitably leads us to the
thought, &lt;em&gt;why only crypto?&lt;/em&gt; The &lt;em&gt;virtio-crypto&lt;/em&gt; example showcases a
simple interface through which we can achieve hardware acceleration, so why not
generalize this to other types of acceleration?&lt;/p&gt;
&lt;p&gt;This gave us the idea to define a simple, hardware-agnostic API to accelerate
any operation, as long as the host supports it. We believe that an API at this
granularity is the right abstraction for serverless frameworks, since it moves
the complexity of accelerating operations from the guest to the host.&lt;/p&gt;
&lt;h3 id="vaccel"&gt;vAccel&lt;/h3&gt;
&lt;p&gt;Let us consider a simple use-case: matrix multiplication. It is a common
operation, used in numerous applications, in HPC, Machine Learning, and Big
Data. In the generic case, the user running the application on a VM would
have to either have access to the GPU hardware and enjoy hardware acceleration,
or perform the operation on the CPU, wasting time and CPU cycles.&lt;/p&gt;
&lt;p&gt;Instead of passing through the GPU hardware, we choose a different
path: we introduce vAccel, a simple paravirtual framework that forwards
operation requests to the monitor, which, in turn, uses native calls to an
acceleration framework, taking advantage of the hardware capabilities of the
host.&lt;/p&gt;
&lt;p&gt;The vAccel framework allows workloads that execute on Virtual Machines to
offload compute-intensive functions to backends provided by the hypervisor. To
achieve this, the system presents a number of host-side accelerator functions
to the guest kernel, which are backed by hardware accelerators (FPGAs, GPUs,
specialized crypto engines etc.).&lt;/p&gt;
&lt;p&gt;vAccel consists of three main parts: the frontend driver, the backend driver
and the runtime. An overview of the system architecture is shown in Figure 3.&lt;/p&gt;
&lt;figure id="figure-figure-3-vaccel-architecture-overview"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel/vaccel.png#center" alt="Figure 3: vAccel architecture overview" loading="lazy" data-zoomable width="70%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 3: vAccel architecture overview
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Frontend and Backend drivers implement the transport layer. We base our
implementation on VirtIO, and follow the generic VirtIO spec, using a single
queue for control and data exchange.&lt;/p&gt;
&lt;p&gt;The runtime includes two components: a &lt;em&gt;host library&lt;/em&gt; that handles
offload requests, and a &lt;em&gt;guest library&lt;/em&gt; that intercepts the actual
offload-able user calls and creates those requests.&lt;/p&gt;
&lt;p&gt;The basic API is given below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-C" data-lang="C"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;typedef&lt;/span&gt; &lt;span class="kt"&gt;uint8_t&lt;/span&gt; &lt;span class="kt"&gt;vaccel_op_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="cm"&gt;/* Get available accelerate-able operations from the backend */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;vaccel_get_operations&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;uint8_t&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;available_operations&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="cm"&gt;/* Start a new session */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kt"&gt;vaccel_session_t&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nf"&gt;create_vaccel_session&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="cm"&gt;/* Invoke an acceleration operation */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;do_operation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;vaccel_session_t&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;vaccel_op_t&lt;/span&gt; &lt;span class="n"&gt;operation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="cm"&gt;/* End a running session */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;destory_vaccel_session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;vaccel_session_t&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To study the potential overhead of such an approach on a common function, we
deploy a generic QEMU/KVM VM on an x86 host, using an FPGA card as the
accelerator (&lt;a href="http://www.netlib.org/lapack/explore-html/db/dc9/group__single__blas__level3_gafe51bacb54592ff5de056acabd83c260.html" target="_blank" rel="noopener"&gt;SGEMM&lt;/a&gt;, implemented with OpenCL). We run the stencil on the host to
obtain a baseline, and then we execute the same benchmark on the vAccel-enabled
guest and capture the results.&lt;/p&gt;
&lt;figure id="figure-figure-4-sgemm-host--guest-results-vs-matrix-size"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel/sgemm.png#center" alt="Figure 4: SGEMM Host / Guest results vs matrix size" loading="lazy" data-zoomable width="80%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 4: SGEMM Host / Guest results vs matrix size
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Figure 4 presents the performance of SGEMM on a single core VM (light bars)
against the respective run on the host (dark bars) for various matrix sizes.
On the Y axis we plot the MFlops achieved by the SGEMM stencil, while on the X
axis we lay the size of the matrices tested. For large matrix sizes (&amp;gt;
128x128), the overhead perceived by the user is minimal, ranging from 16% to
even &amp;lt;3%.&lt;/p&gt;
&lt;h3 id="inference-at-the-edge"&gt;Inference at the edge&lt;/h3&gt;
&lt;p&gt;Let us now consider a more complicated scenario: image classification. From the
user perspective it is a simple operation: (i) provide an image as input, (ii)
define which model will be used to classify the image, (iii) wait for the
result. However, the system internals are a bit more complicated: the image has
to be preprocessed, fed to a pre-trained classification model, and mapped to a
given set of labels. This abstraction is already provided by common frameworks
such as Tensorflow, Caffe etc. However, these frameworks perform optimally with
direct access to hardware accelerators. Figure 5 presents the path to the
hardware accelerator from the VM&amp;rsquo;s userspace.&lt;/p&gt;
&lt;figure id="figure-figure-5-inference-use-case"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel/ml_legacy.png#center" alt="Figure 5: Inference use-case" loading="lazy" data-zoomable width="80%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 5: Inference use-case
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;We use vAccel to expose accelerated inference capabilities to a guest VM.
Specifically, we expose one basic function, image classification. The guest
simply issues a request with the image to be classified and the model to be
used for inference. The backend forwards this request to vAccel-runtime, which,
in turn, calls wrapper functions on top of the Tensorflow runtime to classify
the image. The result is copied back to the guest synchronously. Figure 6
presents the vAccel-enabled path.&lt;/p&gt;
&lt;figure id="figure-figure-6-inference-use-case-with-vaccel"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel/ml_vaccel.png#center" alt="Figure 6: Inference use-case with vAccel" loading="lazy" data-zoomable width="80%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 6: Inference use-case with vAccel
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Figure 7 plots the total execution time of an image classification operation
for various image sizes deployed on a generic QEMU/KVM VM on an &lt;a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-nano/" target="_blank" rel="noopener"&gt;NVIDIA Jetson
Nano&lt;/a&gt;. Dark bars indicate the time required to complete the operation on
the host, whereas light bars show the respective time spent on the guest.
Clearly, the overhead is minimal: the average overhead across all cases is 1%.&lt;/p&gt;
&lt;figure id="figure-figure-7-image-classification-with-vaccel"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel/ml_results.png#center" alt="Figure 7: Image classification with vAccel" loading="lazy" data-zoomable width="80%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 7: Image classification with vAccel
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;As execution moves to the Edge, following the Serverless paradigm, efficiency
is key to provide low power consumption, while at the same time increase the
quality and the diversity of services offered to the end user. Offloading
computation to specialized units is one of the most important aspects to
balance trade-offs related to resource utilization and energy-efficiency and
to minimize request-response latency.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;vAccel is being developed jointly by the &lt;a href="http://research.cslab.ece.ntua.gr" target="_blank" rel="noopener"&gt;Computing Systems Laboratory&lt;/a&gt; of
the &lt;a href="https://www.ntua.gr" target="_blank" rel="noopener"&gt;National Technical University of Athens&lt;/a&gt; and &lt;a href="https://nubificus.co.uk" target="_blank" rel="noopener"&gt;Nubificus LTD&lt;/a&gt;.
vAccel is open-source and WiP; we plan to provide an RFC for the frontend
driver to be upstreamed, as well as respective RFCs for the backends (QEMU,
Firecracker etc.).&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>