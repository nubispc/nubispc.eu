<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>VAccel | Nubificus</title><link>/tag/vaccel/</link><atom:link href="/tag/vaccel/index.xml" rel="self" type="application/rss+xml"/><description>VAccel</description><generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Sat, 31 Jan 2026 11:50:00 +0100</lastBuildDate><image><url>/media/logo_hu_866fdf07312224c.png</url><title>VAccel</title><link>/tag/vaccel/</link></image><item><title>Beyond TinyML: Balance inference accuracy and latency on MCUs</title><link>/event/fosdem2026-2/</link><pubDate>Sat, 31 Jan 2026 11:50:00 +0100</pubDate><guid>/event/fosdem2026-2/</guid><description>&lt;h2 id="code--resources"&gt;Code &amp;amp; Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/urunc-dev/urunc" target="_blank" rel="noopener"&gt;urunc - Container Runtime Repository&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nubificus/bunny" target="_blank" rel="noopener"&gt;bunny - Building Tool Repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>A Lightweight C Library for Fetching OCI Artifacts</title><link>/blog/fetch_models_in_c/</link><pubDate>Sun, 26 Oct 2025 00:00:00 +0000</pubDate><guid>/blog/fetch_models_in_c/</guid><description>&lt;h1 id="from-containers-to-kserve-and-vaccel"&gt;From Containers to &lt;code&gt;KServe&lt;/code&gt; and &lt;code&gt;vAccel&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;
&lt;figure &gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel-oci-c.png" alt="" loading="lazy" data-zoomable width="100%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;/figure&gt;
Container images have become the standard unit of software packaging and
deployment. They’re everywhere: in the cloud, on edge devices, and even in AI
inference pipelines. Yet, despite the ubiquity of OCI (Open Container
Initiative) registries and image formats, there hasn’t been a clean,
lightweight &lt;strong&gt;C library&lt;/strong&gt; for fetching and unpacking OCI images.&lt;/p&gt;
&lt;p&gt;We’ve built exactly that: a &lt;a href="https://github.com/nubificus/oci-c" target="_blank" rel="noopener"&gt;minimalist C client library for OCI
registries&lt;/a&gt;, designed for embedding in
systems software, unikernel runtimes, and edge-native inference frameworks. The
inspiration was &lt;a href="https://github.com/modelpack/model-spec" target="_blank" rel="noopener"&gt;ModelPack&lt;/a&gt;, a recent
CNCF sandbox project that establishes open standards for packaging,
distributing and running AI artifacts in the cloud-native environment.&lt;/p&gt;
&lt;h2 id="why-a-c-oci-client"&gt;Why a C OCI Client?&lt;/h2&gt;
&lt;p&gt;Most existing OCI tooling, such as
&lt;a href="https://github.com/containers/skopeo" target="_blank" rel="noopener"&gt;skopeo&lt;/a&gt;, &lt;a href="https://oras.land/" target="_blank" rel="noopener"&gt;ORAS&lt;/a&gt;, or
&lt;a href="https://www.docker.com/" target="_blank" rel="noopener"&gt;Docker&lt;/a&gt;, is written in Go or Python. While these
tools are excellent for command-line and automation tasks, they’re heavy,
dynamically linked, and not suitable for embedding into low-level runtimes or
constrained environments.&lt;/p&gt;
&lt;p&gt;When you want to pull container artifacts directly into a C codebase, say,
a unikernel launcher, an inference runtime, or an edge orchestrator, your
options are limited. You’d need to shell out to external tools or reimplement
the OCI registry protocol from scratch.&lt;/p&gt;
&lt;p&gt;That’s where our &lt;strong&gt;OCI Client Library&lt;/strong&gt; comes in.&lt;/p&gt;
&lt;h2 id="about-the-project"&gt;About the Project&lt;/h2&gt;
&lt;p&gt;The library provides a clean, almost dependency-free API for interacting with
OCI-compliant registries. It can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Retrieve manifests (including multi-architecture ones)&lt;/li&gt;
&lt;li&gt;Download layer blobs by digest&lt;/li&gt;
&lt;li&gt;Extract &lt;code&gt;.tar.gz&lt;/code&gt; layers to a filesystem&lt;/li&gt;
&lt;li&gt;Even handle fetch authentication tokens from registries (yes, even for public
registry repos, you need an auth token!)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Under the hood, it uses &lt;a href="https://github.com/curl/curl" target="_blank" rel="noopener"&gt;&lt;strong&gt;libcurl&lt;/strong&gt;&lt;/a&gt;,
&lt;a href="https://github.com/tspspi/libcjson" target="_blank" rel="noopener"&gt;&lt;strong&gt;cJSON&lt;/strong&gt;&lt;/a&gt;, and
&lt;a href="https://github.com/libarchive/libarchive" target="_blank" rel="noopener"&gt;&lt;strong&gt;libarchive&lt;/strong&gt;&lt;/a&gt;, but all those
details are hidden. Applications link against a single, self-contained library
and call a handful of (we hope) intuitive functions.&lt;/p&gt;
&lt;h2 id="a-simpler-api"&gt;A Simpler API&lt;/h2&gt;
&lt;p&gt;The library’s design philosophy is straightforward: &lt;strong&gt;fetching and unpacking
container images should be as simple as fetching a file&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Here’s what using it looks like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-c" data-lang="c"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nf"&gt;oci_client_init&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kt"&gt;char&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;fetch_token&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#34;https://harbor.nbfc.io&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#34;models/resnet101-v2.7&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;// get a token
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kt"&gt;char&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;manifest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;fetch_manifest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#34;https://harbor.nbfc.io&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;// registry
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s"&gt;&amp;#34;models/resnet101-v2.7&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;// repo
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s"&gt;&amp;#34;tvm&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;// tag
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s"&gt;&amp;#34;amd64&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;// arch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s"&gt;&amp;#34;linux&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;// os
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;// auth
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;OciLayer&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;oci_manifest_parse_layers&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;manifest&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;// helper function to parse the layers
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;// Get the layers one by one, and extract them
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;Memory&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;blob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;fetch_blob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#34;https://harbor.nbfc.io&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="s"&gt;&amp;#34;models/resnet101-v2.7&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;digest&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nf"&gt;extract_tar_gz&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#34;output&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nf"&gt;oci_layers_free&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;// cleanup
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nf"&gt;oci_client_cleanup&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt; &lt;span class="c1"&gt;// cleanup
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That’s all it takes to pull and extract an OCI image layer-by-layer in native C code.&lt;/p&gt;
&lt;h2 id="integration-with-kserve-and-vaccel"&gt;Integration with &lt;code&gt;KServe&lt;/code&gt; and &lt;code&gt;vAccel&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;This library isn’t just for experiments, it’s becoming a key part of our
cloud-native acceleration stack. Specifically, in
&lt;a href="https://github.com/nubificus/vaccel" target="_blank" rel="noopener"&gt;&lt;code&gt;vAccel&lt;/code&gt;&lt;/a&gt; we &lt;a href="https://github.com/nubificus/vaccel/pull/137" target="_blank" rel="noopener"&gt;refactored the &lt;code&gt;vAccel Resource&lt;/code&gt; handling&lt;/a&gt;
to allow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;local files (as before),&lt;/li&gt;
&lt;li&gt;remote files (from URIs), to fetch models/TVM shared objects etc.&lt;/li&gt;
&lt;li&gt;multiple files (either archives or compressed archives), to account for TF
saved models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The plan is to add an extra, OCI option, to &lt;code&gt;vAccel&lt;/code&gt;, to facilitate software
delivery of models to &lt;code&gt;vAccel&lt;/code&gt; instances (agents or applications).&lt;/p&gt;
&lt;p&gt;This functionality, will enable efficient model fetching in &lt;code&gt;KServe&lt;/code&gt; deployments
that use &lt;code&gt;vAccel&lt;/code&gt;. Thus, instead of relying on &lt;code&gt;KServe&lt;/code&gt;&amp;rsquo;s &lt;code&gt;STORAGE_URI&lt;/code&gt; and
side-car containers to fetch the models and make them available to the
inference service container, we just specify the OCI URI
(&lt;code&gt;oci://harbor.nbfc.io/models/resnet101-v2.7:tvm&lt;/code&gt;) and the binary artifact is
available to &lt;code&gt;vAccel&lt;/code&gt; as a &lt;code&gt;Resource&lt;/code&gt;, ready to be loaded by the relevant
plugin/backend.&lt;/p&gt;
&lt;p&gt;Additionally, to leverage &lt;code&gt;KServe&lt;/code&gt;&amp;rsquo;s simplified workflow, we could patch the code
to allow for a custom side-car that just fetches the model like that, without
relying on heavy-weight containers in Python/Go. This way, we make deployments
faster, more portable, and suitable for edge devices with limited resources.&lt;/p&gt;
&lt;h2 id="why-store-models-in-oci-registries-"&gt;Why store models in OCI Registries ?&lt;/h2&gt;
&lt;p&gt;Storing models as OCI artifacts transforms them into first-class, verifiable,
and portable software units, aligning ML deployment with modern DevOps and
GitOps practices. Using OCI registries to store ML models provides several
important benefits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Immutability: Once pushed, layers and manifests are immutable. This ensures
that models cannot be tampered with after release.&lt;/li&gt;
&lt;li&gt;Verification &amp;amp; Trust: Tools like &lt;code&gt;cosign&lt;/code&gt; allow signing and verifying models,
ensuring integrity and origin.&lt;/li&gt;
&lt;li&gt;Provenance: Registry manifests track digests, timestamps, and annotations,
making it easy to track model versions.&lt;/li&gt;
&lt;li&gt;Compatibility: OCI is an open standard, widely supported across cloud
providers, edge runtimes, and orchestration systems.&lt;/li&gt;
&lt;li&gt;Layered Storage &amp;amp; Reuse: Common dependencies can be shared across models,
reducing storage and bandwidth usage. Tooling such as runtime dependencies
(eg the TVM runtime, alongside the shared object, or labels, alongside a ResNet
torchscript model).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="building-and-running"&gt;Building and Running&lt;/h2&gt;
&lt;p&gt;To build the library, you need Meson and Ninja:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;sudo apt install build-essential meson ninja-build libcurl4-openssl-dev libcjson-dev libarchive-dev
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;meson setup builddir -Dlibrary_type&lt;span class="o"&gt;=&lt;/span&gt;shared
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;meson compile -C builddir
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then try the included demo program:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;./builddir/oci_client_demo \
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; -r https://harbor.nbfc.io \
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; -R models/resnet101-v2.7 \
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; -t tvm -a amd64 -o linux -d output
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It will pull and extract a TVM-enabled shared object for ResNet101 under output/.&lt;/p&gt;
&lt;h2 id="towards-oci-native-acceleration"&gt;Towards OCI-Native Acceleration&lt;/h2&gt;
&lt;p&gt;With this library, we’re filling a missing gap in the OCI ecosystem: a native,
embeddable, dependency-minimal C client for fetching container artifacts.&lt;/p&gt;
&lt;p&gt;We’re excited about what this unlocks for edge computing and AI model lifecycle management.&lt;/p&gt;
&lt;h2 id="get-involved"&gt;Get Involved&lt;/h2&gt;
&lt;p&gt;The code is open source, licensed under Apache-2.0, and available on &lt;a href="https://github.com/nubificus/oci-c" target="_blank" rel="noopener"&gt;GitHub&lt;/a&gt;.&lt;br&gt;
Contributions, feedback, and integrations are welcome!&lt;/p&gt;</description></item><item><title>When More Cores Means Less Speed: Debugging PyTorch with Valgrind on ARM</title><link>/blog/torch-arm-debug/</link><pubDate>Fri, 11 Jul 2025 08:02:51 +0100</pubDate><guid>/blog/torch-arm-debug/</guid><description>&lt;p&gt;If you’ve ever tried to debug a PyTorch program on an ARM64 system using
&lt;a href="https://valgrind.org/" target="_blank" rel="noopener"&gt;&lt;code&gt;Valgrind&lt;/code&gt;&lt;/a&gt;, you might have stumbled on something really
odd: &amp;ldquo;Why does it take so long?&amp;rdquo;. And if you&amp;rsquo;re like us, you would probably try
to run it locally, on a Raspberry pi, to see what&amp;rsquo;s going on&amp;hellip; And the madness
begins!&lt;/p&gt;
&lt;p&gt;TL;DR, as you probably figured out from the title of this post, it’s a
counter-intuitive experience: the more cores your machine has, the slower your
(torch) code seems to run under &lt;code&gt;Valgrind&lt;/code&gt;. Shouldn’t more cores mean more speed?
Let’s dive into why that’s not always the case ;)&lt;/p&gt;
&lt;h2 id="the-background"&gt;The background&lt;/h2&gt;
&lt;p&gt;In an effort to improve our testing infrastructure for
&lt;a href="https://github.com/nubificus/vaccel" target="_blank" rel="noopener"&gt;vAccel&lt;/a&gt; and make it more robust, we
started cleaning up our examples, unifying the build &amp;amp; test scripts and started
adding more elaborate test cases for both the library and the plugins.
&lt;code&gt;Valgrind&lt;/code&gt; provides a quite decent experience for this, especially to catch
multi-arch errors, memory leaks and dangling pointers (something quite common
when writing in C :D).&lt;/p&gt;
&lt;h2 id="the-issue"&gt;The issue&lt;/h2&gt;
&lt;p&gt;While adding the &lt;code&gt;Valgrind&lt;/code&gt; mode of execution in our tests for the vAccel
plugins, we noticed something really weird in the
&lt;a href="https://docs.vaccel.org/latest/plugins/available-plugins/acceleration-plugins/torch-plugin/" target="_blank" rel="noopener"&gt;Torch&lt;/a&gt;
case. The test was taking forever!&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-figure-1-build--test-run-on-amd64"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/images/torch_run_amd64.png" alt="Figure 1: Build &amp; Test run on amd64" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 1: Build &amp;amp; Test run on amd64
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Specifically, while the equivalent &lt;code&gt;amd64&lt;/code&gt; was taking roughly 4 and a half
minutes (Figure 1), the &lt;code&gt;arm64&lt;/code&gt; run was taking nearly an hour (53 minutes) &amp;ndash;
see Figure 2.&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-figure-2-why-is-it-taking-sooo-long"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/images/torch_run_arm64.png" alt="Figure 2: Why is it taking sooo long?" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 2: Why is it taking sooo long?
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id="debugging"&gt;Debugging&lt;/h2&gt;
&lt;p&gt;The first thing that came to mind was that there&amp;rsquo;s something wrong with our
infrastructure. We run self-hosted Github runners, with custom container images
that support the relevant software components we need for each plugin/case. We
run those on our infra, a set of VMs running on top of diverse low-end
bare-metal machines, both &lt;code&gt;amd64&lt;/code&gt; and &lt;code&gt;arm64&lt;/code&gt;. The &lt;code&gt;arm64&lt;/code&gt; runners run on a
couple of &lt;code&gt;Jetson&lt;/code&gt; &lt;code&gt;AGX&lt;/code&gt; &lt;code&gt;Orins&lt;/code&gt;, with 8 cores and 32GB of RAM.&lt;/p&gt;
&lt;p&gt;And what&amp;rsquo;s the first thing to try (especially when debugging on &lt;code&gt;arm64&lt;/code&gt;? A
Raspberry Pi of course!&lt;/p&gt;
&lt;p&gt;So getting the runner container image on a Raspberry Pi 5, with 8GB of RAM,
spinning up the container, building the library and the plugin, all took
roughly 10 minutes. And we&amp;rsquo;re ready for the test:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;#&lt;/span&gt; ninja run-examples-valgrind -C build-container
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;ninja: Entering directory `build-container&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[0/1] Running external command run-examples-valgrind (wrapped by meson to set env)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Arch is 64bit : true
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Running examples with plugin &amp;#39;libvaccel-torch.so&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;+ valgrind --leak-check=full --show-leak-kinds=all --track-origins=yes --errors-for-leak-kinds=all --max-stackframe=3150000 --keep-debuginfo=yes --error-exitcode=1 --suppressions=/home/ananos/develop/vaccel-plugin-torch/scripts/common/config/valgrind.supp --main-stacksize=33554432 --max-stackframe=4000000 --suppressions=/home/ananos/develop/vaccel-plugin-torch/scripts/config/valgrind.supp /home/runner/artifacts/bin/torch_inference /home/runner/artifacts/share/vaccel/images/example.jpg https://s3.nbfc.io/torch/mobilenet.pt /home/runner/artifacts/share/vaccel/labels/imagenet.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== Memcheck, a memory error detector
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== Copyright (C) 2002-2024, and GNU GPL&amp;#39;d, by Julian Seward et al.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== Using Valgrind-3.25.1 and LibVEX; rerun with -h for copyright info
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== Command: /home/runner/artifacts/bin/torch_inference /home/runner/artifacts/share/vaccel/images/example.jpg https://s3.nbfc.io/torch/mobilenet.pt /home/runner/artifacts/share/vaccel/labels/imagenet.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:01.91 - &amp;lt;debug&amp;gt; Initializing vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:01.93 - &amp;lt;info&amp;gt; vAccel 0.7.1-9-b175578f
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:01.93 - &amp;lt;debug&amp;gt; Config:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:01.93 - &amp;lt;debug&amp;gt; plugins = libvaccel-torch.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:01.93 - &amp;lt;debug&amp;gt; log_level = debug
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:01.93 - &amp;lt;debug&amp;gt; log_file = (null)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:01.93 - &amp;lt;debug&amp;gt; profiling_enabled = false
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:01.93 - &amp;lt;debug&amp;gt; version_ignore = false
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:01.94 - &amp;lt;debug&amp;gt; Created top-level rundir: /run/user/0/vaccel/ZpNkGT
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:47.87 - &amp;lt;info&amp;gt; Registered plugin torch 0.2.1-3-0b1978fb
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:48.07 - &amp;lt;debug&amp;gt; Downloading https://s3.nbfc.io/torch/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:53.18 - &amp;lt;debug&amp;gt; Downloaded: 2.4 KB of 13.7 MB (17.2%) | Speed: 474.96 KB/sec
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:54.93 - &amp;lt;debug&amp;gt; Downloaded: 13.7 MB of 13.7 MB (100.0%) | Speed: 2.01 MB/sec
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:54.95 - &amp;lt;debug&amp;gt; Download completed successfully
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:55.04 - &amp;lt;debug&amp;gt; session:1 Registered resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:56.37 - &amp;lt;debug&amp;gt; session:1 Looking for plugin implementing torch_jitload_forward operation
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:48:56.37 - &amp;lt;debug&amp;gt; Returning func from hint plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;CUDA not available, running in CPU mode
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Success!
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Result Tensor :
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Output tensor =&amp;gt; type:7 nr_dims:2
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;size: 4000 B
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Prediction: banana
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== HEAP SUMMARY:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== in use at exit: 339,636 bytes in 3,300 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== total heap usage: 1,779,929 allocs, 1,776,629 frees, 405,074,676 bytes allocated
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== LEAK SUMMARY:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== definitely lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== indirectly lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== possibly lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== still reachable: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== suppressed: 339,636 bytes in 3,300 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== For lists of detected and suppressed errors, rerun with: -s
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==371== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 3160 from 3160)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;+ valgrind --leak-check=full --show-leak-kinds=all --track-origins=yes --errors-for-leak-kinds=all --max-stackframe=3150000 --keep-debuginfo=yes --error-exitcode=1 --suppressions=/home/ananos/develop/vaccel-plugin-torch/scripts/common/config/valgrind.supp --main-stacksize=33554432 --max-stackframe=4000000 --suppressions=/home/ananos/develop/vaccel-plugin-torch/scripts/config/valgrind.supp /home/runner/artifacts/bin/classify /home/runner/artifacts/share/vaccel/images/example.jpg 1 https://s3.nbfc.io/torch/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== Memcheck, a memory error detector
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== Copyright (C) 2002-2024, and GNU GPL&amp;#39;d, by Julian Seward et al.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== Using Valgrind-3.25.1 and LibVEX; rerun with -h for copyright info
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== Command: /home/runner/artifacts/bin/classify /home/runner/artifacts/share/vaccel/images/example.jpg 1 https://s3.nbfc.io/torch/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:54:37.78 - &amp;lt;debug&amp;gt; Initializing vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:54:37.80 - &amp;lt;info&amp;gt; vAccel 0.7.1-9-b175578f
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:54:37.80 - &amp;lt;debug&amp;gt; Config:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:54:37.80 - &amp;lt;debug&amp;gt; plugins = libvaccel-torch.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:54:37.80 - &amp;lt;debug&amp;gt; log_level = debug
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:54:37.80 - &amp;lt;debug&amp;gt; log_file = (null)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:55:30.78 - &amp;lt;debug&amp;gt; Found implementation in torch plugin
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:55:30.78 - &amp;lt;debug&amp;gt; [torch] Loading model from /run/user/0/vaccel/zazTtc/resource.1/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;CUDA not available, running in CPU mode
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-21:01:14.77 - &amp;lt;debug&amp;gt; [torch] Prediction: banana
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;classification tags: banana
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-21:01:23.92 - &amp;lt;debug&amp;gt; Unregistered plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== HEAP SUMMARY:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== in use at exit: 341,280 bytes in 3,304 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== total heap usage: 3,167,523 allocs, 3,164,219 frees, 534,094,402 bytes allocated
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== LEAK SUMMARY:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== definitely lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== indirectly lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== possibly lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== still reachable: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== suppressed: 341,280 bytes in 3,304 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== For lists of detected and suppressed errors, rerun with: -s
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==376== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 3161 from 3161)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;+ set +x
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;Note: We&amp;rsquo;ll talk about the suppressions a bit later&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The test took roughly 13 minutes. At this point, we were scratching our heads.
Why would a high-end Jetson Orin, with way more cores and RAM, perform so much
worse under &lt;code&gt;Valgrind&lt;/code&gt; than a humble Raspberry Pi? Time to dig deeper into what’s
really going on under the hood&amp;hellip;&lt;/p&gt;
&lt;h2 id="the-surprise"&gt;The Surprise&lt;/h2&gt;
&lt;p&gt;When the results came in, the numbers were still striking: the same
&lt;code&gt;Valgrind&lt;/code&gt;-wrapped Torch test that took almost an hour on our Jetson Orin
finished in just 13 minutes on the Raspberry Pi. The Pi, with far less RAM and
CPU muscle, still managed to outperform the Orin by a wide margin under these
specific conditions.&lt;/p&gt;
&lt;p&gt;This result was the definition of counter-intuitive. Everything we know about
hardware says the Orin should wipe the floor with the Pi. Yet, here we were,
staring at the Pi’s prompt, wondering if we’d missed something obvious.&lt;/p&gt;
&lt;h2 id="digging-deeper-whats-really-happening"&gt;Digging Deeper: What’s Really Happening?&lt;/h2&gt;
&lt;p&gt;So, what’s going on? Why does a high-end, multi-core ARM system get crushed by
a humble Pi in this scenario? The answer lies at the intersection of &lt;code&gt;Valgrind&lt;/code&gt;,
multi-threaded workloads, and the quirks of the ARM64 ecosystem.&lt;/p&gt;
&lt;h3 id="thread-count-the-double-edged-sword"&gt;Thread Count: The Double-Edged Sword&lt;/h3&gt;
&lt;p&gt;Modern CPUs, especially high-end ARM chips like the Orin, have lots of cores,
and frameworks like PyTorch are eager to use them all. By default, PyTorch will
spawn as many threads as it thinks your system can handle, aiming for maximum
parallelism.&lt;/p&gt;
&lt;p&gt;But &lt;code&gt;Valgrind&lt;/code&gt;, which works by instrumenting every memory access and
synchronizing thread activity to catch bugs, doesn’t scale gracefully with
thread count. In fact:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each additional thread multiplies &lt;code&gt;Valgrind&lt;/code&gt;’s overhead. More threads mean more
context switches, more synchronization, and more internal bookkeeping.&lt;/li&gt;
&lt;li&gt;On platforms where &lt;code&gt;Valgrind&lt;/code&gt;’s threading support is less mature (like
aarch64), this overhead can balloon out of control.&lt;/li&gt;
&lt;li&gt;On the Raspberry Pi, with its modest core count, PyTorch only spawns a
handful of threads. But on the Orin, with many more cores, PyTorch ramps up
the thread count—and &lt;code&gt;Valgrind&lt;/code&gt;’s overhead explodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="the-arm64-valgrind-quirk"&gt;The ARM64 &lt;code&gt;Valgrind&lt;/code&gt; Quirk&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;arm64&lt;/code&gt; port of &lt;code&gt;Valgrind&lt;/code&gt; is still catching up to its &lt;code&gt;amd64&lt;/code&gt; sibling in
terms of optimizations and robustness. Some operations, especially those
involving threads and memory, are simply slower to emulate and track on &lt;code&gt;arm64&lt;/code&gt;.
This compounds the thread explosion problem, making high-core-count systems
paradoxically slower under &lt;code&gt;Valgrind&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id="dealing-with-library-suppressions-on-arm64-with-valgrind"&gt;Dealing with library suppressions on arm64 with &lt;code&gt;Valgrind&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;For instance, when running applications that rely on specific libraries under
&lt;code&gt;Valgrind&lt;/code&gt; on &lt;code&gt;arm64&lt;/code&gt; systems, developers frequently encounter a barrage of
memory-related warnings and errors. Many of these issues are not actual bugs in
your code, but rather artifacts of how these libraries manage memory
internally, or limitations in &lt;code&gt;Valgrind&lt;/code&gt;’s emulation on such architectures.&lt;/p&gt;
&lt;p&gt;For instance, OpenSSL is known for its custom memory management strategies. It
often allocates memory statically or uses platform-specific tricks, which can
confuse &lt;code&gt;Valgrind&lt;/code&gt;’s memory checker. For example, you might see reports of
&amp;ldquo;still reachable&amp;rdquo; memory or even &amp;ldquo;definitely lost&amp;rdquo; memory at program exit.&lt;/p&gt;
&lt;p&gt;In reality, much of this memory is intentionally held for the lifetime of the
process—such as global tables or the state for the random number generator.
These are not leaks in the conventional sense, but &lt;code&gt;Valgrind&lt;/code&gt; will still flag
them, especially if you run with strict leak checking enabled.&lt;/p&gt;
&lt;p&gt;On &lt;code&gt;arm64&lt;/code&gt; platforms, the situation can be further complicated. &lt;code&gt;Valgrind&lt;/code&gt; may
not fully emulate every instruction used by the specific library. This can lead
to false positives, such as uninitialized value warnings, or even more dramatic
errors like &lt;code&gt;SIGILL&lt;/code&gt; (illegal instruction) if &lt;code&gt;Valgrind&lt;/code&gt; encounters an
unsupported operation.&lt;/p&gt;
&lt;p&gt;It’s not uncommon to see a flood of warnings that are, in practice, harmless or
simply not actionable unless you’re developing for that specific library itself.&lt;/p&gt;
&lt;p&gt;To manage this noise and focus on real issues in our application, we use
&lt;code&gt;Valgrind&lt;/code&gt;’s suppression mechanism. Suppression files allow us
to tell &lt;code&gt;Valgrind&lt;/code&gt; to ignore specific known issues, so we can zero in on genuine
bugs in our own code.&lt;/p&gt;
&lt;p&gt;Suppression entries are typically matched by library object names, so on
&lt;code&gt;arm64&lt;/code&gt; we use patterns like &lt;code&gt;/usr/lib/aarch64-linux-gnu/libssh.so*&lt;/code&gt; or
&lt;code&gt;obj:*libc10*.so*&lt;/code&gt;, &lt;code&gt;obj:*libtorch*.so*&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;An example suppression snippet (&lt;code&gt;valgrind.supp&lt;/code&gt;) looks like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;[...]
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; suppress_libtorch_leaks
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; Memcheck:Leak
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; match-leak-kinds: reachable,possible
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ...
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; obj:*libtorch*.so*
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; suppress_libtorch_ovelaps
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; Memcheck:Overlap
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ...
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; obj:*libtorch*.so*
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;[...]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;It’s important to note that not all problems can be suppressed away. For
example, if &lt;code&gt;Valgrind&lt;/code&gt; encounters a truly unsupported instruction and throws a
&lt;code&gt;SIGILL&lt;/code&gt;, a suppression file won’t help; you may need to update &lt;code&gt;Valgrind&lt;/code&gt; or avoid
that code path. Still, for the majority of benign memory warnings from OpenSSL
or Torch, well-crafted suppressions keeps our &lt;code&gt;Valgrind&lt;/code&gt; output manageable
and meaningful.&lt;/p&gt;
&lt;h3 id="debug-symbol-overhead"&gt;Debug Symbol Overhead&lt;/h3&gt;
&lt;p&gt;Another factor: large binaries with lots of debug symbols (common in deep
learning stacks) can cause &lt;code&gt;Valgrind&lt;/code&gt; to spend an inordinate amount of time just
parsing and managing symbol information. The more complex the binary and its
dependencies, the longer the startup and runtime overhead. Again, amplified on
&lt;code&gt;arm64&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="lessons-learned-and-what-you-can-do"&gt;Lessons Learned (and What You Can Do)&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Limit Thread Count&lt;/em&gt;: When running under &lt;code&gt;Valgrind&lt;/code&gt;, explicitly set PyTorch to
use a single thread &lt;code&gt;OMP_NUM_THREADS=1&lt;/code&gt;. This alone can make a world of
difference.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Test Small&lt;/em&gt;: Use the smallest possible model and dataset for &lt;code&gt;Valgrind&lt;/code&gt; runs.
Save the big workloads for native or lighter-weight profiling tools.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Expect the Unexpected&lt;/em&gt;: Don’t assume that “bigger is better” when debugging
with &lt;code&gt;Valgrind&lt;/code&gt; &amp;ndash; sometimes, less really is more!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Profile Performance Separately&lt;/em&gt;: Use &lt;code&gt;Valgrind&lt;/code&gt; for correctness and bug-hunting,
not for benchmarking or performance profiling.&lt;/p&gt;
&lt;p&gt;And here&amp;rsquo;s the full snippet of the test, on a runner VM on the Jetson Orin,
taking less than 6 minutes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; ninja run-examples-valgrind -C build
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;ninja: Entering directory `build&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[0/1] Running external command run-examples-valgrind (wrapped by meson to set env)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Arch is 64bit : true
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Default config dir : /home/ananos/vaccel-plugin-torch/scripts/common/config
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Package : vaccel-torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Package config dir : /home/ananos/vaccel-plugin-torch/scripts/config
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Package lib dir : /home/ananos/vaccel-plugin-torch/build/src
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;vAccel prefix : /home/runner/artifacts
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;vAccel lib dir : /home/runner/artifacts/lib/aarch64-linux-gnu
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;vAccel bin dir : /home/runner/artifacts/bin
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;vAccel share dir : /home/runner/artifacts/share/vaccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Running examples with plugin &amp;#39;libvaccel-torch.so&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;+ eval valgrind --leak-check=full --show-leak-kinds=all --track-origins=yes --errors-for-leak-kinds=all --max-stackframe=3150000 --keep-debuginfo=yes --error-exitcode=1 --suppressions=/home/ananos/vaccel-plugin-torch/scripts/common/config/valgrind.supp --main-stacksize=33554432 --max-stackframe=4000000 --fair-sched=no --suppressions=/home/ananos/vaccel-plugin-torch/scripts/config/valgrind.supp /home/runner/artifacts/bin/torch_inference /home/runner/artifacts/share/vaccel/images/example.jpg https://s3.nbfc.io/torch/mobilenet.pt /home/runner/artifacts/share/vaccel/labels/imagenet.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;+ valgrind --leak-check=full --show-leak-kinds=all --track-origins=yes --errors-for-leak-kinds=all --max-stackframe=3150000 --keep-debuginfo=yes --error-exitcode=1 --suppressions=/home/ananos/vaccel-plugin-torch/scripts/common/config/valgrind.supp --main-stacksize=33554432 --max-stackframe=4000000 --fair-sched=no --suppressions=/home/ananos/vaccel-plugin-torch/scripts/config/valgrind.supp /home/runner/artifacts/bin/torch_inference /home/runner/artifacts/share/vaccel/images/example.jpg https://s3.nbfc.io/torch/mobilenet.pt /home/runner/artifacts/share/vaccel/labels/imagenet.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== Memcheck, a memory error detector
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== Copyright (C) 2002-2024, and GNU GPL&amp;#39;d, by Julian Seward et al.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== Using Valgrind-3.25.1 and LibVEX; rerun with -h for copyright info
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== Command: /home/runner/artifacts/bin/torch_inference /home/runner/artifacts/share/vaccel/images/example.jpg https://s3.nbfc.io/torch/mobilenet.pt /home/runner/artifacts/share/vaccel/labels/imagenet.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:06:28.83 - &amp;lt;debug&amp;gt; Initializing vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:06:28.85 - &amp;lt;info&amp;gt; vAccel 0.7.1-9-b175578f
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:06:28.86 - &amp;lt;debug&amp;gt; Config:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:06:28.86 - &amp;lt;debug&amp;gt; plugins = libvaccel-torch.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:06:28.86 - &amp;lt;debug&amp;gt; log_level = debug
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:06:28.86 - &amp;lt;debug&amp;gt; log_file = (null)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:06:28.86 - &amp;lt;debug&amp;gt; profiling_enabled = false
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:06:28.86 - &amp;lt;debug&amp;gt; version_ignore = false
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:06:28.87 - &amp;lt;debug&amp;gt; Created top-level rundir: /run/user/1000/vaccel/P01ae4
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.35 - &amp;lt;info&amp;gt; Registered plugin torch 0.2.1-3-0b1978fb
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.35 - &amp;lt;debug&amp;gt; Registered op torch_jitload_forward from plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.35 - &amp;lt;debug&amp;gt; Registered op torch_sgemm from plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.35 - &amp;lt;debug&amp;gt; Registered op image_classify from plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.35 - &amp;lt;debug&amp;gt; Loaded plugin torch from libvaccel-torch.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.39 - &amp;lt;debug&amp;gt; Initialized resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Initialized model resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.39 - &amp;lt;debug&amp;gt; New rundir for session 1: /run/user/1000/vaccel/P01ae4/session.1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.39 - &amp;lt;debug&amp;gt; Initialized session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Initialized vAccel session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.40 - &amp;lt;debug&amp;gt; New rundir for resource 1: /run/user/1000/vaccel/P01ae4/resource.1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:27.62 - &amp;lt;debug&amp;gt; Downloading https://s3.nbfc.io/torch/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:33.90 - &amp;lt;debug&amp;gt; Downloaded: 555.7 KB of 13.7 MB (4.0%) | Speed: 88.84 KB/sec
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:36.78 - &amp;lt;debug&amp;gt; Downloaded: 13.7 MB of 13.7 MB (100.0%) | Speed: 1.50 MB/sec
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:36.80 - &amp;lt;debug&amp;gt; Download completed successfully
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:36.94 - &amp;lt;debug&amp;gt; session:1 Registered resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:38.16 - &amp;lt;debug&amp;gt; session:1 Looking for plugin implementing torch_jitload_forward operation
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:38.16 - &amp;lt;debug&amp;gt; Returning func from hint plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:38.16 - &amp;lt;debug&amp;gt; Found implementation in torch plugin
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:38.16 - &amp;lt;debug&amp;gt; [torch] session:1 Jitload &amp;amp; Forward Process
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:38.16 - &amp;lt;debug&amp;gt; [torch] Model: /run/user/1000/vaccel/P01ae4/resource.1/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:07:38.17 - &amp;lt;debug&amp;gt; [torch] Loading model from /run/user/1000/vaccel/P01ae4/resource.1/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;CUDA not available, running in CPU mode
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Success!
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Result Tensor :
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Output tensor =&amp;gt; type:7 nr_dims:2
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;size: 4000 B
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Prediction: banana
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:39.93 - &amp;lt;debug&amp;gt; session:1 Unregistered resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:39.94 - &amp;lt;debug&amp;gt; Released session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:39.94 - &amp;lt;debug&amp;gt; Removing file /run/user/1000/vaccel/P01ae4/resource.1/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:39.95 - &amp;lt;debug&amp;gt; Released resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:48.91 - &amp;lt;debug&amp;gt; Cleaning up vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:48.91 - &amp;lt;debug&amp;gt; Cleaning up sessions
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:48.91 - &amp;lt;debug&amp;gt; Cleaning up resources
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:48.91 - &amp;lt;debug&amp;gt; Cleaning up plugins
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:48.92 - &amp;lt;debug&amp;gt; Unregistered plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== HEAP SUMMARY:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== in use at exit: 304,924 bytes in 3,290 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== total heap usage: 1,780,098 allocs, 1,776,808 frees, 406,800,553 bytes allocated
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== LEAK SUMMARY:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== definitely lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== indirectly lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== possibly lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== still reachable: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== suppressed: 304,924 bytes in 3,290 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== For lists of detected and suppressed errors, rerun with: -s
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1655== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 3153 from 3153)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;+ [ 1 = 1 ]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;+ eval valgrind --leak-check=full --show-leak-kinds=all --track-origins=yes --errors-for-leak-kinds=all --max-stackframe=3150000 --keep-debuginfo=yes --error-exitcode=1 --suppressions=/home/ananos/vaccel-plugin-torch/scripts/common/config/valgrind.supp --main-stacksize=33554432 --max-stackframe=4000000 --fair-sched=no --suppressions=/home/ananos/vaccel-plugin-torch/scripts/config/valgrind.supp /home/runner/artifacts/bin/classify /home/runner/artifacts/share/vaccel/images/example.jpg 1 https://s3.nbfc.io/torch/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;+ valgrind --leak-check=full --show-leak-kinds=all --track-origins=yes --errors-for-leak-kinds=all --max-stackframe=3150000 --keep-debuginfo=yes --error-exitcode=1 --suppressions=/home/ananos/vaccel-plugin-torch/scripts/common/config/valgrind.supp --main-stacksize=33554432 --max-stackframe=4000000 --fair-sched=no --suppressions=/home/ananos/vaccel-plugin-torch/scripts/config/valgrind.supp /home/runner/artifacts/bin/classify /home/runner/artifacts/share/vaccel/images/example.jpg 1 https://s3.nbfc.io/torch/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== Memcheck, a memory error detector
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== Copyright (C) 2002-2024, and GNU GPL&amp;#39;d, by Julian Seward et al.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== Using Valgrind-3.25.1 and LibVEX; rerun with -h for copyright info
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== Command: /home/runner/artifacts/bin/classify /home/runner/artifacts/share/vaccel/images/example.jpg 1 https://s3.nbfc.io/torch/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:50.40 - &amp;lt;debug&amp;gt; Initializing vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:50.42 - &amp;lt;info&amp;gt; vAccel 0.7.1-9-b175578f
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:50.42 - &amp;lt;debug&amp;gt; Config:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:50.42 - &amp;lt;debug&amp;gt; plugins = libvaccel-torch.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:50.42 - &amp;lt;debug&amp;gt; log_level = debug
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:50.42 - &amp;lt;debug&amp;gt; log_file = (null)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:50.42 - &amp;lt;debug&amp;gt; profiling_enabled = false
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:50.42 - &amp;lt;debug&amp;gt; version_ignore = false
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:08:50.43 - &amp;lt;debug&amp;gt; Created top-level rundir: /run/user/1000/vaccel/73XJNT
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:48.93 - &amp;lt;info&amp;gt; Registered plugin torch 0.2.1-3-0b1978fb
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:48.93 - &amp;lt;debug&amp;gt; Registered op torch_jitload_forward from plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:48.93 - &amp;lt;debug&amp;gt; Registered op torch_sgemm from plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:48.93 - &amp;lt;debug&amp;gt; Registered op image_classify from plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:48.93 - &amp;lt;debug&amp;gt; Loaded plugin torch from libvaccel-torch.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:48.94 - &amp;lt;debug&amp;gt; New rundir for session 1: /run/user/1000/vaccel/73XJNT/session.1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:48.95 - &amp;lt;debug&amp;gt; Initialized session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Initialized session with id: 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:48.97 - &amp;lt;debug&amp;gt; Initialized resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:48.98 - &amp;lt;debug&amp;gt; New rundir for resource 1: /run/user/1000/vaccel/73XJNT/resource.1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:49.19 - &amp;lt;debug&amp;gt; Downloading https://s3.nbfc.io/torch/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:55.17 - &amp;lt;debug&amp;gt; Downloaded: 816.6 KB of 13.7 MB (5.8%) | Speed: 137.30 KB/sec
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:57.71 - &amp;lt;debug&amp;gt; Downloaded: 13.7 MB of 13.7 MB (100.0%) | Speed: 1.62 MB/sec
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:57.73 - &amp;lt;debug&amp;gt; Download completed successfully
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:57.87 - &amp;lt;debug&amp;gt; session:1 Registered resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:57.88 - &amp;lt;debug&amp;gt; session:1 Looking for plugin implementing VACCEL_OP_IMAGE_CLASSIFY
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:57.88 - &amp;lt;debug&amp;gt; Returning func from hint plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:57.88 - &amp;lt;debug&amp;gt; Found implementation in torch plugin
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:09:57.88 - &amp;lt;debug&amp;gt; [torch] Loading model from /run/user/1000/vaccel/73XJNT/resource.1/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;CUDA not available, running in CPU mode
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:31.42 - &amp;lt;debug&amp;gt; [torch] Prediction: banana
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;classification tags: banana
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;classification imagename: PLACEHOLDER
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:31.93 - &amp;lt;debug&amp;gt; session:1 Unregistered resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:31.93 - &amp;lt;debug&amp;gt; Removing file /run/user/1000/vaccel/73XJNT/resource.1/mobilenet.pt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:31.94 - &amp;lt;debug&amp;gt; Released resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:31.95 - &amp;lt;debug&amp;gt; Released session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:44.12 - &amp;lt;debug&amp;gt; Cleaning up vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:44.12 - &amp;lt;debug&amp;gt; Cleaning up sessions
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:44.12 - &amp;lt;debug&amp;gt; Cleaning up resources
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:44.12 - &amp;lt;debug&amp;gt; Cleaning up plugins
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.07.10-20:11:44.12 - &amp;lt;debug&amp;gt; Unregistered plugin torch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== HEAP SUMMARY:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== in use at exit: 306,616 bytes in 3,294 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== total heap usage: 3,167,511 allocs, 3,164,217 frees, 533,893,229 bytes allocated
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== LEAK SUMMARY:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== definitely lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== indirectly lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== possibly lost: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== still reachable: 0 bytes in 0 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== suppressed: 306,616 bytes in 3,294 blocks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== For lists of detected and suppressed errors, rerun with: -s
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;==1657== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 3153 from 3153)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;+ set +x
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and the actual test in Figure 3, taking 8 minutes, almost 7 times faster than
the original execution:&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-figure-3-fixed-arm64-valgrind-test"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/images/torch_run_arm64-fixed.png" alt="Figure 3: Fixed arm64 valgrind test" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 3: Fixed arm64 valgrind test
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id="wrapping-up"&gt;Wrapping Up&lt;/h2&gt;
&lt;p&gt;This experience was a great reminder that debugging tools and parallel
workloads don’t always play nicely, especially on less mature platforms.
Sometimes, the humble Raspberry Pi will leave a high-end chip in the dust,
at least when &lt;code&gt;Valgrind&lt;/code&gt; is in the mix.&lt;/p&gt;
&lt;p&gt;So next time you’re staring at a progress bar that refuses to budge, remember:
more cores might just mean more waiting. And don’t be afraid to try your tests
on the &amp;ldquo;little guy&amp;rdquo; &amp;ndash; you might be surprised by what you find.&lt;/p&gt;</description></item><item><title>“It’s just localhost. How slow could it be?”</title><link>/blog/ttrpc-tcp/</link><pubDate>Sun, 22 Jun 2025 08:02:51 +0100</pubDate><guid>/blog/ttrpc-tcp/</guid><description>&lt;p&gt;That’s what we thought when setting up a BERT-based hate speech classifier.
This was part of a broader experiment using
&lt;a href="https://github.com/nubificus/vaccel" target="_blank" rel="noopener"&gt;&lt;code&gt;vAccel&lt;/code&gt;&lt;/a&gt;, our hardware acceleration
abstraction for AI inference across the Cloud-Edge-IoT continuum.&lt;/p&gt;
&lt;p&gt;We had offloading working locally (on the same physical host &amp;amp; OS),
and started experimenting with our &lt;a href="https://docs.vaccel.org/latest/plugins/available-plugins/transport-plugins/rpc-plugin" target="_blank" rel="noopener"&gt;transport
plugins&lt;/a&gt;
to make sure everything works smoothly so that we can deploy that as part of a
distributed kubernetes setup. First thing to try was localhost, and we expected
communication to be lightning fast. Instead, we got&amp;hellip; surprises.&lt;/p&gt;
&lt;h2 id="the-original-experiment"&gt;The original experiment&lt;/h2&gt;
&lt;h3 id="how-bert-works"&gt;How BERT works&lt;/h3&gt;
&lt;p&gt;The BERT model (Bidirectional Encoder Representations from Transformers) is a
transformer-based architecture that maps input text to contextual embeddings.
In this example, we’re using a distilled BERT checkpoint, traced via
TorchScript (&lt;code&gt;cnn_trace.pt&lt;/code&gt;), to classify short tweets into three categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;offensive-language&lt;/li&gt;
&lt;li&gt;hate-speech&lt;/li&gt;
&lt;li&gt;neither&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each line of input (a tweet) goes through the following stages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Tokenization&lt;/em&gt;
The tweet is split into word/subword tokens using a predefined vocabulary and
tokenizer (e.g. WordPiece). Each token is mapped to an integer ID.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Embedding + Encoding&lt;/em&gt;
These token IDs are passed through BERT’s embedding layer and several
transformer encoder blocks, generating context-aware representations of each
token.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Classification Head&lt;/em&gt;
For classification, we only use the embedding of the special [CLS] token (added
at the start). This vector is fed into a small feed forward layer that outputs
logits for each class.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Prediction&lt;/em&gt;
The class with the highest logit is selected as the prediction.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The model is serialized with TorchScript so that it can be loaded and run from
C++ or via runtime frameworks like vAccel. This avoids Python overhead and
allows seamless execution across backends (CPU, CUDA, remote offload, etc.).&lt;/p&gt;
&lt;p&gt;So if we run this on a subset of an &lt;a href="https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset" target="_blank" rel="noopener"&gt;example
dataset&lt;/a&gt;
and see the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; ./build-stock-cpu/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Processing 100 lines from: build-local/tweets_100.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;== [Vocab Loaded] ==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 5: Duration: 141.214 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 6: Duration: 115.528 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 7: Duration: 117.649 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 10: Duration: 69.3163 ms Prediction: neither
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 99: Duration: 117.06 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 100: Duration: 110.692 ms Prediction: hate-speech
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Average (after 4rd iteration): 92.07 ms
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;CPU execution on such models seems to take quite some time. If you enable GPU execution we get something really better:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; ./build-stock/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Processing 100 lines from: build-local/tweets_100.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;== [Vocab Loaded] ==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;== [Using GPU] ==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 5: Duration: 7.98571 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 6: Duration: 7.81541 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 7: Duration: 7.76802 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 10: Duration: 8.22414 ms Prediction: neither
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 99: Duration: 7.76586 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 100: Duration: 7.8277 ms Prediction: hate-speech
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Average (after 4rd iteration): 7.88 ms
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="how-vaccel-facilitates-the-execution"&gt;How vAccel facilitates the execution&lt;/h3&gt;
&lt;p&gt;vAccel enables seamless interchange between &lt;a href="https://docs.vaccel.org/latest/plugins/available-plugins/acceleration-plugins/" target="_blank" rel="noopener"&gt;hardware&lt;/a&gt; and &lt;a href="https://docs.vaccel.org/latest/plugins/available-plugins/transport-plugins/rpc-plugin/" target="_blank" rel="noopener"&gt;transport&lt;/a&gt; plugins at
runtime. So given a port of this classifier to consume the vAccel API, all we
need to do is configure vAccel to use the CPU or the GPU plugin at runtime.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_LOG_LEVEL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_PLUGINS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;/vaccel-plugin-torch/build-cpu/src/libvaccel-torch.so
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; ./build-local/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-13:44:38.82 - &amp;lt;info&amp;gt; vAccel 0.7.0-7-e67e52b6
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-13:44:38.82 - &amp;lt;info&amp;gt; Registered plugin torch 0.2.0-2-f80dd939-dirty
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Processing 100 lines from: build-local/tweets_100.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;== [Vocab Loaded] ==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-13:44:38.86 - &amp;lt;warn&amp;gt; Path does not seem to have a `&amp;lt;prefix&amp;gt;://`
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-13:44:38.86 - &amp;lt;warn&amp;gt; Assuming build-local/cnn_trace.pt is a local path
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Created new model resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Initialized vAccel session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 5: Duration: 142.267 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 6: Duration: 116.333 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 7: Duration: 117.776 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 10: Duration: 69.8447 ms Prediction: neither
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 99: Duration: 118.195 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 100: Duration: 111.499 ms Prediction: hate-speech
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Average (after 4rd iteration): 92.41 ms
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and the equivalent GPU execution by just tweaking an environment variable:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_LOG_LEVEL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_PLUGINS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;/vaccel-plugin-torch/build-gpu/src/libvaccel-torch.so
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; ./build-local/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-13:50:09.38 - &amp;lt;info&amp;gt; vAccel 0.7.0-7-e67e52b6
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-13:50:09.39 - &amp;lt;info&amp;gt; Registered plugin torch 0.2.0-2-f80dd939
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Processing 100 lines from: build-local/tweets_100.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;== [Vocab Loaded] ==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-13:50:09.43 - &amp;lt;warn&amp;gt; Path does not seem to have a `&amp;lt;prefix&amp;gt;://`
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-13:50:09.43 - &amp;lt;warn&amp;gt; Assuming build-local/cnn_trace.pt is a local path
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Created new model resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Initialized vAccel session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;CUDA is available, switching to GPU mode
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 5: Duration: 8.38249 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 6: Duration: 8.20436 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 7: Duration: 8.13868 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 10: Duration: 8.22329 ms Prediction: neither
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 99: Duration: 8.17031 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 100: Duration: 8.1666 ms Prediction: hate-speech
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Average (after 4rd iteration): 8.27 ms
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="table-summary-of-results"&gt;Table Summary of Results:&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;First inference (cold start)&lt;/th&gt;
&lt;th&gt;Average inference time [*]&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Stock PyTorch (CPU)&lt;/td&gt;
&lt;td&gt;531.083 ms&lt;/td&gt;
&lt;td&gt;92.07 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vAccel (CPU)&lt;/td&gt;
&lt;td&gt;532.026 ms&lt;/td&gt;
&lt;td&gt;92.41 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stock PyTorch (GPU)&lt;/td&gt;
&lt;td&gt;507.915 ms&lt;/td&gt;
&lt;td&gt;7.88 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vAccel (GPU)&lt;/td&gt;
&lt;td&gt;643.077 ms&lt;/td&gt;
&lt;td&gt;8.27 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;[*] &lt;em&gt;Excludes the first 4 lines to avoid cold-start effects.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So far so good. The overhead is specific, and relevant to the library calls we
do under the hood in vAccel and the actual copy of data when needed.&lt;/p&gt;
&lt;h3 id="remote-execution"&gt;Remote execution&lt;/h3&gt;
&lt;p&gt;Given we can run this remotely over vAccel, we first test the execution on a
single node, for the sake of debugging.&lt;/p&gt;
&lt;p&gt;First we spawn the agent:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_PLUGINS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;/vaccel-plugin-torch/build-cpu/src/libvaccel-torch.so
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; vaccel-rpc-agent -a unix:///tmp/bert.sock
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:01:10Z INFO ttrpc::sync::server] server listen started
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:01:10Z INFO ttrpc::sync::server] server started
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:01:10Z INFO vaccel_rpc_agent] vAccel RPC agent started
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:01:10Z INFO vaccel_rpc_agent] Listening on &amp;#39;unix:///tmp/bert.sock&amp;#39;, press Ctrl+C to exit
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then we specify the &lt;code&gt;RPC&lt;/code&gt; plugin and point it to where the agent listens:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_PLUGINS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;libvaccel-rpc.so
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_RPC_ADDRESS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;unix:///tmp/bert.sock
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; ./build-local/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:02:36.25 - &amp;lt;info&amp;gt; vAccel 0.7.0-7-e67e52b6
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:02:36.27 - &amp;lt;info&amp;gt; Registered plugin rpc 0.2.0-1-eca9e440
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Processing 100 lines from: build-local/tweets_100.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;== [Vocab Loaded] ==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:02:36.31 - &amp;lt;warn&amp;gt; Path does not seem to have a `&amp;lt;prefix&amp;gt;://`
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:02:36.31 - &amp;lt;warn&amp;gt; Assuming build-local/cnn_trace.pt is a local path
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Created new model resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Initialized vAccel session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 5: Duration: 147.155 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 6: Duration: 115.631 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 7: Duration: 124.443 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 10: Duration: 68.7716 ms Prediction: neither
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 99: Duration: 123.694 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 100: Duration: 112.011 ms Prediction: hate-speech
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Average (after 4rd iteration): 92.29 ms
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and the GPU equivalent execution:&lt;/p&gt;
&lt;p&gt;Agent:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_PLUGINS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;/vaccel-plugin-torch/build-gpu/src/libvaccel-torch.so
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; vaccel-rpc-agent -a unix:///tmp/bert.sock
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:27:16Z INFO ttrpc::sync::server] server listen started
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:27:16Z INFO ttrpc::sync::server] server started
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:27:16Z INFO vaccel_rpc_agent] vAccel RPC agent started
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:27:16Z INFO vaccel_rpc_agent] Listening on &amp;#39;unix:///tmp/bert.sock&amp;#39;, press Ctrl+C to exit
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And the classifier:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; ./build-local/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:27:28.47 - &amp;lt;info&amp;gt; vAccel 0.7.0-7-e67e52b6
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:27:28.49 - &amp;lt;info&amp;gt; Registered plugin rpc 0.2.0-1-eca9e440
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Processing 100 lines from: build-local/tweets_100.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;== [Vocab Loaded] ==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:27:28.53 - &amp;lt;warn&amp;gt; Path does not seem to have a `&amp;lt;prefix&amp;gt;://`
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:27:28.53 - &amp;lt;warn&amp;gt; Assuming build-local/cnn_trace.pt is a local path
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Created new model resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Initialized vAccel session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 5: Duration: 8.62251 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 6: Duration: 8.18224 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 7: Duration: 8.40555 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 10: Duration: 8.74234 ms Prediction: neither
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 99: Duration: 8.26042 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 100: Duration: 8.38511 ms Prediction: hate-speech
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Average (after 4rd iteration): 8.36 ms
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can see the overhead is negligible, almost identical to the local execution.
This is expected.&lt;/p&gt;
&lt;p&gt;When we do that over a &lt;code&gt;TCP&lt;/code&gt; socket though, we see a completely different result.&lt;/p&gt;
&lt;p&gt;Again, we spawn the agent:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; vaccel-rpc-agent -a tcp://0.0.0.0:8192
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:06:45Z INFO ttrpc::sync::server] server listen started
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:06:45Z INFO ttrpc::sync::server] server started
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:06:45Z INFO vaccel_rpc_agent] vAccel RPC agent started
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:06:45Z INFO vaccel_rpc_agent] Listening on &amp;#39;tcp://0.0.0.0:8192&amp;#39;, press Ctrl+C to exit
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and point the &lt;code&gt;RPC&lt;/code&gt; plugin to the &lt;code&gt;TCP&lt;/code&gt; endpoint:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_RPC_ADDRESS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;tcp://localhost:8192
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; ./build-local/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:07:29.84 - &amp;lt;info&amp;gt; vAccel 0.7.0-7-e67e52b6
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:07:29.90 - &amp;lt;info&amp;gt; Registered plugin rpc 0.2.0-1-eca9e440
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Processing 100 lines from: build-local/tweets_100.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;== [Vocab Loaded] ==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:07:29.95 - &amp;lt;warn&amp;gt; Path does not seem to have a `&amp;lt;prefix&amp;gt;://`
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:07:29.95 - &amp;lt;warn&amp;gt; Assuming build-local/cnn_trace.pt is a local path
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Created new model resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Initialized vAccel session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 5: Duration: 187.425 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 6: Duration: 156.973 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 7: Duration: 164.083 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 10: Duration: 109.588 ms Prediction: neither
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 99: Duration: 159.926 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 100: Duration: 149.851 ms Prediction: hate-speech
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Average (after 4rd iteration): 131.94 ms
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We see some overhead, which we could easily account to the network stack
(~130ms vs 90ms). Still a bit high, but one could mistake that for TCP/IP stack
traversals. However, if we do that using the GPU plugin, things get really
weird!&lt;/p&gt;
&lt;p&gt;Agent spawn:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_PLUGINS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$HOME&lt;/span&gt;/vaccel-plugin-torch/build-gpu/src/libvaccel-torch.so
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; vaccel-rpc-agent -a tcp://0.0.0.0:8192
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:09:56Z INFO ttrpc::sync::server] server listen started
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:09:56Z INFO ttrpc::sync::server] server started
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:09:56Z INFO vaccel_rpc_agent] vAccel RPC agent started
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:09:56Z INFO vaccel_rpc_agent] Listening on &amp;#39;tcp://0.0.0.0:8192&amp;#39;, press Ctrl+C to exit
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_RPC_ADDRESS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;tcp://localhost:8192
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; ./build-local/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:10:00.40 - &amp;lt;info&amp;gt; vAccel 0.7.0-7-e67e52b6
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:10:00.42 - &amp;lt;info&amp;gt; Registered plugin rpc 0.2.0-1-eca9e440
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Processing 100 lines from: build-local/tweets_100.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;== [Vocab Loaded] ==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:10:00.46 - &amp;lt;warn&amp;gt; Path does not seem to have a `&amp;lt;prefix&amp;gt;://`
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:10:00.46 - &amp;lt;warn&amp;gt; Assuming build-local/cnn_trace.pt is a local path
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Created new model resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Initialized vAccel session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 5: Duration: 1598.79 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 6: Duration: 50.5106 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 7: Duration: 1896.81 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 10: Duration: 90.4444 ms Prediction: neither
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 99: Duration: 90.4848 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 100: Duration: 90.3126 ms Prediction: hate-speech
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Average (after 4rd iteration): 124.18 ms
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="updated-table-summary-of-results"&gt;Updated Table Summary of Results:&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Average inference time [*]&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Stock PyTorch (CPU)&lt;/td&gt;
&lt;td&gt;92.07 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vAccel (CPU)&lt;/td&gt;
&lt;td&gt;92.41 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stock PyTorch (GPU)&lt;/td&gt;
&lt;td&gt;7.88 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vAccel (GPU)&lt;/td&gt;
&lt;td&gt;8.27 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vAccel remote &lt;code&gt;UNIX&lt;/code&gt; (CPU)&lt;/td&gt;
&lt;td&gt;92.29 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vAccel remote &lt;code&gt;UNIX&lt;/code&gt; (GPU)&lt;/td&gt;
&lt;td&gt;8.36 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vAccel remote &lt;code&gt;TCP&lt;/code&gt; (CPU)&lt;/td&gt;
&lt;td&gt;131.94 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vAccel remote &lt;code&gt;TCP&lt;/code&gt; (GPU)&lt;/td&gt;
&lt;td&gt;124.18 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;How would that be even possible? 124 ms vs 8.36 ms for the GPU execution?&lt;/p&gt;
&lt;h2 id="a-curious-latency-bump"&gt;A Curious Latency Bump&lt;/h2&gt;
&lt;p&gt;When calling the BERT classifier from the client via &lt;code&gt;ttrpc&lt;/code&gt;, we saw minimal
overhead for &lt;code&gt;UNIX&lt;/code&gt; sockets (&amp;lt;~5%) compared to native execution. This is
expected as the data path for RPC in vAccel uses copies. When running over TCP
sockets, we saw an almost &lt;strong&gt;10x difference&lt;/strong&gt; (~100ms vs ~10ms). This got us
thinking what could have gone wrong in the plugin&amp;hellip; The code is identical, the
only thing that is different is the kind of socket&amp;hellip;&lt;/p&gt;
&lt;p&gt;The classifier code itself was fast (inference ~7-9ms), over &lt;code&gt;UNIX&lt;/code&gt; sockets it
was almost the same (~9-10ms); but with TCP sockets we were getting ~100ms.&lt;/p&gt;
&lt;h2 id="setting-the-stage"&gt;Setting the Stage&lt;/h2&gt;
&lt;p&gt;We isolated the issue to the transport mechanism (&lt;code&gt;ttrpc-rust&lt;/code&gt;) so we wrote a
small microbenchmark: a &lt;code&gt;ttrpc&lt;/code&gt; program exchanging empty &lt;code&gt;protobuf&lt;/code&gt; messages in a
tight loop.&lt;/p&gt;
&lt;p&gt;The goal was to measure raw round-trip latency—no ML, no I/O, just the
transport.&lt;/p&gt;
&lt;p&gt;You can find the benchmark here:
&lt;a href="https://github.com/nubificus/ttrpc-rs-benchmark" target="_blank" rel="noopener"&gt;ttrpc-rs-benchmark&lt;/a&gt;&lt;/p&gt;
&lt;h2 id="reproducing-the-problem"&gt;Reproducing the Problem&lt;/h2&gt;
&lt;p&gt;Here&amp;rsquo;s how we tested:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; git clone https://github.com/nubificus/ttrpc-rs-benchmark
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; &lt;span class="nb"&gt;cd&lt;/span&gt; ttrpc-rs-benchmark
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; cargo build --release
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; ./target/release/ttrpc-benchmark
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Running ttrpc-rust latency benchmark with 1000 iterations...
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Testing Unix sockets...
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Unix Socket Results:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt; Min: 58.029µs
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt; Average: 73.217µs
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt; Max: 887.728µs
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt; P99: 116.979µs
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Testing TCP sockets...
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;TCP Socket Results:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt; Min: 81.40514ms
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt; Average: 82.004085ms
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt; Max: 83.021974ms
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt; P99: 82.465309ms
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Comparison:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt; Unix sockets are 1120.01x faster than TCP
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That’s even worse that what we&amp;rsquo;ve seen with vAccel.&lt;/p&gt;
&lt;h2 id="the-nagle-surprise"&gt;The Nagle Surprise&lt;/h2&gt;
&lt;p&gt;We generated flamegraphs for both paths, and one thing stood out: &lt;code&gt;send()&lt;/code&gt; was
stalling on the TCP path.&lt;/p&gt;
&lt;p&gt;Digging deeper, we figured out the culprit: &lt;strong&gt;Nagle&amp;rsquo;s algorithm&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Nagle tries to reduce small-packet overhead by coalescing writes; but that’s
poison for latency-sensitive communication over TCP. Especially when the
protocol uses small messages.&lt;/p&gt;
&lt;h2 id="disabling-nagle--without-touching-code"&gt;Disabling Nagle — Without Touching Code&lt;/h2&gt;
&lt;p&gt;Unfortunately, &lt;code&gt;ttrpc-rust&lt;/code&gt; does not expose a socket config option. But we
prefer not to patch the library.&lt;/p&gt;
&lt;p&gt;So we wrote a preloadable shared object, &lt;code&gt;nodelay.so&lt;/code&gt;, that intercepts &lt;code&gt;socket()&lt;/code&gt; and &lt;code&gt;setsockopt()&lt;/code&gt; to enforce &lt;code&gt;TCP_NODELAY&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can get it from &lt;a href="https://github.com/nubificus/nodelay" target="_blank" rel="noopener"&gt;here&lt;/a&gt;, build it like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;git clone https://github.com/nubificus/nodelay
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; nodelay
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;make
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and preload it like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nv"&gt;LD_PRELOAD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;./nodelay.so ./target/release/ttrpc-benchmark
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This reduces the TCP latency and brings it in par with &lt;code&gt;UNIX&lt;/code&gt;-socket latency.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; &lt;span class="nv"&gt;LD_PRELOAD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;../nodelay/nodelay.so ./target/release/ttrpc-benchmark
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Running ttrpc-rust latency benchmark with 1000 iterations...
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Testing Unix sockets...
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Unix Socket Results:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt; Min: 55.985µs
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt; Average: 69.387µs
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt; Max: 373.772µs
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt; P99: 101.441µs
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Testing TCP sockets...
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[hook] TCP_NODELAY enabled on socket 16
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[hook] TCP_NODELAY enabled on socket 12
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;TCP Socket Results:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt; Min: 81.323µs
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt; Average: 92.432µs
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt; Max: 420.38µs
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt; P99: 126.688µs
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Comparison:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt; Unix sockets are 1.33x faster than TCP
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="running-the-original-example-using-the-nodelayso-hack"&gt;Running the original example using the &lt;code&gt;nodelay.so&lt;/code&gt; hack&lt;/h2&gt;
&lt;p&gt;Keeping the same settings as our last execution attempt, only now using the &lt;code&gt;nodelay.so&lt;/code&gt;, we see the following:&lt;/p&gt;
&lt;p&gt;Agent spawn:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; &lt;span class="nv"&gt;LD_PRELOAD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;../nodelay/nodelay.so vaccel-rpc-agent -a tcp://0.0.0.0:8192
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[hook] TCP_NODELAY enabled on socket 3
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:14:49Z INFO ttrpc::sync::server] server listen started
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:14:49Z INFO ttrpc::sync::server] server started
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:14:49Z INFO vaccel_rpc_agent] vAccel RPC agent started
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[2025-06-22T15:14:49Z INFO vaccel_rpc_agent] Listening on &amp;#39;tcp://0.0.0.0:8192&amp;#39;, press Ctrl+C to exit
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; &lt;span class="nv"&gt;LD_PRELOAD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;../nodelay/nodelay.so ./build-local/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:16:19.66 - &amp;lt;info&amp;gt; vAccel 0.7.0-7-e67e52b6
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:16:19.68 - &amp;lt;info&amp;gt; Registered plugin rpc 0.2.0-1-eca9e440
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Processing 100 lines from: build-local/tweets_100.txt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;== [Vocab Loaded] ==
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:16:19.72 - &amp;lt;warn&amp;gt; Path does not seem to have a `&amp;lt;prefix&amp;gt;://`
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2025.06.22-15:16:19.72 - &amp;lt;warn&amp;gt; Assuming build-local/cnn_trace.pt is a local path
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Created new model resource 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[hook] TCP_NODELAY enabled on socket 3
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Initialized vAccel session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 5: Duration: 8.61344 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 6: Duration: 8.4136 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 7: Duration: 8.69745 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 10: Duration: 8.75759 ms Prediction: neither
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 99: Duration: 8.80859 ms Prediction: offensive-language
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Line 100: Duration: 8.30694 ms Prediction: hate-speech
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Average (after 4rd iteration): 8.50 ms
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="final-table-summary-of-results"&gt;Final Table Summary of Results:&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Configuration&lt;/th&gt;
&lt;th&gt;Average inference time [*]&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Stock PyTorch (CPU)&lt;/td&gt;
&lt;td&gt;92.07 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vAccel (CPU)&lt;/td&gt;
&lt;td&gt;92.41 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Stock PyTorch (GPU)&lt;/td&gt;
&lt;td&gt;7.88 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vAccel (GPU)&lt;/td&gt;
&lt;td&gt;8.27 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vAccel remote &lt;code&gt;UNIX&lt;/code&gt; (CPU)&lt;/td&gt;
&lt;td&gt;92.29 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vAccel remote &lt;code&gt;UNIX&lt;/code&gt; (GPU)&lt;/td&gt;
&lt;td&gt;8.36 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vAccel remote &lt;code&gt;TCP&lt;/code&gt; (CPU)&lt;/td&gt;
&lt;td&gt;131.94 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vAccel remote &lt;code&gt;TCP&lt;/code&gt; (GPU)&lt;/td&gt;
&lt;td&gt;124.18 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vAccel remote &lt;code&gt;TCP_NODELAY&lt;/code&gt; (CPU)&lt;/td&gt;
&lt;td&gt;92.55 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vAccel remote &lt;code&gt;TCP_NODELAY&lt;/code&gt; (GPU)&lt;/td&gt;
&lt;td&gt;8.50 ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="takeaways"&gt;Takeaways&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Even on localhost, &lt;code&gt;TCP&lt;/code&gt; can be surprisingly slow if Nagle&amp;rsquo;s algorithm is enabled.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;UNIX&lt;/code&gt; sockets avoid these issues entirely; but come with deployment trade-offs.&lt;/li&gt;
&lt;li&gt;Flamegraphs are a powerful way to uncover unexpected bottlenecks.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LD_PRELOAD&lt;/code&gt; is still a useful hack for tweaking behaviors without code changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For AI workloads that rely on tight client-server loops (like ML inference
offloading), these small optimizations matter.&lt;/p&gt;
&lt;h2 id="resources"&gt;Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/nubificus/ttrpc-rs-benchmark" target="_blank" rel="noopener"&gt;ttrpc-rs-benchmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nubificus/nodelay" target="_blank" rel="noopener"&gt;&lt;code&gt;nodelay.so&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://brooker.co.za/blog/2024/05/09/nagle.html" target="_blank" rel="noopener"&gt;Dan Brooker&amp;rsquo;s excellent post on Nagle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nubificus/vaccel" target="_blank" rel="noopener"&gt;vAccel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/brendangregg/Flamegraph" target="_blank" rel="noopener"&gt;flamegraph&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="appendix-i--flamegraphs"&gt;Appendix I &amp;ndash; Flamegraphs&lt;/h2&gt;
&lt;p&gt;To produce a flamegraph follow these steps:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;sudo apt install linux-tools-common linux-tools-&lt;span class="k"&gt;$(&lt;/span&gt;uname -r&lt;span class="k"&gt;)&lt;/span&gt; linux-tools-generic
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;git clone https://github.com/brendangregg/Flamegraph
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; Flamegraph
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;sudo perf record -F &lt;span class="m"&gt;99&lt;/span&gt; -g -- ./my-program --arg1 myarg1 etc..
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;sudo perf script &amp;gt; out.perf
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;./stackcollapse-perf.pl out.perf &amp;gt; out.folded
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;./flamegraph.pl out.folded &amp;gt; flamegraph.svg
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="appendix-ii--hardware-testbed"&gt;Appendix II &amp;ndash; Hardware Testbed&lt;/h2&gt;
&lt;h3 id="hardware-testbed-summary"&gt;Hardware Testbed Summary&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Component&lt;/th&gt;
&lt;th&gt;Specification&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;OS&lt;/td&gt;
&lt;td&gt;Ubuntu 24.04.2 LTS (noble)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CPU&lt;/td&gt;
&lt;td&gt;AMD Ryzen 5 2600, 6 cores / 12 threads @ 3.4 GHz&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RAM&lt;/td&gt;
&lt;td&gt;64 GB DDR4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GPU&lt;/td&gt;
&lt;td&gt;NVIDIA GeForce RTX 2060 SUPER (8 GB GDDR6)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CUDA Toolkit&lt;/td&gt;
&lt;td&gt;12.0 (nvcc 12.0.140)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CUDA Driver&lt;/td&gt;
&lt;td&gt;12.8 (Driver Version: 570.133.20)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GPU Usage&lt;/td&gt;
&lt;td&gt;vaccel-rpc-agent (228 MiB GPU memory used during tests)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="description"&gt;Description&lt;/h3&gt;
&lt;p&gt;All benchmarks were executed on a modern workstation equipped with a 6-core AMD
Ryzen CPU and 64 GB of system memory. For GPU-accelerated runs, the machine
used an NVIDIA RTX 2060 SUPER with 8 GB of VRAM, running CUDA 12.8. The vAccel
RPC agent utilized a small portion of GPU memory during execution. Tests were
conducted under Ubuntu 24.04.2 with minimal background processes to ensure
measurement consistency.&lt;/p&gt;
&lt;h2 id="appendix-iii--ttrpc-rust--tiny-transport-rpc-in-rust"&gt;Appendix III &amp;ndash; &lt;code&gt;ttrpc-rust&lt;/code&gt; – Tiny Transport RPC in Rust&lt;/h2&gt;
&lt;p&gt;As part of the latency benchmarking setup, we used
&lt;a href="https://github.com/containerd/ttrpc-rust" target="_blank" rel="noopener"&gt;&lt;code&gt;ttrpc-rust&lt;/code&gt;&lt;/a&gt;, a minimalist
transport abstraction for low-latency RPC-style communication. &lt;code&gt;ttrpc-rust&lt;/code&gt; is
the &lt;code&gt;Rust&lt;/code&gt; version of &lt;a href="https://github.com/containerd/ttrpc" target="_blank" rel="noopener"&gt;&lt;code&gt;ttrpc&lt;/code&gt;&lt;/a&gt;. &lt;code&gt;ttrpc&lt;/code&gt;
is &lt;a href="https://grpc.io/" target="_blank" rel="noopener"&gt;GRPC&lt;/a&gt; for low-memory environments.&lt;/p&gt;
&lt;p&gt;By default, ttrpc-rust supports:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unix domain sockets (fast, local IPC)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;AF_VSOCK&lt;/code&gt; sockets&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For our experiments, we extended it in a &lt;a href="https://github.com/nubificus/ttrpc-rust/tree/0.8.0%2Bvaccel" target="_blank" rel="noopener"&gt;downstream
fork&lt;/a&gt; to also
support &lt;code&gt;TCP&lt;/code&gt; sockets. In addition to this, we also added an environment
variable to control the &lt;code&gt;TCP_NODELAY&lt;/code&gt; feature, so that we don&amp;rsquo;t have to do the
&lt;code&gt;nodelay.so&lt;/code&gt; hack. Use with the following variable set:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-gdscript3" data-lang="gdscript3"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="n"&gt;TTRPC_TCP_NODELAY_ENABLED&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description></item><item><title>Fosscomm 2024</title><link>/blog/fosscomm-2024/</link><pubDate>Mon, 11 Nov 2024 08:10:38 +0000</pubDate><guid>/blog/fosscomm-2024/</guid><description>&lt;p&gt;If you’re into open-source and tech meetups, FOSSCOMM is the event to be at. This year, it was held in Thessaloniki, organized by the open-source community of the University of Macedonia (UoM), and they absolutely crushed it with the setup and vibe!&lt;/p&gt;
&lt;figure id="figure-main-entrance----uom-building"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/images/fosscomm2.jpg#center" alt="Main Entrance -- UoM building" loading="lazy" data-zoomable width="50%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Main Entrance &amp;ndash; UoM building
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The atmosphere was awesome—super inclusive and welcoming, with folks from all over Greece and beyond coming together to celebrate open-source software. Whether you were a newcomer or a seasoned developer, everyone felt part of the community, sharing knowledge, projects, and ideas.&lt;/p&gt;
&lt;p&gt;We were lucky to give two talks at this year’s FOSSCOMM! The first one was about k8s, CRDs, and how we’re using them to tailor k8s clusters to fit our projects&amp;rsquo; needs. If you’re curious, here’s the &lt;a href="https://s3.nephos.gr/fosscomm/2024/FOSSCOMM-2024-NUBIS-k8s-crd.pdf" target="_blank" rel="noopener"&gt;slide deck&lt;/a&gt; for that session. We also demoed some of our work—check out the video &lt;a href="https://docs.google.com/file/d/1KJ5ikE4ndNd1TPvWpjCBKVfhscbKMBGk/preview" target="_blank" rel="noopener"&gt;here&lt;/a&gt;. Additionally, we built a step-by-step tutorial for this at &lt;a href="https://killercoda.com/nubis/scenario/CRD_Etherpad" target="_blank" rel="noopener"&gt;&lt;code&gt;Killercoda&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;figure id="figure-panos-mavrikos-talking-about-k8s-operators--crds"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/images/fosscomm1.jpg#center" alt="Panos Mavrikos talking about k8s operators &amp; CRDs" loading="lazy" data-zoomable width="50%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Panos Mavrikos talking about k8s operators &amp;amp; CRDs
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The second talk tackled sandboxing workloads in cloud-edge environments, exploring flexibility and performance, the role of hardware accelerators, and how these all impact cloud-edge use cases. It was an exciting session, sparking lots of ideas around security and efficiency. Here’s the &lt;a href="https://s3.nephos.gr/fosscomm/2024/FOSSCOMM-2024-NUBIS-hwaccel.pdf" target="_blank" rel="noopener"&gt;slide deck&lt;/a&gt; and three demo videos for this talk: &lt;a href="https://s3.nephos.gr/fosscomm/2024/Bert-torch-vaccel-correct.mp4" target="_blank" rel="noopener"&gt;Demo 1&lt;/a&gt;, &lt;a href="https://s3.nephos.gr/fosscomm/2024/SOL-torch-vaccel.mp4" target="_blank" rel="noopener"&gt;Demo 2&lt;/a&gt;, and &lt;a href="https://s3.nephos.gr/fosscomm/2024/vaccel-tvm.mov" target="_blank" rel="noopener"&gt;Demo 3&lt;/a&gt;.&lt;/p&gt;
&lt;figure id="figure-anastassios-nanos-talking-about-hardware-acceleration-in-sandboxed-environments"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/images/fosscomm4.jpg#center" alt="Anastassios Nanos talking about hardware acceleration in sandboxed environments" loading="lazy" data-zoomable width="50%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Anastassios Nanos talking about hardware acceleration in sandboxed environments
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;The program was absolutely packed this year, with tons of insightful sessions for anyone into open-source, cloud, DevOps, education, IoT, and more! Here are just a couple of standout talks from the packed weekend that really captured the spirit of innovation and open knowledge sharing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Lessons from the Unix History&amp;rdquo; by Professor Diomidis Spinellis from AUEB: A must-see for any history and tech lovers, this talk walked us through the evolution of Unix, exploring its massive influence on modern operating systems and open-source culture. If you’ve ever wondered how Unix’s legacy continues to impact today’s software, this was the place to be.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Running a Company on Free Software: The First 20 Years&amp;rdquo; by Apollon Oikonomopoulos, CIO of Skroutz: Apollon shared an inspiring journey of building and managing the core infrastructure of a successful business using free software for over two decades. This talk was especially interesting for anyone wondering about the practicalities of relying on open-source tools in a business setting.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;How to Build a Sustainable Open Source Company&amp;rdquo; by Frank Karlitschek, CEO of NextCloud: Frank brought tons of insight into what it takes to not just create open-source software but to build a thriving, sustainable business around it. This session had great takeaways on balancing open-source ideals with business realities.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;Quo Vadis, Free Software? Quo Vadis, Society?&amp;rdquo; by Luis Falcon, Founder of GNU Solidario: A thought-provoking talk on the societal role of free software, and the vision of a world where open-source tech powers humanitarian causes. This session looked at how free software can impact issues like healthcare and education and was a great reminder of the deeper purpose driving open-source communities.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&amp;ldquo;EdgeAI: A HW-SW Codesign Approach&amp;rdquo; by George Keramidas from Aristotle University of Thessaloniki: This one was a treat for those interested in the cutting-edge (literally!) of AI at the hardware level. George explored the integration of hardware and software for AI at the edge, showing how open-source innovation is shaping this critical area.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure &gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/images/fosscomm3.jpg#floatleft" alt="" loading="lazy" data-zoomable width="50%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;But it wasn’t just about the talks. FOSSCOMM this year was a hub of energy, ideas, and connections! The booth setups were fantastic, ranging from beginner-friendly intros to cutting-edge tech demos. And the people—wow. We met a huge range of enthusiasts, from students trying open-source for the first time to veterans in the field sharing their latest work.&lt;/p&gt;
&lt;p&gt;Big shoutout to the entire organizing team for a smooth, engaging event. Everything ran like clockwork, and you could really tell how much care went into making it a positive, collaborative space. If you’re thinking of attending next year, don’t hesitate—FOSSCOMM always delivers, and we can’t wait to see what’s in store for next time!&lt;/p&gt;
&lt;p&gt;Looking forward to FOSSCOMM 2025!&lt;/p&gt;</description></item><item><title>vAccel-go: Golang bindings for vAccel</title><link>/blog/vaccel_go/</link><pubDate>Tue, 02 Jan 2024 14:28:46 +0000</pubDate><guid>/blog/vaccel_go/</guid><description>&lt;p&gt;To facilitate the use of vAccel, we provide bindings for popular languages,
apart from &lt;code&gt;C&lt;/code&gt;. Essentially, the vAccel &lt;code&gt;C&lt;/code&gt; API can be called from any language
that interacts with &lt;code&gt;C&lt;/code&gt; libraries. Building on this, we are thrilled to present
support for &lt;a href="https://golang.dev" target="_blank" rel="noopener"&gt;Go&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Essentially, the &lt;code&gt;C&lt;/code&gt;/&lt;code&gt;Go&lt;/code&gt; interaction is already pretty smooth, given the
native &lt;code&gt;CGO&lt;/code&gt; support available. We introduce v0.1 of the vAccel-go bindings,
pending a feature-full update in the coming months. In this post, we go
through the initial implementation details, as well as a hands-on tutorial on
how to write your first vAccel program in &lt;code&gt;Go&lt;/code&gt;!&lt;/p&gt;
&lt;h3 id="vaccel-overview"&gt;vAccel overview&lt;/h3&gt;
&lt;p&gt;vAccel is a library for hardware acceleration.
Actually, it is a set of software tools that semantically
expose hardware acceleration functionality to isolated
workloads running on VM sandboxes. For more insights or
examples about vAccel, there is a &lt;a href="https://vaccel.org" target="_blank" rel="noopener"&gt;splash page&lt;/a&gt;,
along with more &lt;a href="https://docs.vaccel.org" target="_blank" rel="noopener"&gt;elaborate documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="golang-overview"&gt;Golang overview&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;Go&lt;/code&gt; Programming Language is designed for scalability,
making it suitable for cloud computing and large-scale servers.
Go enhances development speed and efficiency, since it compiles
quickly (compared to other languages), and provides a great
Standard Library, along with built-in concurrency tools. Golang
is also a cloud-native Programming Language. Information
about Go installation can be found &lt;a href="https://go.dev/doc/install" target="_blank" rel="noopener"&gt;here&lt;/a&gt;,
but there are also instructions on how to install &lt;code&gt;Go&lt;/code&gt; in the
&lt;a href="https://github.com/nubificus/go-vaccel" target="_blank" rel="noopener"&gt;vAccel-go bindings installation guide&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="vaccel-go-package"&gt;vAccel Go package&lt;/h3&gt;
&lt;p&gt;The vaccel package in Golang provides access to vAccel operations, which
can be used by the developers on their own &lt;code&gt;Go&lt;/code&gt; programs. The vaccel package uses
the native &lt;code&gt;C&lt;/code&gt; bindings in order to use the vAccel &lt;code&gt;C&lt;/code&gt; API. The following diagram
demonstrates the functionality of the vaccel package:&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-figure-1-high-level-overview-of-the-vaccel-go-package"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel_go.png" alt="vaccel-go-package" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 1: High-level overview of the vAccel Go package
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id="installation-guide"&gt;Installation Guide&lt;/h3&gt;
&lt;h4 id="vaccel-installation"&gt;vAccel Installation&lt;/h4&gt;
&lt;p&gt;First of all, a vAccelRT installation is required before proceeding to the next sections.&lt;/p&gt;
&lt;h4 id="build-from-source"&gt;Build from source&lt;/h4&gt;
&lt;p&gt;In Ubuntu-based systems, you need to have the following packages to build vaccelrt:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;cmake&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;build-essential&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can install them using the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;sudo apt-get install -y cmake build-essential
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Get the source code for &lt;strong&gt;vaccelrt&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;git clone https://github.com/cloudkernels/vaccelrt --recursive
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Prepare the build directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;cd vaccelrt
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;mkdir build
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;cd build
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id="building-the-core-runtime-library"&gt;Building the core runtime library&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;# This sets the installation path to /usr/local, and the current build
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;# type to &amp;#39;Release&amp;#39;. The other option is the &amp;#39;Debug&amp;#39; build
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;cmake ../ -DCMAKE_INSTALL_PREFIX=/usr/local -DCMAKE_BUILD_TYPE=Release -DBUILD_EXAMPLES=ON -DBUILD_PLUGIN_EXEC=ON -DBUILD_PLUGIN_NOOP=ON
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;make
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;sudo make install
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id="vaccel-go-bindings-installation"&gt;vAccel-Go Bindings Installation&lt;/h4&gt;
&lt;h5 id="go-installation"&gt;Go Installation&lt;/h5&gt;
&lt;p&gt;Of course, prior to installing the bindings, we have to make sure that Golang 1.20 or newer is installed in our system. We can check this using the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;go version
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Otherwise, &lt;code&gt;go 1.20&lt;/code&gt; needs to be installed. You can find instructions on how to install Go &lt;a href="https://go.dev/doc/install" target="_blank" rel="noopener"&gt;here.&lt;/a&gt;&lt;/p&gt;
&lt;h5 id="build-the-bindings-from-source"&gt;Build the Bindings from source&lt;/h5&gt;
&lt;p&gt;Download the source code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;git clone https://github.com/nubificus/go-vaccel.git
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;First, you can build the examples:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-gdscript3" data-lang="gdscript3"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Set vaccel location&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="n"&gt;PKG_CONFIG_PATH&lt;/span&gt;&lt;span class="o"&gt;=/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;share&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;cd&lt;/span&gt; &lt;span class="n"&gt;go&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;vaccel&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;make&lt;/span&gt; &lt;span class="n"&gt;all&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now you have successfully built some vaccel programs using Go. The executables are located in go-vaccel/bin. You can run the &lt;code&gt;noop&lt;/code&gt; example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-gdscript3" data-lang="gdscript3"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="n"&gt;VACCEL_BACKENDS&lt;/span&gt;&lt;span class="o"&gt;=/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;libvaccel&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;noop&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;so&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="o"&gt;./&lt;/span&gt;&lt;span class="n"&gt;bin&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;noop&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Or the &lt;code&gt;exec&lt;/code&gt; example, providing a path for the shared object and an integer:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-gdscript3" data-lang="gdscript3"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="n"&gt;VACCEL_BACKENDS&lt;/span&gt;&lt;span class="o"&gt;=/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;libvaccel&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;exec&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;so&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="o"&gt;./&lt;/span&gt;&lt;span class="n"&gt;bin&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;exec&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;libmytestlib&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;so&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# if everything go as expected, the&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# plugin will probably double the integer &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="tutorial"&gt;Tutorial&lt;/h3&gt;
&lt;p&gt;The following example demonstrates the usage of the &lt;code&gt;vaccel&lt;/code&gt; package to build vaccel-enabled &lt;code&gt;Go&lt;/code&gt; programs. The tutorial will perform an image classification operation, using the no-op plugin.
Keep in mind the following three conditions before building:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Make sure to import the package in your programs:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-go" data-lang="go"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#34;github.com/nubificus/go-vaccel/vaccel&amp;#34;&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;2. Define &lt;code&gt;vaccel&lt;/code&gt; location:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-gdscript3" data-lang="gdscript3"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="n"&gt;PKG_CONFIG_PATH&lt;/span&gt;&lt;span class="o"&gt;=/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;share&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;3. And finally, always define the location of the &lt;code&gt;vaccel-plugin&lt;/code&gt; you are willing to use:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-gdscript3" data-lang="gdscript3"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# In case of No-Op for testing:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="n"&gt;VACCEL_BACKENDS&lt;/span&gt;&lt;span class="o"&gt;=/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;libvaccel&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;noop&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;so&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="example"&gt;Example&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Create the project directory&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;cd ~
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;mkdir go-vaccel-test
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;cd go-vaccel-test
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Initialize the Module&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;go mod init go-vaccel-test
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Download the bindings&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;go get github.com/nubificus/go-vaccel
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;And create a &lt;code&gt;Go&lt;/code&gt; file&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;touch main.go
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Add the following lines to perform Image Classification&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-go" data-lang="go"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;package&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#34;fmt&amp;#34;&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#34;os&amp;#34;&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#34;github.com/nubificus/go-vaccel/vaccel&amp;#34;&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kd"&gt;func&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;&amp;lt;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;Println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#34;Usage: ./main &amp;lt;filename&amp;gt;&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;return&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="cm"&gt;/* Get the filename from command line argument */&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;filePath&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Args&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="cm"&gt;/* Session */&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;session&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;vaccel&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;Session&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;vaccel&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;SessionInit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nx"&gt;session&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;Println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#34;error initializing session&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;Exit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kd"&gt;var&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;outText&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kt"&gt;string&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="cm"&gt;/* Read the image-bytes */&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;imageBytes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;e&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;:=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;ReadFile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;filePath&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;e&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kc"&gt;nil&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;Printf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#34;Error reading file: %s\n&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;Exit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="cm"&gt;/* Perform Image Classification */&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;outText&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;vaccel&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;ImageClassification&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nx"&gt;session&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;imageBytes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;Println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#34;Image Classification failed&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;Exit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;err&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;Println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#34;Output: &amp;#34;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;outText&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="cm"&gt;/* Free Session */&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;vaccel&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;SessionFree&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="nx"&gt;session&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nx"&gt;fmt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nf"&gt;Println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#34;An error occurred while freeing the session&amp;#34;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="w"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Then, specify vaccel location:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-gdscript3" data-lang="gdscript3"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="n"&gt;PKG_CONFIG_PATH&lt;/span&gt;&lt;span class="o"&gt;=/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;share&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Define the location of the plugin:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-gdscript3" data-lang="gdscript3"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="n"&gt;VACCEL_BACKENDS&lt;/span&gt;&lt;span class="o"&gt;=/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;libvaccel&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;noop&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;so&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Build the source file:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;go build main.go
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;And run:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;./main &amp;lt;/path/to/image&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;You must see the following message:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;[noop] Calling Image classification for session 1
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;[noop] Dumping arguments for Image classification:
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;[noop] len_img: &amp;lt;numBytes&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;[noop] will return a dummy result
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;Output: This is a dummy classification tag!
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="conclusion"&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;The above example shows how to use the &lt;code&gt;vaccel&lt;/code&gt; package in &lt;code&gt;Go&lt;/code&gt; to use various vaccel features. As you can see, the example doesn&amp;rsquo;t run an actual image classification operation, since we use the no-op plugin for testing purposes. However, we could use a vaccel backend that performs the operation. &lt;a href="https://github.com/nubificus/go-vaccel/" target="_blank" rel="noopener"&gt;Here&lt;/a&gt;, you can find more vaccel tools and operations that you could possibly use in your &lt;code&gt;Go&lt;/code&gt; programs. For example, except image classification, you can write &lt;a href="https://github.com/nubificus/go-vaccel/blob/main/exec/main.go" target="_blank" rel="noopener"&gt;programs that use the exec plugin&lt;/a&gt;, which gives you the opportunity to use functions contained in a shared object. Or, finally, you could also use the &lt;a href="https://github.com/nubificus/go-vaccel/blob/main/noop/main.go" target="_blank" rel="noopener"&gt;&lt;code&gt;noop&lt;/code&gt; example&lt;/a&gt; if you just want to test the installation of the package.&lt;/p&gt;</description></item><item><title>Isolated, hardware-accelerated functions on Jetson AGX Orin</title><link>/blog/orin-vm-vaccel/</link><pubDate>Sun, 24 Sep 2023 16:31:04 +0100</pubDate><guid>/blog/orin-vm-vaccel/</guid><description>&lt;p&gt;Following up on a successful &lt;a href="/orin-vm"&gt;VM boot&lt;/a&gt; on a &lt;a href="https://www.nvidia.com/en-il/autonomous-machines/embedded-systems/jetson-orin/" target="_blank" rel="noopener"&gt;Jetson AGX
Orin&lt;/a&gt;,
we continue exploring the capabilities of this edge device, focusing on the
cloud-native aspect of application deployment.&lt;/p&gt;
&lt;p&gt;As a team, we&amp;rsquo;ve built &lt;a href="https://docs.vaccel.org" target="_blank" rel="noopener"&gt;vAccel&lt;/a&gt;, a hardware
acceleration framework that decouples the operation from its hardware
implementation. One of the awesome things vAccel offers is the ability to
execute hardware-accelerated applications in a VM that has no direct access to
a hardware accelerator. Given the Jetson Orin board has an Ampere GPU, with
1792 cores and 56 Tensor Cores, it sounds like a perfect edge device to try
isolated hardware-accelerated workload execution through VM sandboxes.&lt;/p&gt;
&lt;p&gt;Additionally, coupled with our downstream &lt;code&gt;kata-containers&lt;/code&gt; port, we can invoke a
VM sandboxed container through CRI that can execute compute-intensive tasks
faster by using the GPU without having direct access to it!&lt;/p&gt;
&lt;h2 id="overview"&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this post we go through the high-level architecture of
&lt;a href="#how-kata-containers-work"&gt;kata-containers&lt;/a&gt; and &lt;a href="#how-vaccel-works"&gt;vAccel&lt;/a&gt;
and provide insights into the &lt;a href="#kata-vaccel"&gt;integration&lt;/a&gt; we did.&lt;/p&gt;
&lt;p&gt;Then, we go through the steps to &lt;a href="#kata-containers-binary-installation"&gt;run a stock kata
container&lt;/a&gt; using various supported VMMs
(&lt;a href="https://github.com/qemu/qemu" target="_blank" rel="noopener"&gt;QEMU&lt;/a&gt;, &lt;a href="https://github.com/firecracker-microvm/firecracker" target="_blank" rel="noopener"&gt;AWS
Firecracker&lt;/a&gt;, &lt;a href="https://github.com/cloud-hypervisor/cloud-hypervisor" target="_blank" rel="noopener"&gt;Cloud
hypervisor&lt;/a&gt;, and
&lt;a href="https://github.com/kata-containers/kata-containers/" target="_blank" rel="noopener"&gt;Dragonball&lt;/a&gt;) on the
Jetson AGX Orin board.&lt;/p&gt;
&lt;p&gt;Finally, we provide the steps to &lt;a href="#enable-vaccel-on-the-kata-runtime"&gt;enable
vAccel&lt;/a&gt; on a VM sandboxed container using
our custom &lt;code&gt;kata-containers&lt;/code&gt; runtime for the
&lt;a href="https://github.com/nubificus/kata-containers/tree/vaccel-v3.2" target="_blank" rel="noopener"&gt;Go&lt;/a&gt; and
&lt;a href="https://github.com/nubificus/kata-containers/tree/rs-vaccel-fusion" target="_blank" rel="noopener"&gt;Rust&lt;/a&gt;
variants.&lt;/p&gt;
&lt;h3 id="how-kata-containers-work"&gt;How kata containers work&lt;/h3&gt;
&lt;p&gt;Kata containers is a sandboxed container runtime, combining the benefits of VMs
in terms of workload isolation, with those of containers, in terms of
portability. Installing and configuring &lt;code&gt;kata-containers&lt;/code&gt; is straightforward
and we have covered that in &lt;a href="/posts/kata-build-source"&gt;some&lt;/a&gt; of our
&lt;a href="/posts/kata-build-configure-qemu"&gt;previous&lt;/a&gt;
&lt;a href="/posts/kata-build-configure-fc"&gt;posts&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this post, we will not go through the details of building &amp;ndash; we will just
install &lt;code&gt;kata-containers&lt;/code&gt; from the stock releases and add the binaries to enable
vAccel execution.&lt;/p&gt;
&lt;h3 id="how-vaccel-works"&gt;How vAccel works&lt;/h3&gt;
&lt;p&gt;vAccel is a software framework to expose hardware acceleration functionality to
workloads that do not have direct access to an acceleration device. vAccel
features a modular design where runtime &lt;code&gt;plugins&lt;/code&gt; implement API operations.
Figure 1 shows the software stack for the core vAccel library, along with a
number of plugins and their accompanying external libraries.&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-figure-1-the-vaccel-software-stack"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/orin/vaccel-stack.png" alt="vaccel-stack" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 1: The vAccel software stack
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Apart from &amp;ldquo;hardware&amp;rdquo; plugins (that implement the API operations in some sort
of acceleration library, eg CUDA, OpenCL etc.), vAccel features virtual plugins
that are able to forward requests to a remote Host. This functionality makes it
an open-ended API remoting framework. Based on the transport plugin (virtio or
vsock) multiple setups can be enabled, making it ideal to use on VMs or on
resource-constrained devices for remote execution. You can find more info about
vAccel in the &lt;a href="https://docs.vaccel.org" target="_blank" rel="noopener"&gt;vAccel docs website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The socket mode of vAccel for remote operations of vAccel works as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the Host component (direct access to an accelerator), listens for requests
using a predefined protocol (over gRPC) and issues vAccel API calls to the
core vAccelRT library.&lt;/li&gt;
&lt;li&gt;the Remote/Guest component (no direct access to an accelerator), forwards
requests to the Host component via a gRPC channel and receives results from
the execution.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the moment, the Host component is a simple gRPC agent (&lt;code&gt;vaccel-agent&lt;/code&gt;) and
the Remote/Guest component is a vAccel plugin (&lt;code&gt;libvaccel-vsock.so&lt;/code&gt;). A logical
diagram of the execution flow for a VM workload taking advantage of the vAccel
framework to run accelerated operations is shown in Figure 2.&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-figure-2-execution-flow-for-a-vaccel-application-running-in-a-vm"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/orin/vaccel-vm-flow.png" alt="vaccel" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 2: Execution flow for a vAccel application running in a VM
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id="kata-vaccel"&gt;kata-vAccel&lt;/h3&gt;
&lt;p&gt;Integrating the vAccel framework to a sandboxed container runtime removes the
need for complicated passthrough setups. No kernel prerequisites are needed
(apart from the &lt;code&gt;VSOCK&lt;/code&gt; options which is by-default enabled in most sandboxed
container runtimes).&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-figure-3-kata-vaccel-modes"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/orin/kata-vaccel-modes.png" alt="vaccel" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 3: kata-vAccel modes
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Currently, there are two modes for kata-vAccel (Figure 3): (a) the first one,
&lt;code&gt;exec&lt;/code&gt;/&lt;code&gt;external&lt;/code&gt; mode, supporting both runtime variants of &lt;code&gt;kata-containers&lt;/code&gt; (Go
and Rust), spawns the &lt;code&gt;vaccelrt-agent&lt;/code&gt; as an external binary, listening to the
vsock socket available for the &lt;code&gt;kata-agent&lt;/code&gt; component of the container runtime;
(b) the second mode, &lt;code&gt;integrated&lt;/code&gt; is only supported for the rust runtime
variant of &lt;code&gt;kata-containers&lt;/code&gt; and embeds the functionality of the &lt;code&gt;vaccelrt-agent&lt;/code&gt;
into the runtime, allowing better control and sandboxing of the components
running on the host system.&lt;/p&gt;
&lt;p&gt;An overview of the execution flow for a vAccel-enabled
sandboxed container running on &lt;code&gt;kata-containers&lt;/code&gt; is shown in Figure 4.&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-figure-4-execution-flow-for-a-vaccel-enabled-sandboxed-container"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/orin/vaccel-kata-rs.png" alt="kata-vaccel" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 4: Execution flow for a vAccel-enabled sandboxed container
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h4 id="kata-containers-binary-installation"&gt;kata-containers binary installation&lt;/h4&gt;
&lt;p&gt;The steps to install the stock &lt;code&gt;kata-containers&lt;/code&gt; runtime are shown below:&lt;/p&gt;
&lt;p&gt;Get the release tarball:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;wget https://github.com/kata-containers/kata-containers/releases/download/3.2.0-rc0/kata-static-3.2.0-rc0-arm64.tar.xz
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Unpack it in a temporary directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;mkdir /tmp/kata-release
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;xzcat kata-static-3.2.0-rc0-arm64.tar.xz &lt;span class="p"&gt;|&lt;/span&gt; tar -xvf - -C /tmp/kata-release
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You should be presented with the following directory structure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# tree -d /tmp/kata-release/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;/tmp/kata-release/
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;└── opt
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; └── kata
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ├── bin
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ├── libexec
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ├── runtime-rs
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │   └── bin
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; └── share
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ├── bash-completion
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │   └── completions
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ├── defaults
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; │   └── kata-containers
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ├── kata-containers
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; └── kata-qemu
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; └── qemu
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; └── firmware
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="m"&gt;15&lt;/span&gt; directories
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Move the files to &lt;code&gt;/opt/kata&lt;/code&gt; (This is a hard requirement, as all config files assume this folder):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;rsync -oagxvPH /tmp/kata-release/opt/kata /opt/
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;rm -rf /tmp/kata-release
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Create helper files for the kata VMMs you would like to try out:&lt;/p&gt;
&lt;p&gt;Cloud-hypervisor:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;cat &amp;gt; /usr/local/bin/containerd-shim-kata-clh-v2 &lt;span class="s"&gt;&amp;lt;&amp;lt; EOF
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;#!/bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;KATA_CONF_FILE=/opt/kata/share/defaults/kata-containers/configuration-clh.toml /opt/kata/bin/containerd-shim-kata-v2 $@
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;EOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Dragonball:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;cat &amp;gt; /usr/local/bin/containerd-shim-kata-rs-v2 &lt;span class="s"&gt;&amp;lt;&amp;lt; EOF
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;#!/bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;KATA_CONF_FILE=/opt/kata/share/defaults/kata-containers/configuration-dragonball.toml /opt/kata/runtime-rs/bin/containerd-shim-kata-v2 $@
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;EOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;AWS Firecracker:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;cat &amp;gt; /usr/local/bin/containerd-shim-kata-fc-v2 &lt;span class="s"&gt;&amp;lt;&amp;lt; EOF
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;#!/bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;KATA_CONF_FILE=/opt/kata/share/defaults/kata-containers/configuration-fc.toml /opt/kata/bin/containerd-shim-kata-v2 $@
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;EOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;QEMU:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;cat &amp;gt; /usr/local/bin/containerd-shim-kata-qemu-v2 &lt;span class="s"&gt;&amp;lt;&amp;lt; EOF
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;#!/bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;KATA_CONF_FILE=/opt/kata/share/defaults/kata-containers/configuration-qemu.toml /opt/kata/bin/containerd-shim-kata-v2 $@
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;EOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For each of the VMMs you need an entry in the &lt;code&gt;/etc/containerd/config.toml&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="o"&gt;[&lt;/span&gt;plugins.&lt;span class="s2"&gt;&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span&gt;.containerd.runtimes.kata-fc&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;privileged_without_host_devices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;privileged_without_host_devices_all_devices_allowed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;false&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;runtime_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;io.containerd.kata-fc.v2&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;snapshotter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;devmapper&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;pod_annotations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;*&amp;#34;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;container_annotations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;*&amp;#34;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="o"&gt;[&lt;/span&gt;plugins.&lt;span class="s2"&gt;&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span&gt;.containerd.runtimes.kata-rs&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;privileged_without_host_devices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;false&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;privileged_without_host_devices_all_devices_allowed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;false&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;runtime_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;io.containerd.kata-rs.v2&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="c1"&gt;#snapshotter = &amp;#34;devmapper&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;pod_annotations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;*&amp;#34;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;container_annotations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;*&amp;#34;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="o"&gt;[&lt;/span&gt;plugins.&lt;span class="s2"&gt;&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span&gt;.containerd.runtimes.kata-clh&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;privileged_without_host_devices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;false&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;privileged_without_host_devices_all_devices_allowed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;false&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;runtime_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;io.containerd.kata-clh.v2&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;pod_annotations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;*&amp;#34;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;container_annotations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;*&amp;#34;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="o"&gt;[&lt;/span&gt;plugins.&lt;span class="s2"&gt;&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span&gt;.containerd.runtimes.kata-qemu&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;privileged_without_host_devices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;false&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;privileged_without_host_devices_all_devices_allowed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;false&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;runtime_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;#34;io.containerd.kata-qemu.v2&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;pod_annotations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;*&amp;#34;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="nv"&gt;container_annotations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;#34;*&amp;#34;&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: for AWS Firecracker, the only supported snapshotter is &lt;code&gt;devmapper&lt;/code&gt;.
Make sure you &lt;a href="/posts/kata-build-configure-fc/#devmapper-snapshotter"&gt;follow the
instructions&lt;/a&gt; to setup
it correctly in &lt;code&gt;containerd&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Restart &lt;code&gt;containerd&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;systemctl restart containerd
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You should now be able to spawn a stock kata-container using one of the VMM configurations above:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;$ sudo nerdctl run --rm -it --runtime io.containerd.kata-fc.v2 --snapshotter devmapper ubuntu:latest uname -a
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;Linux d13db14b0c9a 5.15.26 #2 SMP Fri Apr 21 05:05:44 BST 2023 aarch64 aarch64 aarch64 GNU/Linux
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id="enable-vaccel-on-the-kata-runtime"&gt;Enable vAccel on the kata runtime&lt;/h4&gt;
&lt;p&gt;To enable vAccel, we need to add a custom kata runtime binary that instantiates
the &lt;code&gt;vaccel-agent&lt;/code&gt; alongside the container spawn.&lt;/p&gt;
&lt;p&gt;For now, we build binaries for &lt;code&gt;x86_64&lt;/code&gt; and &lt;code&gt;aarch64&lt;/code&gt;:&lt;/p&gt;
&lt;h5 id="go-runtime"&gt;Go runtime&lt;/h5&gt;
&lt;p&gt;For the go runtime (&lt;code&gt;kata-fc&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;wget https://s3.nbfc.io/nbfc-assets/github/vaccel-go-lib/aarch64/containerd-shim-kata-v2 -O /opt/kata/bin/containerd-shim-kata-vaccel-v2
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;chmod +x /opt/kata/bin/containerd-shim-kata-vaccel-v2
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;wget https://s3.nbfc.io/nbfc-assets/github/vaccel-go-lib/toml/configuration-fc-vaccel.toml -O /opt/kata/share/defaults/kata-containers/configuration-fc-vaccel.toml
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The next step is to setup helper scripts like above.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-bash" data-lang="bash"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;cat &amp;gt; /usr/local/bin/containerd-shim-kata-fc-vaccel-v2 &lt;span class="s"&gt;&amp;lt;&amp;lt; EOF
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;#!/bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;KATA_CONF_FILE=/opt/kata/share/defaults/kata-containers/configuration-fc-vaccel.toml /opt/kata/bin/containerd-shim-kata-vaccel-v2 $@
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;EOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and add the runtime to &lt;code&gt;/etc/containerd/config.toml&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; [plugins.&amp;#34;io.containerd.grpc.v1.cri&amp;#34;.containerd.runtimes.kata-fc-vaccel]
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; privileged_without_host_devices = true
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; privileged_without_host_devices_all_devices_allowed = true
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; runtime_type = &amp;#34;io.containerd.kata-fc-vaccel.v2&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; snapshotter = &amp;#34;devmapper&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; pod_annotations = [&amp;#34;*&amp;#34;]
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; container_annotations = [&amp;#34;*&amp;#34;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h5 id="rust-runtime"&gt;Rust runtime&lt;/h5&gt;
&lt;p&gt;For the rust runtime (&lt;code&gt;kata-rs&lt;/code&gt;)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;wget https://s3.nbfc.io/nbfc-assets/github/vaccel-rs-lib/shim/main/aarch64/containerd-shim-kata-v2 -O /opt/kata/runtime-rs/bin/containerd-shim-kata-vaccel-v2
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;chmod +x /opt/kata/runtime-rs/bin/containerd-shim-kata-vaccel-v2
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;wget https://s3.nbfc.io/nbfc-assets/github/vaccel-rs-lib/toml/main/aarch64/configuration-dragonball.toml -O /opt/kata/share/defaults/kata-containers/configuration-dbs-vaccel.toml
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The next step is to setup helper scripts like above.&lt;/p&gt;
&lt;p&gt;Dragonball with vAccel:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;cat &amp;gt; /usr/local/bin/containerd-shim-kata-rs-vaccel-v2 &lt;span class="s"&gt;&amp;lt;&amp;lt; EOF
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;#!/bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;KATA_CONF_FILE=/opt/kata/share/defaults/kata-containers/configuration-dbs-vaccel.toml /opt/kata/runtime-rs/bin/containerd-shim-kata-vaccel-v2 $@
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;EOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and add the runtime to &lt;code&gt;/etc/containerd/config.toml&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; [plugins.&amp;#34;io.containerd.grpc.v1.cri&amp;#34;.containerd.runtimes.kata-rs-vaccel]
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; privileged_without_host_devices = false
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; privileged_without_host_devices_all_devices_allowed = false
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; runtime_type = &amp;#34;io.containerd.kata-rs-vaccel.v2&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; #snapshotter = &amp;#34;devmapper&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; pod_annotations = [&amp;#34;*&amp;#34;]
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; container_annotations = [&amp;#34;*&amp;#34;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We should now be able to spawn a vAccel-enabled kata container with both go and rust runtimes:&lt;/p&gt;
&lt;p&gt;Go:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;$ nerdctl run --runtime io.containerd.kata-fc-vaccel.v2 --rm -it --snapshotter devmapper harbor.nbfc.io/nubificus/test-vaccel:latest /bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;root@bce78aa7d1b4:/#
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Rust:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;$ nerdctl run --runtime io.containerd.kata-rs-vaccel.v2 --rm -it harbor.nbfc.io/nubificus/test-vaccel:latest /bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;root@2ce045d042bc:/#
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="install-and-setup-vaccel-and-the-jetson-inference-plugin-on-the-host"&gt;Install and Setup vAccel and the &lt;code&gt;jetson-inference&lt;/code&gt; plugin on the host&lt;/h3&gt;
&lt;h4 id="setup-vaccel"&gt;Setup vAccel&lt;/h4&gt;
&lt;p&gt;To install vAccel on the Orin we need to download the core runtime library, the
agent and setup the jetson plugin. For a binary install, you can use the following snippet:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;wget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/main/aarch64/Release-deb/vaccel-0.5.0-Linux.deb
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;wget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/agent/main/aarch64/Release-deb/vaccelrt-agent-0.3.6-Linux.deb
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;wget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/plugins/jetson_inference/main/aarch64/Release-deb/vaccelrt-plugin-jetson-0.1.0-Linux.deb
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;dpkg -i vaccel-0.5.0-Linux.deb
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;dpkg -i vaccelrt-agent-0.3.6-Linux.deb
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;dpkg -i vaccelrt-plugin-jetson-0.1.0-Linux.deb
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To be able to use the vAccel &lt;code&gt;jetson-inference&lt;/code&gt; plugin, we need to install the
&lt;code&gt;jetson-inference&lt;/code&gt; framework on the host system.&lt;/p&gt;
&lt;h4 id="setup-jetson-inference-on-the-orin"&gt;Setup &lt;code&gt;jetson-inference&lt;/code&gt; on the Orin&lt;/h4&gt;
&lt;p&gt;We follow the installation instructions from
&lt;a href="https://github.com/dusty-nv/jetson-inference" target="_blank" rel="noopener"&gt;dusty-nv/jetson-inference&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;TL;DR:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;git clone https://github.com/dusty-nv/jetson-inference --depth &lt;span class="m"&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; jetson-inference
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;git submodule update --init
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;mkdir build &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;cd&lt;/span&gt; build
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;cmake ../ -DBUILD_INTERACTIVE&lt;span class="o"&gt;=&lt;/span&gt;OFF
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;make &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; make install
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Make sure you link the models to a directory in
&lt;code&gt;/usr/local/share/imagenet-models&lt;/code&gt;, as this is the default path where the
vAccel jetson plugin looks for them:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;mkdir /usr/local/share/imagenet-models
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;ln -s &lt;span class="nv"&gt;$PWD&lt;/span&gt;/../data/networks /usr/local/share/imagenet-models/
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For more info on a full &lt;code&gt;jetson-inference&lt;/code&gt; vaccel example, have a look at the &lt;a href="https://docs.vaccel.org/quickstart/#jetson-example" target="_blank" rel="noopener"&gt;vAccel docs&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id="configure-the-container-runtime-to-use-vaccel"&gt;Configure the container runtime to use vAccel&lt;/h4&gt;
&lt;p&gt;Now that we have kata containers setup and vAccel installed, let&amp;rsquo;s configure the runtime!&lt;/p&gt;
&lt;p&gt;Make sure you specify the relevant vaccel backend/plugins in the kata
configuration file. As mentioned above, there are two modes of operation
supported in the kata runtime for the &lt;code&gt;vaccelrt-agent&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;(a) &lt;em&gt;external&lt;/em&gt;: the kata runtime spawns the &lt;code&gt;vaccelrt-agent&lt;/code&gt; binary, as an external
application. The setup for the host vAccel plugins is done in the kata
configuration file.&lt;/p&gt;
&lt;p&gt;(b) &lt;em&gt;integrated&lt;/em&gt;: the kata runtime includes the &lt;code&gt;vaccelrt-agent&lt;/code&gt; functionality,
embedded in the runtime code. The setup for the host vAccel plugins is done in
the helper script, as an environment variable (&lt;code&gt;VACCEL_BACKENDS&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;To configure the system for mode (a), for the go runtime we edit
&lt;code&gt;/opt/kata/share/defaults/kata-containers/configuration-fc-vaccel.toml&lt;/code&gt; and add
the &lt;code&gt;jetson&lt;/code&gt; plugin before &lt;code&gt;noop&lt;/code&gt; (default/debug) plugin:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;@@ -54,7 +54,7 @@
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; # vaccel_guest_backend can be one of &amp;#34;virtio&amp;#34; or &amp;#34;vsock&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; # Your distribution recommends: vaccel_host_backend = noop
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; # Your distribution recommends: vaccel_guest_backend = vsock
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;-vaccel_host_backends = &amp;#34;noop&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;+vaccel_host_backends = &amp;#34;jetson,noop&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; vaccel_guest_backend = &amp;#34;vsock&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; # If vaccel_guest_backend=vsock specify the vAccel Agent vsock port
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For the rust runtime, we edit the &lt;code&gt;/opt/kata/share/defaults/kata-containers/configuration-dbs-vaccel.toml&lt;/code&gt; file as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;agent_path= &amp;#34;/usr/local/bin/vaccelrt-agent&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;debug= &amp;#34;1&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;backends= &amp;#34;jetson,noop&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;backends_library= &amp;#34;/usr/local/lib/&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;endpoint_port= &amp;#34;2048&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;execution_type= &amp;#34;exec&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For mode (b), we do:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;agent_path= &amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;debug= &amp;#34;1&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;backends= &amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;backends_library= &amp;#34;/usr/local/lib/&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;endpoint_port= &amp;#34;2048&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;execution_type= &amp;#34;integrated&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and edit the helper script to specify the plugins:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;cat &amp;gt; /usr/local/bin/containerd-shim-kata-rs-vaccel-v2 &lt;span class="s"&gt;&amp;lt;&amp;lt; EOF
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;#!/bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;VACCEL_BACKENDS=/usr/local/lib/libvaccel-jetson.so:/usr/local/lib/libvaccel-noop.so KATA_CONF_FILE=/opt/kata/share/defaults/kata-containers/configuration-dbs-vaccel.toml /opt/kata/runtime-rs/bin/containerd-shim-kata-vaccel-v2 $@
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="s"&gt;EOF&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="spawn-a-vaccel-enabled-sandboxed-container"&gt;Spawn a vAccel-enabled sandboxed container&lt;/h3&gt;
&lt;p&gt;We are now ready to spawn a vAccel-enabled container. We already have a test container image,
but if you would like to build your own, the Dockerfile is the one below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;FROM ubuntu:latest
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;RUN apt update &amp;amp;&amp;amp; apt install -y wget &amp;amp;&amp;amp; \
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; wget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/master/aarch64/Release-deb/vaccel-0.5.0-Linux.deb &amp;amp;&amp;amp; \
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; wget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/plugins/vsock/master/aarch64/Release-deb/vaccelrt-plugin-vsock-0.1.0-Linux.deb &amp;amp;&amp;amp; \
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; dpkg -i vaccel-0.5.0-Linux.deb vaccelrt-plugin-vsock-0.1.0-Linux.deb
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;ENV VACCEL_DEBUG_LEVEL=4
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;ENV VACCEL_BACKENDS=/usr/local/lib/libvaccel-vsock.so
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;build like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;docker build -t vaccel-test-container:aarch64 -f Dockerfile .
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Essentially, it installs the vAccel core library, and the &lt;code&gt;vsock&lt;/code&gt; plugin that
communicates with the vAccel agent running on the host (spawned using the modes
described above).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;$ sudo nerdctl run --runtime io.containerd.kata-rs-vaccel.v2 --rm -it nubificus/vaccel-test-container:latest /bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;root@23a8738e14a8:/#
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s get an image:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-gdscript3" data-lang="gdscript3"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="o"&gt;$&lt;/span&gt; &lt;span class="n"&gt;wget&lt;/span&gt; &lt;span class="n"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;upload&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wikimedia&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;wikipedia&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;commons&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;bd&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Golden_Retriever_Dukedestiny01_drvd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="mi"&gt;2023&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;09&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;17&lt;/span&gt; &lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;48&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;09&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt; &lt;span class="n"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="o"&gt;//&lt;/span&gt;&lt;span class="n"&gt;upload&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wikimedia&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;wikipedia&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;commons&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;bd&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;Golden_Retriever_Dukedestiny01_drvd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;Resolving&lt;/span&gt; &lt;span class="n"&gt;upload&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wikimedia&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;upload&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wikimedia&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="mf"&gt;185.15&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;59.240&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="n"&gt;a02&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;ec80&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;300&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;ed1a&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;Connecting&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;upload&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wikimedia&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;upload&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;wikimedia&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="mf"&gt;185.15&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;59.240&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mf"&gt;443.&lt;/span&gt;&lt;span class="o"&gt;..&lt;/span&gt; &lt;span class="n"&gt;connected&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;HTTP&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt; &lt;span class="n"&gt;sent&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;awaiting&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt; &lt;span class="n"&gt;OK&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;Length&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;165000&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;161&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;jpeg&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;Saving&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Golden_Retriever_Dukedestiny01_drvd.jpg&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;Golden_Retriever_Dukedestiny01_drvd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jpg&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;==================================================================================================&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="mf"&gt;161.13&lt;/span&gt;&lt;span class="n"&gt;K&lt;/span&gt; &lt;span class="mi"&gt;701&lt;/span&gt;&lt;span class="n"&gt;KB&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="mi"&gt;2023&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;09&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;17&lt;/span&gt; &lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;48&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;14&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;701&lt;/span&gt; &lt;span class="n"&gt;KB&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Golden_Retriever_Dukedestiny01_drvd.jpg&amp;#39;&lt;/span&gt; &lt;span class="n"&gt;saved&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;165000&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;165000&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and try to issue a classify operation using vAccel&amp;rsquo;s &lt;code&gt;jetson-inference&lt;/code&gt; backend:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;root@2ce045d042bc:/# /usr/local/bin/classify Golden_Retriever_Dukedestiny01_drvd.jpg &lt;span class="m"&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;Initialized session with id: &lt;span class="m"&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;Image size: 165000B
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;classification tags: 86.768% golden retriever
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Awesome, right?! We are able to use hardware acceleration on a sandboxed
container that does not have direct access to the accelerator device. Stay
tuned for more info on our progress with &lt;code&gt;kata-containers&lt;/code&gt; and vAccel!&lt;/p&gt;</description></item><item><title>Boot a VM on an NVIDIA Jetson AGX Orin</title><link>/blog/orin-vm/</link><pubDate>Mon, 13 Feb 2023 10:14:04 +0100</pubDate><guid>/blog/orin-vm/</guid><description>&lt;p&gt;In 2022, NVIDIA released the &lt;a href="https://www.nvidia.com/en-il/autonomous-machines/embedded-systems/jetson-orin/" target="_blank" rel="noopener"&gt;Jetson
Orin&lt;/a&gt;
modules, specifically designed for extreme computation at the Edge. The NVIDIA
Jetson AGX Orin modules deliver up to 275 TOPS of AI performance with power
configurable between 15W and 60W.&lt;/p&gt;
&lt;figure id="figure-figure-1-the-nvidia-jetson-agx-orin-devkit--module"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/nvidia-jetson-agx-orin.jpg#center" alt="Figure 1: The NVIDIA Jetson AGX Orin devkit &amp; module" loading="lazy" data-zoomable width="80%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 1: The NVIDIA Jetson AGX Orin devkit &amp;amp; module
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;These powerful machines, apart from bleeding edge GPUs, feature 12 ARMv8 cores
and come with 16GB, 32GB or 64GB of memory. The number of cores, combined with
these amounts of RAM appear ideal for some use-cases where multi-tenancy is
essential; making use of the virtualization extensions in these cores, we
enforce stronger isolation among the workloads running at the Edge. Moreover,
with the use of &lt;a href="https://katacontainers.io" target="_blank" rel="noopener"&gt;kata-containers&lt;/a&gt;, we maintain the
cloud-native aspect of the application deployment at the Edge.&lt;/p&gt;
&lt;p&gt;Following up on our work with &lt;a href="https://katacontainers.io" target="_blank" rel="noopener"&gt;kata-containers&lt;/a&gt; and
&lt;a href="https://docs.vaccel.org" target="_blank" rel="noopener"&gt;vAccel&lt;/a&gt;, we got a couple of Orin nodes and started
experimenting. Unfortunately, things are a bit different than with the original
Jetson AGX Xavier boards we were used to. The SoC is slightly different, with a
fully-featured, &lt;a href="https://developer.nvidia.com/orin-series-soc-technical-reference-manual" target="_blank" rel="noopener"&gt;multi-core
GICv3&lt;/a&gt;
implementation.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s great news! Right? well&amp;hellip; not really, as the necessary control
structures for GICv3 are not created during boot, caused by an &lt;a href="https://forums.developer.nvidia.com/t/fix-interrupts-declarations-for-gicv3/240566" target="_blank" rel="noopener"&gt;incomplete
interrupts declaration in the device
tree&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://forums.developer.nvidia.com/u/vadim.likholetov/summary" target="_blank" rel="noopener"&gt;Vadim&lt;/a&gt; &amp;amp;
&lt;a href="https://forums.developer.nvidia.com/u/alexey13/summary" target="_blank" rel="noopener"&gt;Alexey&lt;/a&gt; identified the
issue and provided the necessary patches to the device tree source files and ..
tada! we can boot a non-emulated VM, using GICv3 on Jetson Orin!&lt;/p&gt;
&lt;h3 id="walk-through-the-issue"&gt;Walk through the issue&lt;/h3&gt;
&lt;p&gt;So, initially, we started to see if the stock kernel has KVM enabled:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;#&lt;/span&gt; dmesg &lt;span class="p"&gt;|&lt;/span&gt;grep -i kvm
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.362967] kvm [1]: IPA Size Limit: 48 bits
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.363130] kvm [1]: VHE mode initialized successfully
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Looks like there is support. So we moved on to try running an AWS Firecracker
VM. Got our &lt;code&gt;vmlinux&lt;/code&gt;, &lt;code&gt;rootfs.img&lt;/code&gt; and &lt;code&gt;config.json&lt;/code&gt; built for our tests and
tried to boot the VM:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;wget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/vm-example/aarch64/rust-vmm/vmlinux
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;wget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/vm-example/aarch64/rootfs.img
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;wget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/vm-example/aarch64/fc/config_vsock.json
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;wget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/vm-example/aarch64/fc/firecracker
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;#&lt;/span&gt; ./firecracker --api-sock fc.sock --config-file config_vsock.json
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023-02-13T18:54:29.061153660 [anonymous-instance:main:ERROR:src/firecracker/src/main.rs:480] Building VMM configured from cmdline json failed: Internal(Vm(VmCreateGIC(CreateGIC(Error(19)))))
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We were a bit troubled as the exact same setup was working fine on a Jetson AGX
Xavier. We also tried
&lt;a href="https://forums.developer.nvidia.com/t/jetson-agx-orin-devkit-34-1-1-gicv3-vgic-creation-failed/216192" target="_blank" rel="noopener"&gt;QEMU&lt;/a&gt;,
with the same results:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;#&lt;/span&gt; qemu-system-aarch64 -cpu max -machine virt,gic-version&lt;span class="o"&gt;=&lt;/span&gt;3,kernel-irqchip&lt;span class="o"&gt;=&lt;/span&gt;on -m &lt;span class="m"&gt;1024&lt;/span&gt; -nographic -monitor none -kernel /boot/Image -enable-kvm
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;qemu-system-aarch64: gic-version=3 is not supported with kernel-irqchip=off
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;but if we disable KVM on qemu, we can see the kernel booting:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;#&lt;/span&gt; qemu-system-aarch64 -cpu max -machine virt,gic-version&lt;span class="o"&gt;=&lt;/span&gt;3,kernel-irqchip&lt;span class="o"&gt;=&lt;/span&gt;on -m &lt;span class="m"&gt;1024&lt;/span&gt; -nographic -monitor none -kernel /boot/Image
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Booting Linux on physical CPU 0x0000000000 [0x000f0510]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Linux version 5.10.65-tegra (buildbrain@mobile-u64-5266-d7000) (aarch64-buildroot-linux-gnu-gcc.br_real (Buildroot 2020.08) 9.3.0, GNU ld (GNU Binutils) 2.33.1) #1 SMP PREEMPT Tue Mar 15 00:53:43 PDT 2022
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] OF: fdt: memory scan node memory@40000000, reg size 16,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] OF: fdt: - 40000000 , 40000000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Machine model: linux,dummy-virt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] efi: UEFI not found.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Zone ranges:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] DMA [mem 0x0000000040000000-0x000000007fffffff]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] DMA32 empty
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Normal empty
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Movable zone start for each node
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So after digging into this issue, googling and trying various workarounds, we
came across the points raised above about the GICv3 initialization.&lt;/p&gt;
&lt;h3 id="solution"&gt;Solution&lt;/h3&gt;
&lt;p&gt;Following the
&lt;a href="https://docs.nvidia.com/jetson/archives/r35.2.1/DeveloperGuide/text/SD/Kernel/KernelCustomization.html" target="_blank" rel="noopener"&gt;instructions&lt;/a&gt;
on how to build a kernel for the Jetson AGX Orin series, we get the sources and
patch the device tree sources using the following snippet:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-patch" data-lang="patch"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gd"&gt;--- Linux_for_Tegra/source/public/hardware/nvidia/soc/t23x/kernel-dts/tegra234-soc/tegra234-soc-minimal.dtsi.orig 2022-08-11 03:14:51.000000000 +0000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gi"&gt;+++ Linux_for_Tegra/source/public/hardware/nvidia/soc/t23x/kernel-dts/tegra234-soc/tegra234-soc-minimal.dtsi 2023-02-12 09:07:10.259761186 +0000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gu"&gt;@@ -43,6 +43,10 @@
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; reg = &amp;lt;0x0 0x0f400000 0x0 0x00010000 /* GICD */
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; 0x0 0x0f440000 0x0 0x00200000&amp;gt;; /* GICR CPU 0-15 */
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ranges;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gi"&gt;+ interrupts = &amp;lt;GIC_PPI 9
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gi"&gt;+ (GIC_CPU_MASK_SIMPLE(8) | IRQ_TYPE_LEVEL_HIGH)&amp;gt;;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gi"&gt;+ interrupt-parent = &amp;lt;&amp;amp;intc&amp;gt;;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gi"&gt;+
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; status = &amp;#34;disabled&amp;#34;;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; gic_v2m: v2m@f410000 {
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We build the kernel using the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;Linux_for_Tegra/source/public# ./nvbuild.sh -o kernel_out_updated
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Essentially, just the device tree is needed:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;kernel_out_updated/arch/arm64/boot/dts/nvidia/tegra234-p3701-0000-p3737-0000.dtb
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We copy this file to the board at the following directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;/boot/dtb
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and tweak the boot loader config to load the updated device-tree file, instead
of the default one:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-patch" data-lang="patch"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gd"&gt;--- /boot/extlinux/extlinux.conf.orig 2023-02-13 18:41:26.208771762 +0000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gi"&gt;+++ /boot/extlinux/extlinux.conf 2023-02-13 18:41:37.452854874 +0000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gu"&gt;@@ -1,6 +1,6 @@
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; LABEL primary
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; MENU LABEL primary kernel
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; LINUX /boot/Image
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gd"&gt;- FDT /boot/dtb/kernel_tegra234-p3701-0000-p3737-0000.dtb
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gi"&gt;+ FDT /boot/dtb/tegra234-p3701-0000-p3737-0000.dtb
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; INITRD /boot/initrd
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; APPEND ${cbootargs} root=/dev/mmcblk0p1 rw rootwait rootfstype=ext4 mminit_loglevel=4 console=ttyTCU0,115200 console=ttyAMA0,115200 console=tty0 firmware_class.path=/etc/firmware fbcon=map:0 net.ifnames=0
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And now we&amp;rsquo;re ready for reboot! Assuming all went well, you&amp;rsquo;ll end up with the
following in the kernel boot logs:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;#&lt;/span&gt; dmesg &lt;span class="p"&gt;|&lt;/span&gt;grep -i kvm
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 3.048360] kvm [1]: IPA Size Limit: 48 bits
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 3.052646] kvm [1]: GICv3: no GICV resource entry
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 3.057437] kvm [1]: disabling GICv2 emulation
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 3.061891] kvm [1]: GIC system register CPU interface enabled
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 3.067830] kvm [1]: vgic interrupt IRQ9
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 3.071899] kvm [1]: VHE mode initialized successfully
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;And if we try spawning a firecracker VM as above, we get the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;#&lt;/span&gt; ./bin/firecracker --config-file config_vsock.json --api-sock fc.sock
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Booting Linux on physical CPU 0x0000000000 [0x410fd421]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Linux version 5.10.0 (runner@gh-cloud-pod-ckp6h) (gcc (Ubuntu/Linaro 8.4.0-1ubuntu1~18.04) 8.4.0, GNU ld (GNU Binutils for Ubuntu) 2.30) #1 SMP Wed May 4 06:10:52 UTC 2022
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Machine model: linux,dummy-virt
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] earlycon: uart0 at MMIO 0x0000000040003000 (options &amp;#39;&amp;#39;)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] printk: bootconsole [uart0] enabled
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] efi: UEFI not found.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] NUMA: No NUMA configuration found
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] NUMA: Faking a node at [mem 0x0000000080000000-0x000000017fffffff]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] NUMA: NODE_DATA [mem 0x17f6d7900-0x17f6f8fff]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Zone ranges:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] DMA [mem 0x0000000080000000-0x00000000bfffffff]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] DMA32 [mem 0x00000000c0000000-0x00000000ffffffff]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Normal [mem 0x0000000100000000-0x000000017fffffff]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Movable zone start for each node
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Early memory node ranges
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] node 0: [mem 0x0000000080000000-0x000000017fffffff]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Initmem setup node 0 [mem 0x0000000080000000-0x000000017fffffff]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] On node 0 totalpages: 1048576
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] DMA zone: 4096 pages used for memmap
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] DMA zone: 0 pages reserved
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] DMA zone: 262144 pages, LIFO batch:63
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] DMA32 zone: 4096 pages used for memmap
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] DMA32 zone: 262144 pages, LIFO batch:63
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Normal zone: 8192 pages used for memmap
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Normal zone: 524288 pages, LIFO batch:63
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] psci: probing for conduit method from DT.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] psci: PSCIv1.0 detected in firmware.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] psci: Using standard PSCI v0.2 function IDs
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] psci: Trusted OS migration not required
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] psci: SMC Calling Convention v1.1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] percpu: Embedded 22 pages/cpu s49944 r8192 d31976 u90112
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] pcpu-alloc: s49944 r8192 d31976 u90112 alloc=22*4096
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] pcpu-alloc: [0] 0 [0] 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Detected PIPT I-cache on CPU0
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] CPU features: detected: GIC system register CPU interface
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] CPU features: detected: Hardware dirty bit management
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] CPU features: detected: Spectre-v4
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Built 1 zonelists, mobility grouping on. Total pages: 1032192
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Policy zone: Normal
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Kernel command line: console=ttyS0 reboot=k panic=1 pci=off loglevel=8 root=/dev/vda ip=172.42.0.2::172.42.0.1:255.255.255.0::eth0:off random.trust_cpu=on root=/dev/vda rw earlycon=uart,mmio,0x40003000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Dentry cache hash table entries: 524288 (order: 10, 4194304 bytes, linear)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Inode-cache hash table entries: 262144 (order: 9, 2097152 bytes, linear)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] mem auto-init: stack:off, heap alloc:off, heap free:off
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] software IO TLB: mapped [mem 0x00000000bbfff000-0x00000000bffff000] (64MB)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Memory: 4024740K/4194304K available (8064K kernel code, 7700K rwdata, 2060K rodata, 1408K init, 3005K bss, 169564K reserved, 0K cma-reserved)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] random: get_random_u64 called from __kmem_cache_create+0x2c/0x4a0 with crng_init=0
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] SLUB: HWalign=64, Order=0-3, MinObjects=0, CPUs=2, Nodes=1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] rcu: Hierarchical RCU implementation.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] rcu: RCU restricting CPUs from NR_CPUS=128 to nr_cpu_ids=2.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] Tracing variant of Tasks RCU enabled.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] rcu: RCU calculated value of scheduler-enlistment delay is 25 jiffies.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] rcu: Adjusting geometry for rcu_fanout_leaf=16, nr_cpu_ids=2
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] NR_IRQS: 64, nr_irqs: 64, preallocated irqs: 0
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] GICv3: 96 SPIs implemented
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] GICv3: 0 Extended SPIs implemented
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] GICv3: Distributor has no Range Selector support
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] GICv3: 16 PPIs implemented
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] GICv3: CPU0: found redistributor 0 region 0:0x000000003ffb0000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] arch_timer: cp15 timer(s) running at 31.25MHz (virt).
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000000] clocksource: arch_sys_counter: mask: 0xffffffffffffff max_cycles: 0xe6a171046, max_idle_ns: 881590405314 ns
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.000003] sched_clock: 56 bits at 31MHz, resolution 32ns, wraps every 4398046511088ns
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.001027] Console: colour dummy device 80x25
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.001600] Calibrating delay loop (skipped), value calculated using timer frequency.. 62.50 BogoMIPS (lpj=125000)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.003003] pid_max: default: 32768 minimum: 301
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.003621] LSM: Security Framework initializing
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.004239] SELinux: Initializing.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.004705] Mount-cache hash table entries: 8192 (order: 4, 65536 bytes, linear)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.005642] Mountpoint-cache hash table entries: 8192 (order: 4, 65536 bytes, linear)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.007399] rcu: Hierarchical SRCU implementation.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.008182] EFI services will not be available.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.008825] smp: Bringing up secondary CPUs ...
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.016089] Detected PIPT I-cache on CPU1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.016135] GICv3: CPU1: found redistributor 1 region 0:0x000000003ffd0000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.016251] CPU1: Booted secondary processor 0x0000000001 [0x410fd421]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.016842] smp: Brought up 1 node, 2 CPUs
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.019483] SMP: Total of 2 processors activated.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.020055] CPU features: detected: Privileged Access Never
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.020721] CPU features: detected: LSE atomic instructions
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.021326] CPU features: detected: User Access Override
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.021913] CPU features: detected: 32-bit EL0 Support
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.022464] CPU features: detected: Common not Private translations
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.023120] CPU features: detected: RAS Extension Support
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.023723] CPU features: detected: Data cache clean to the PoU not required for I/D coherence
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.024694] CPU features: detected: CRC32 instructions
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.025269] CPU features: detected: Speculative Store Bypassing Safe (SSBS)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.051711] CPU: All CPU(s) started at EL1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.052455] alternatives: patching kernel code
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.054839] devtmpfs: initialized
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.056234] clocksource: jiffies: mask: 0xffffffff max_cycles: 0xffffffff, max_idle_ns: 7645041785100000 ns
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.057936] futex hash table entries: 512 (order: 3, 32768 bytes, linear)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.059329] DMI not present or invalid.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.060220] NET: Registered protocol family 16
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.062118] DMA: preallocated 512 KiB GFP_KERNEL pool for atomic allocations
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.064139] DMA: preallocated 512 KiB GFP_KERNEL|GFP_DMA pool for atomic allocations
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.066196] DMA: preallocated 512 KiB GFP_KERNEL|GFP_DMA32 pool for atomic allocations
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.067848] audit: initializing netlink subsys (disabled)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.069643] audit: type=2000 audit(0.068:1): state=initialized audit_enabled=0 res=1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.069818] thermal_sys: Registered thermal governor &amp;#39;fair_share&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.071171] thermal_sys: Registered thermal governor &amp;#39;step_wise&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.071923] thermal_sys: Registered thermal governor &amp;#39;user_space&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.072795] cpuidle: using governor ladder
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.074720] cpuidle: using governor menu
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.075499] hw-breakpoint: found 6 breakpoint and 4 watchpoint registers.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.076945] ASID allocator initialised with 65536 entries
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.085849] HugeTLB registered 1.00 GiB page size, pre-allocated 0 pages
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.087077] HugeTLB registered 32.0 MiB page size, pre-allocated 0 pages
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.088149] HugeTLB registered 2.00 MiB page size, pre-allocated 0 pages
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.089190] HugeTLB registered 64.0 KiB page size, pre-allocated 0 pages
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.093766] iommu: Default domain type: Translated
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.094597] SCSI subsystem initialized
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.095080] pps_core: LinuxPPS API ver. 1 registered
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.095636] pps_core: Software ver. 5.3.6 - Copyright 2005-2007 Rodolfo Giometti &amp;lt;giometti@linux.it&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.096684] PTP clock support registered
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.097441] NetLabel: Initializing
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.097830] NetLabel: domain hash size = 128
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.098321] NetLabel: protocols = UNLABELED CIPSOv4 CALIPSO
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.098967] NetLabel: unlabeled traffic allowed by default
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.099750] clocksource: Switched to clocksource arch_sys_counter
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.100568] VFS: Disk quotas dquot_6.6.0
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.101037] VFS: Dquot-cache hash table entries: 512 (order 0, 4096 bytes)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.101952] FS-Cache: Loaded
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.102771] CacheFiles: Loaded
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.104955] NET: Registered protocol family 2
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.105781] tcp_listen_portaddr_hash hash table entries: 2048 (order: 3, 32768 bytes, linear)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.107212] TCP established hash table entries: 32768 (order: 6, 262144 bytes, linear)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.108659] TCP bind hash table entries: 32768 (order: 7, 524288 bytes, linear)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.110424] TCP: Hash tables configured (established 32768 bind 32768)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.111486] UDP hash table entries: 2048 (order: 4, 65536 bytes, linear)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.112545] UDP-Lite hash table entries: 2048 (order: 4, 65536 bytes, linear)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.113614] NET: Registered protocol family 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.114900] Initialise system trusted keyrings
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.115467] Key type blacklist registered
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.116112] workingset: timestamp_bits=36 max_order=20 bucket_order=0
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.118568] squashfs: version 4.0 (2009/01/31) Phillip Lougher
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.132869] Key type asymmetric registered
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.133466] Asymmetric key parser &amp;#39;x509&amp;#39; registered
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.134217] Block layer SCSI generic (bsg) driver version 0.4 loaded (major 251)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.136154] Serial: 8250/16550 driver, 1 ports, IRQ sharing disabled
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.137435] printk: console [ttyS0] disabled
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.138118] 40003000.uart: ttyS0 at MMIO 0x40003000 (irq = 14, base_baud = 1500000) is a 16550A
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.139487] printk: console [ttyS0] enabled
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.139487] printk: console [ttyS0] enabled
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.140875] printk: bootconsole [uart0] disabled
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.140875] printk: bootconsole [uart0] disabled
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.142604] cacheinfo: Unable to detect cache hierarchy for CPU 0
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.146080] loop: module loaded
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.146816] virtio_blk virtio0: [vda] 2097152 512-byte logical blocks (1.07 GB/1.00 GiB)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.147764] vda: detected capacity change from 0 to 1073741824
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.149279] Loading iSCSI transport class v2.0-870.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.150525] iscsi: registered transport (tcp)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.151107] tun: Universal TUN/TAP device driver, 1.6
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.152381] rtc-pl031 40004000.rtc: designer ID = 0x41
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.153012] rtc-pl031 40004000.rtc: revision = 0x1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.153898] rtc-pl031 40004000.rtc: char device (254:0)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.154530] rtc-pl031 40004000.rtc: registered as rtc0
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.155158] rtc-pl031 40004000.rtc: setting system clock to 2023-02-13T19:08:49 UTC (1676315329)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.156324] hid: raw HID events driver (C) Jiri Kosina
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.157485] Initializing XFRM netlink socket
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.158395] NET: Registered protocol family 10
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.160444] Segment Routing with IPv6
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.160971] NET: Registered protocol family 17
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.161565] Key type dns_resolver registered
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.162179] NET: Registered protocol family 40
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.163868] registered taskstats version 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.164554] Loading compiled-in X.509 certificates
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.166411] Loaded X.509 cert &amp;#39;Build time autogenerated kernel key: 5c87d35d601eb9a30312d062d8891ae920951e61&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.167750] Key type ._fscrypt registered
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.168431] Key type .fscrypt registered
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.169023] Key type fscrypt-provisioning registered
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.170251] Key type encrypted registered
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023-02-13T19:08:49.402741111 [anonymous-instance:ERROR:src/devices/src/virtio/net/device.rs:390] Failed to write to tap: Os { code: 5, kind: Other, message: &amp;#34;Input/output error&amp;#34; }
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.187729] IP-Config: Complete:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.188127] device=eth0, hwaddr=aa:fc:00:00:00:01, ipaddr=172.42.0.2, mask=255.255.255.0, gw=172.42.0.1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.189265] host=172.42.0.2, domain=, nis-domain=(none)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.189908] bootserver=255.255.255.255, rootserver=255.255.255.255, rootpath=
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.190773]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.193054] EXT4-fs (vda): mounted filesystem with ordered data mode. Opts: (null)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.194395] VFS: Mounted root (ext4 filesystem) on device 254:0.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.195952] devtmpfs: mounted
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.197005] Freeing unused kernel memory: 1408K
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.215773] Run /sbin/init as init process
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.216663] with arguments:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.217281] /sbin/init
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.217844] with environment:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.218496] HOME=/
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.218989] TERM=linux
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.219552] pci=off
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;SELinux: Could not open policy file &amp;lt;= /etc/selinux/targeted/policy/policy.33: No such file or directory
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.258127] systemd[1]: Failed to find module &amp;#39;autofs4&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.263505] systemd[1]: systemd 245.4-4ubuntu3.11 running in system mode. (+PAM +AUDIT +SELINUX +IMA +APPARMOR +SMACK +SYSVINIT +UTMP +LIBCRYPTSETUP +GCRYPT +GNUTLS +ACL +XZ +LZ4 +SECCOMP +BLKID +ELFUTILS +KMOD +IDN2 -IDN +PCRE2 default-hierarchy=hybrid)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[ 0.266780] systemd[1]: Detected architecture arm64.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Welcome to Ubuntu 20.04.2 LTS!
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[snipped]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Initially, we got an RTC initialization error, so in case you get such an
issue, disable &lt;code&gt;pl031_driver_init&lt;/code&gt; through the &lt;code&gt;initcall_blacklist&lt;/code&gt; kernel
cmdline option:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-patch" data-lang="patch"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gd"&gt;--- config_vsock.json.orig 2023-02-13 19:17:43.750699285 +0000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gi"&gt;+++ config_vsock.json 2023-02-13 19:17:57.154793334 +0000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gu"&gt;@@ -1,7 +1,7 @@
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;boot-source&amp;#34;: {
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;kernel_image_path&amp;#34;: &amp;#34;vmlinux&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gd"&gt;- &amp;#34;boot_args&amp;#34;: &amp;#34;console=ttyS0 reboot=k panic=1 pci=off loglevel=8 root=/dev/vda ip=172.42.0.2::172.42.0.1:255.255.255.0::eth0:off random.trust_cpu=on&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gi"&gt;+ &amp;#34;boot_args&amp;#34;: &amp;#34;console=ttyS0 reboot=k panic=1 pci=off loglevel=8 root=/dev/vda ip=172.42.0.2::172.42.0.1:255.255.255.0::eth0:off random.trust_cpu=on initcall_blacklist=pl031_driver_init&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; },
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;drives&amp;#34;: [
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="future-steps"&gt;Future steps&lt;/h2&gt;
&lt;p&gt;Give us a shout at &lt;a href="mailto:team@cloudkernels.net"&gt;team@cloudkernels.net&lt;/a&gt; if you liked it! The plan is to use
these boards to expose acceleration functionality in isolated workloads running
on sandboxed containers using &lt;a href="https://katacontainers.io" target="_blank" rel="noopener"&gt;kata-containers&lt;/a&gt; and
&lt;a href="https://vaccel.org" target="_blank" rel="noopener"&gt;vAccel&lt;/a&gt;. Take a sneak peek on what we are working on
&lt;a href="https://docs.vaccel.org/container_runtimes" target="_blank" rel="noopener"&gt;here&lt;/a&gt; and stay tuned for the next
post where we describe the process to build and run a vAccel-enabled sandboxed
container on a Jetson Orin!&lt;/p&gt;</description></item><item><title>Hardware Acceleration for Unikernels - A Status Update of vAccel</title><link>/event/fosdem2023/</link><pubDate>Sun, 05 Feb 2023 14:00:00 +0000</pubDate><guid>/event/fosdem2023/</guid><description>&lt;h2 id="code--resources"&gt;Code &amp;amp; Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.vaccel.org/latest/" target="_blank" rel="noopener"&gt;vAccel Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nubificus/vaccel" target="_blank" rel="noopener"&gt;vAccel Core Library Source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cloudkernels/vaccel/releases/tag/v0.5.0" target="_blank" rel="noopener"&gt;vAccel v0.5.0 Release&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Remote FPGA acceleration: vAccel on PYNQ-Z1</title><link>/blog/pynq-z1-vaccel/</link><pubDate>Fri, 20 Jan 2023 10:14:04 +0100</pubDate><guid>/blog/pynq-z1-vaccel/</guid><description>&lt;p&gt;Running applications that need hardware acceleration in public clouds remains a
challenge, both for end-users and for service providers. The reasons for this are
mostly related to the complicated hardware abstractions that acceleration devices expose,
as well as to the complicated software stacks that drive these devices.&lt;/p&gt;
&lt;p&gt;In an effort to hide the complexity of the software stack under the hood, and
provide end-users with the capability of accelerating their applications, we
have introduced &lt;a href="https://vaccel.org" target="_blank" rel="noopener"&gt;vAccel&lt;/a&gt;, a hardware acceleration
abstraction to semantically expose functions that can be accelerated to
workloads running in VMs or even remote hosts.&lt;/p&gt;
&lt;p&gt;In this post, we will present how we can use vAccel to remotely execute basic
FPGA operations on a PYNQ-Z1 board. First, we go through a brief description of
the hardware and software components of this example, as well as the steps to
reproduce the experiment. We install the vAccel software stack to the board
running a generic linux distribution and run a local example. Then, we run the
same example remotely, using a client machine connected to the same network as
our development board.&lt;/p&gt;
&lt;h2 id="overview"&gt;Overview&lt;/h2&gt;
&lt;p&gt;As mentioned above, we are using a PYNQ-Z1 development board. The PYNQ-Z1 board
is the hardware platform for the PYNQ open-source framework. It features a
Zynq-7000 (XC7Z020-1CLG400C) All Programmable System-On-Chip (APSoC),
integrating a feature-rich dual-core Cortex-A9 based processing system (PS) and
Xilinx programmable logic (PL) in a single device. Figure 1 shows an image of
the PYNQ-Z1 development board by Digilent.&lt;/p&gt;
&lt;p&gt;Apart from Petalinux, you can install a generic linux distribution. We recently
walked through the process of &lt;a href="../pynq-z1"&gt;installing debian on a PYNQ-Z1&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="install-vaccel"&gt;Install vAccel&lt;/h2&gt;
&lt;p&gt;We can use the binary release or build from source. For the sake of completeness we present both options.&lt;/p&gt;
&lt;h3 id="install-from-binaries"&gt;Install from binaries&lt;/h3&gt;
&lt;p&gt;Get the deb package for the core vAccelRT library and install it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;wget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/master/aarch32/Release-deb/vaccel-0.5.0-Linux.deb
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;sudo dpkg -i vaccel-0.5.0-Linux.deb
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We should be presented with a couple of libraries in &lt;code&gt;/usr/local/lib&lt;/code&gt; as well
as some example binaries on &lt;code&gt;/usr/local/bin&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Skip the next section and go directly to &lt;a href="#test-the-installation"&gt;Test the installation&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="build-from-source"&gt;Build from source&lt;/h3&gt;
&lt;p&gt;Clone the repo and prepare to build:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;git clone https://github.com/cloudkernels/vaccelrt --recursive
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; vaccelrt
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;mkdir -p build &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;cd&lt;/span&gt; build
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;cmake ../ -DBUILD_PLUGIN_NOOP&lt;span class="o"&gt;=&lt;/span&gt;ON -DBUILD_EXAMPLES&lt;span class="o"&gt;=&lt;/span&gt;ON
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To build and install use the following simple command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;make install
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="test-the-installation"&gt;Test the installation&lt;/h2&gt;
&lt;p&gt;To make sure we&amp;rsquo;ve got everything setup correctly, we can run a couple of examples. First we could use the &lt;code&gt;noop&lt;/code&gt; plugin to do image classification on an image.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Set the path to the vAccel libraries&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;LD_LIBRARY_PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$LD_LIBRARY_PATH&lt;/span&gt;:/usr/local/lib
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Set the plugin to noop&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_BACKENDS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/usr/local/lib/libvaccel-noop.so
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# enable Debug&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_DEBUG_LEVEL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Run the classify example&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;/usr/local/bin/classify /usr/local/share/images/example.jpg &lt;span class="m"&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We should be presented with debug output and a dummy classification tag:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;user@debian-fpga:~$ /usr/local/bin/classify /usr/local/share/images/example.jpg 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Initializing vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Created top-level rundir: /run/user/1001/vaccel.ogcPti
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Registered plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Registered function noop from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Registered function sgemm from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Registered function image classification from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Registered function image detection from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Registered function image segmentation from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Registered function image pose estimation from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Registered function image depth estimation from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Registered function exec from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Registered function TensorFlow session load from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Registered function TensorFlow session run from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Registered function TensorFlow session delete from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Registered function MinMax from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Registered function Array copy from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Registered function Vector Add from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Registered function Parallel acceleration from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Registered function Matrix multiplication from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; Loaded plugin noop from /usr/local/lib/libvaccel-noop.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.09 - &amp;lt;debug&amp;gt; session:1 New session
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Initialized session with id: 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Image size: 79281B
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.11 - &amp;lt;debug&amp;gt; session:1 Looking for plugin implementing image classification
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.11 - &amp;lt;debug&amp;gt; Found implementation in noop plugin
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[noop] Calling Image classification for session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[noop] Dumping arguments for Image classification:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[noop] len_img: 79281
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[noop] will return a dummy result
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;classification tags: This is a dummy classification tag!
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.11 - &amp;lt;debug&amp;gt; session:1 Free session
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.11 - &amp;lt;debug&amp;gt; Shutting down vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.11 - &amp;lt;debug&amp;gt; Cleaning up plugins
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:46:23.11 - &amp;lt;debug&amp;gt; Unregistered plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="run-local-example"&gt;Run local example&lt;/h2&gt;
&lt;p&gt;An example more tailored to the board we&amp;rsquo;re running on could be a vector
operation, such as a vector addition (as in
&lt;a href="https://github.com/ikwzm" target="_blank" rel="noopener"&gt;Ichiro&lt;/a&gt;&amp;rsquo;s
&lt;a href="https://github.com/ikwzm/FPGA-SoC-Linux-Example-1-PYNQ-Z1" target="_blank" rel="noopener"&gt;example&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;We already have a pre-compiled example for a vector addition in
&lt;code&gt;pynq_vector_add_generic&lt;/code&gt;. Lets try to execute it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;/usr/local/bin/pynq_vector_add_generic
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The output is similar to the above. Since we&amp;rsquo;re using the &lt;code&gt;noop&lt;/code&gt; plugin, the
result is a dummy result, only for debugging.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;user@debian-fpga:~$ /usr/local/bin/pynq_vector_add_generic
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Initializing vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Created top-level rundir: /run/user/1001/vaccel.epcZBL
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Registered plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Registered function noop from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Registered function sgemm from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Registered function image classification from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Registered function image detection from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Registered function image segmentation from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Registered function image pose estimation from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Registered function image depth estimation from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Registered function exec from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Registered function TensorFlow session load from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Registered function TensorFlow session run from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Registered function TensorFlow session delete from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Registered function MinMax from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Registered function Array copy from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Registered function Vector Add from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Registered function Parallel acceleration from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Registered function Matrix multiplication from plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Loaded plugin noop from /usr/local/lib/libvaccel-noop.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; session:1 New session
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Initialized session with id: 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; session:1 Looking for plugin implementing fpga_vector_add operation
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Found implementation in noop plugin
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[noop] Calling v_vectoradd for session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[noop] Dumping arguments for v_vectoradd:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;[noop] len_a: 5 len_b: 5
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;9.100000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;9.100000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;9.100000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;9.100000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;9.100000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; session:1 Free session
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Shutting down vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Cleaning up plugins
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:49:17.02 - &amp;lt;debug&amp;gt; Unregistered plugin noop
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="get-the-pynq-hardware-plugin"&gt;Get the PYNQ hardware plugin&lt;/h2&gt;
&lt;p&gt;Now that we have established that vAccel is working correctly on the board, we
can use the hardware plugin, built for PYNQ. It implements three vector
operations (&lt;code&gt;vector_add&lt;/code&gt;, &lt;code&gt;array_copy&lt;/code&gt; and &lt;code&gt;mmult&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;To get it, we grab the deb from the
&lt;a href="https://docs.vaccel.org/binaries" target="_blank" rel="noopener"&gt;binaries&lt;/a&gt; page of vAccel:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;wget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/plugins/pynq/main/aarch32/Release-deb/vaccelrt-plugin-pynq-0.1-Linux.deb
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;dpkg -i vaccelrt-plugin-pynq-0.1-Linux.deb
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once installed, it should place a shared object in &lt;code&gt;/usr/local/lib&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; ls -la /usr/local/lib/arm-linux-gnueabihf/libvaccel-pynq.so
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;-rw-r--r-- 1 root root 13144 Dec 25 03:41 /usr/local/lib/arm-linux-gnueabihf/libvaccel-pynq.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We use this shared object as the vAccel plugin and re-run the &lt;code&gt;pynq_vector_add_generic&lt;/code&gt;
program:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Set the path to the vAccel libraries&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;LD_LIBRARY_PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$LD_LIBRARY_PATH&lt;/span&gt;:/usr/local/lib
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Set the plugin to PYNQ&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_BACKENDS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/usr/local/lib/arm-linux-gnueabihf/libvaccel-pynq.so
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# enable Debug&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_DEBUG_LEVEL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Run the vector add example&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;/usr/local/bin/pynq_vector_add_generic
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The output should be like below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:56:07.30 - &amp;lt;debug&amp;gt; Initializing vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:56:07.30 - &amp;lt;debug&amp;gt; Created top-level rundir: /run/user/0/vaccel.kPHyje
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:56:07.31 - &amp;lt;debug&amp;gt; Registered plugin fpga_functions
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:56:07.31 - &amp;lt;debug&amp;gt; Registered function Array copy from plugin fpga_functions
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:56:07.31 - &amp;lt;debug&amp;gt; Registered function Vector Add from plugin fpga_functions
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:56:07.31 - &amp;lt;debug&amp;gt; Registered function Parallel acceleration from plugin fpga_functions
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:56:07.31 - &amp;lt;debug&amp;gt; Registered function Matrix multiplication from plugin fpga_functions
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:56:07.31 - &amp;lt;debug&amp;gt; Loaded plugin fpga_functions from /usr/local/lib/arm-linux-gnueabihf/libvaccel-pynq.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:56:07.31 - &amp;lt;debug&amp;gt; session:1 New session
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Initialized session with id: 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:56:07.31 - &amp;lt;debug&amp;gt; session:1 Looking for plugin implementing fpga_vector_add operation
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:56:07.31 - &amp;lt;debug&amp;gt; Found implementation in fpga_functions plugin
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Calling Vector Add function (FPGA) 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2.800000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2.100000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;8.500000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;3.500000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;11.299999
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:56:07.31 - &amp;lt;debug&amp;gt; session:1 Free session
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:56:07.31 - &amp;lt;debug&amp;gt; Shutting down vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:56:07.31 - &amp;lt;debug&amp;gt; Cleaning up plugins
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-02:56:07.31 - &amp;lt;debug&amp;gt; Unregistered plugin fpga_functions
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As we can see, it actually performed the addition on the two vectors. See the relevant snippet from the &lt;a href="https://github.com/cloudkernels/vaccelrt/blob/master/examples/pynq_vector_add_generic.c" target="_blank" rel="noopener"&gt;code&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-C" data-lang="C"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="p"&gt;[...]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="kt"&gt;float&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.2&lt;/span&gt; &lt;span class="p"&gt;};&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="kt"&gt;float&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;2.2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;1.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;6.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;6.1&lt;/span&gt; &lt;span class="p"&gt;};&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="p"&gt;[...]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &lt;span class="n"&gt;ret&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;vaccel_fpga_vadd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;len_a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;len_b&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id="run-remote-example"&gt;Run remote example&lt;/h2&gt;
&lt;p&gt;To be able to run the above example remotely, we need two things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;run the &lt;code&gt;vaccelrt-agent&lt;/code&gt; as a vAccel application locally&lt;/li&gt;
&lt;li&gt;run the &lt;code&gt;pynq_vector_add_generic&lt;/code&gt; program on the remote host, using the relevant
plugin that enables remote execution (&lt;code&gt;vsock&lt;/code&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="run-the-vaccelrt-agent"&gt;Run the vAccelRT Agent&lt;/h3&gt;
&lt;p&gt;The vAccelRT agent is essentially a vAccel application that on one side
consumes the vAccel API, and on the other side listens for gRPC requests from
remote hosts. To get it use the following commands:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;wget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/agent/59704ec358de8f68345556a774c60788ac957183/aarch32/release/vaccelrt-agent
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;chmod +x vaccelrt-agent
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To expose the above functionality, we use the exact same environment variables,
only this time we run the agent, not the &lt;code&gt;pynq_vector_add_generic&lt;/code&gt; program:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Set the path to the vAccel libraries&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;LD_LIBRARY_PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$LD_LIBRARY_PATH&lt;/span&gt;:/usr/local/lib
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Set the plugin to PYNQ&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_BACKENDS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/usr/local/lib/arm-linux-gnueabihf/libvaccel-pynq.so
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# enable Debug&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_DEBUG_LEVEL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# set the local endpoint&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_AGENT_ENDPOINT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;tcp://0.0.0.0:8192
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Run the vAccelRT agent&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;./vaccelrt-agent -a &lt;span class="nv"&gt;$VACCEL_AGENT_ENDPOINT&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The output should be something like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-03:03:34.06 - &amp;lt;debug&amp;gt; Initializing vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-03:03:34.06 - &amp;lt;debug&amp;gt; Created top-level rundir: /run/user/0/vaccel.apxqms
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-03:03:34.06 - &amp;lt;debug&amp;gt; Registered plugin fpga_functions
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-03:03:34.06 - &amp;lt;debug&amp;gt; Registered function Array copy from plugin fpga_functions
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-03:03:34.06 - &amp;lt;debug&amp;gt; Registered function Vector Add from plugin fpga_functions
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-03:03:34.06 - &amp;lt;debug&amp;gt; Registered function Parallel acceleration from plugin fpga_functions
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-03:03:34.06 - &amp;lt;debug&amp;gt; Registered function Matrix multiplication from plugin fpga_functions
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.21-03:03:34.06 - &amp;lt;debug&amp;gt; Loaded plugin fpga_functions from /usr/local/lib/arm-linux-gnueabihf/libvaccel-pynq.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;vaccel ttRPC server started. address: tcp://0.0.0.0:8192
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Server is running, press Ctrl + C to exit
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="run-the-application-on-the-remote-host"&gt;Run the application on the remote host&lt;/h3&gt;
&lt;p&gt;On the remote host, depending on the architecture and variant we need to setup vAccelRT and the relevant plugin that enables remote execution: &lt;code&gt;vaccelrt-plugin-vsock&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s assume it&amp;rsquo;s an &lt;code&gt;x86_64&lt;/code&gt; host. The commands needed to setup vAccel are the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Get &amp;amp; Install vAccelRT core library&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;wget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/master/x86_64/Release-deb/vaccel-0.5.0-Linux.deb
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;dpkg -i vaccel-0.5.0-Linux.deb
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Get &amp;amp; Install the vSock plugin&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;wget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/plugins/vsock/master/x86_64/Release-deb/vaccelrt-plugin-vsock-0.1.0-Linux.deb
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;dpkg -i vaccelrt-plugin-vsock-0.1.0-Linux.deb
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now we should have the following on &lt;code&gt;/usr/local/lib&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; tree /usr/local/lib/
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;/usr/local/lib/
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;├── libmytestlib.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;├── libvaccel-noop.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;├── libvaccel-python.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;├── libvaccel.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;├── libvaccel-vsock.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="err"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;0 directories, 5 files
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As previously the process to execute the vAccel application is the same, with the only difference that we need to point the plugin to the IP address and port where the vAccelRT Agent listens:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-sh" data-lang="sh"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Set the path to the vAccel libraries&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;LD_LIBRARY_PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$LD_LIBRARY_PATH&lt;/span&gt;:/usr/local/lib
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# enable Debug&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_DEBUG_LEVEL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Set the plugin to VSOCK&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_BACKENDS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/usr/local/lib/libvaccel-vsock.so
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# set the IP &amp;amp; port&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;VACCEL_VSOCK&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;tcp://192.168.4.21:8192
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="c1"&gt;# Run the program&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;/usr/local/bin/pynq_vector_add_generic
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The output should be something similar to this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-console" data-lang="console"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="gp"&gt;$&lt;/span&gt; /usr/local/bin/pynq_vector_add_generic
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.18 - &amp;lt;debug&amp;gt; Initializing vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.18 - &amp;lt;debug&amp;gt; Created top-level rundir: /run/user/0/vaccel.X1hPel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.20 - &amp;lt;debug&amp;gt; Registered plugin vsock
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.20 - &amp;lt;debug&amp;gt; vsock is a VirtIO module
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.20 - &amp;lt;debug&amp;gt; Registered function sgemm from plugin vsock
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.20 - &amp;lt;debug&amp;gt; Registered function image classification from plugin vsock
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.20 - &amp;lt;debug&amp;gt; Registered function image detection from plugin vsock
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.20 - &amp;lt;debug&amp;gt; Registered function image segmentation from plugin vsock
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.20 - &amp;lt;debug&amp;gt; Registered function image depth estimation from plugin vsock
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.20 - &amp;lt;debug&amp;gt; Registered function image pose estimation from plugin vsock
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.20 - &amp;lt;debug&amp;gt; Registered function TensorFlow session load from plugin vsock
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.20 - &amp;lt;debug&amp;gt; Registered function TensorFlow session delete from plugin vsock
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.20 - &amp;lt;debug&amp;gt; Registered function TensorFlow session run from plugin vsock
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.20 - &amp;lt;debug&amp;gt; Registered function MinMax from plugin vsock
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.20 - &amp;lt;debug&amp;gt; Registered function Array copy from plugin vsock
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.20 - &amp;lt;debug&amp;gt; Registered function Matrix multiplication from plugin vsock
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.20 - &amp;lt;debug&amp;gt; Registered function Vector Add from plugin vsock
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.20 - &amp;lt;debug&amp;gt; Registered function Parallel acceleration from plugin vsock
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.20 - &amp;lt;debug&amp;gt; Registered function exec from plugin vsock
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.20 - &amp;lt;debug&amp;gt; Loaded plugin vsock from /usr/local/lib/libvaccel-vsock.so
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.21 - &amp;lt;debug&amp;gt; [vsock] Initializing session
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.21 - &amp;lt;debug&amp;gt; [vsock] New session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.21 - &amp;lt;debug&amp;gt; session:1 New session
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;Initialized session with id: 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.21 - &amp;lt;debug&amp;gt; session:1 Looking for plugin implementing fpga_vector_add operation
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.21 - &amp;lt;debug&amp;gt; Found implementation in vsock plugin
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2.800000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2.100000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;8.500000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;3.500000
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;11.299999
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.22 - &amp;lt;debug&amp;gt; [vsock] Destroying session 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.22 - &amp;lt;debug&amp;gt; [vsock] Destroying vsock client
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.22 - &amp;lt;debug&amp;gt; session:1 Free session
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.22 - &amp;lt;debug&amp;gt; Shutting down vAccel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.22 - &amp;lt;debug&amp;gt; Cleaning up plugins
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="go"&gt;2023.01.20-20:16:25.22 - &amp;lt;debug&amp;gt; Unregistered plugin vsock
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That&amp;rsquo;s it! we managed to use a PYNQ-Z1 board to run simple operations on the
FPGA fabric from a remote host. In addition to that, the remote host is of
different architecture than the PYNQ board.&lt;/p&gt;
&lt;h2 id="future-steps"&gt;Future steps&lt;/h2&gt;
&lt;p&gt;As we are far from being experts on Hardware design, we plan to build a more
elaborate example for the FPGA board (e.g. an &lt;a href="https://www.hackster.io/petrohi/use-tensil-and-pynq-to-run-resnet-20-on-pynq-z1-fpga-board-c71a29" target="_blank" rel="noopener"&gt;Image inference accelerator
using
Tensil&lt;/a&gt;)
and use this as a backend for vAccel&amp;rsquo;s Image inference API ;)&lt;/p&gt;
&lt;p&gt;Give us a shout at &lt;a href="mailto:team@cloudkernels.net"&gt;team@cloudkernels.net&lt;/a&gt; if you liked it, or visit the
&lt;a href="https://vaccel.org" target="_blank" rel="noopener"&gt;vAccel&lt;/a&gt; website and drop us a note at
&lt;a href="mailto:vaccel@nubificus.co.uk"&gt;vaccel@nubificus.co.uk&lt;/a&gt; for more info!&lt;/p&gt;</description></item><item><title>vAccel - Interoperable Application Hardware Acceleration</title><link>/event/duac2022/</link><pubDate>Thu, 01 Sep 2022 09:00:00 +0000</pubDate><guid>/event/duac2022/</guid><description>&lt;h2 id="code--resources"&gt;Code &amp;amp; Resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.vaccel.org/latest/" target="_blank" rel="noopener"&gt;vAccel Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/nubificus/vaccel" target="_blank" rel="noopener"&gt;vAccel Core Library Source&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=-_mgEVP4Dnc" target="_blank" rel="noopener"&gt;vAccel Demo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Interoperable Hardware Acceleration for Serverless</title><link>/event/openinfra2022/</link><pubDate>Wed, 08 Jun 2022 09:00:00 +0000</pubDate><guid>/event/openinfra2022/</guid><description/></item><item><title>Hardware acceleration in the Age of Functions (vol II)</title><link>/blog/vaccel_v2/</link><pubDate>Fri, 04 Dec 2020 14:14:04 +0100</pubDate><guid>/blog/vaccel_v2/</guid><description>&lt;p&gt;In our &lt;a href="https://blog.cloudkernels.net/posts/vaccel/" target="_blank" rel="noopener"&gt;previous post&lt;/a&gt; we spoke about the potential solutions for
deploying serverless offerings with hardware acceleration support. With the
increasing adoption of the serverless and FaaS paradigms, providers will need
to offer some form of hardware acceleration semantics.&lt;/p&gt;
&lt;p&gt;For some time now, Amazon has &lt;a href="https://github.com/firecracker-microvm/firecracker/issues/1179" target="_blank" rel="noopener"&gt;identified&lt;/a&gt; this as a &amp;ldquo;compelling use
case&amp;rdquo; for their AWS Firecracker hypervisor which powers the Amazon Lambda
service. What is more, they identify traditional techniques for GPU support in
VMs such as GPU passthrough comes with limitations and significantly increases
the attack surface of the hypervisor.&lt;/p&gt;
&lt;p&gt;As an alternative to passing through the accelerator device inside the guest,
paravirtual interfaces can expose hardware acceleration capabilities inside
the VM with minimal overhead and offering a simple user interface for
offloading code to the host for acceleration.&lt;/p&gt;
&lt;p&gt;In fact, such interfaces already exist. &lt;code&gt;virtio-crypto&lt;/code&gt; is an example, where
the guest VM uses a simple crypto API while the actual computation is
offloaded, through the paravirtual driver, to the host.&lt;/p&gt;
&lt;p&gt;We believe that the same paradigm can be applied to any kind of computation
that can benefit from acceleration. Whether that is crypto operations, Machine
Learning or linear algebra operators, the workflow from the point of view of
the developer these days is the same; You will use a framework such as
cryptodev, Jetson Inference or the BLAS library, to write your applications and
you will not deal with the low-level complexities of the actual accelerator.
Moreover, that workflow should not be different whether your application runs
on baremetal or inside a VM.&lt;/p&gt;
&lt;p&gt;In the rest of this post we present &lt;em&gt;vAccel&lt;/em&gt;, an acceleration framework that
enables &lt;strong&gt;portable&lt;/strong&gt; and &lt;strong&gt;hardware agnostic&lt;/strong&gt; acceleration for cloud
and edge applications.&lt;/p&gt;
&lt;h2 id="vaccel-design"&gt;vAccel design&lt;/h2&gt;
&lt;p&gt;In simple terms, vAccel is an acceleration API. It offers support for a set of
operators that commonly use hardware acceleration to increase performance,
such as machine learning and linear algebra operators.&lt;/p&gt;
&lt;p&gt;The API is implemented by &lt;em&gt;VaccelRT&lt;/em&gt; a thin and efficient runtime system that links
against the user application and is responsible for dispatching operations to
the relevant hardware accelerators. The interaction with the hardware itself is
mediated by plugins which implement the API for the specific hardware
accelerator.&lt;/p&gt;
&lt;p&gt;This design is driven by our requirements for high degree of portability, an
application that consumes the vAccel API can run without modification or even
re-compilation to any platform for which there is suitable back-end plugin.&lt;/p&gt;
&lt;p&gt;In fact, this opens up the way to enable the vAccel API inside a VM guest. The missing
bits are a virtio driver that implements the vAccel API and a backend plugin that
speaks with the virtio device. Once you have this components in place, you can
run your existing vAccel application inside a VM, just by using the virtio-plugin at
runtime.&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-vaccelrt"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel_v2/vaccelrt.png#center" alt="vAccel runtime" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
VaccelRT
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id="vaccel-support-in-aws-firecracker"&gt;vAccel support in AWS Firecracker&lt;/h2&gt;
&lt;p&gt;Once we implemented the frontend vAccel-virtio driver and virtio plugin for VaccelRT,
we need a hypervisor to test this on. We already showed, in the previous post some
initial vAccel results with QEMU as the target hypervisor. In this post, we will focus
on AWS Firecracker.&lt;/p&gt;
&lt;p&gt;AWS Firecracker has been designed having in mind really small boot times and small attack
surface, which makes it a compelling choice for cloud and edge deployments.
Moreover, it powers up Lambda, Amazon&amp;rsquo;s serverless platform, which we see as a
paradigm for which vAccel&amp;rsquo;s hardware abstraction level is a perfect fit.&lt;/p&gt;
&lt;p&gt;AWS Firecracker already implements virtio backend drivers for net, block and vsock. That
was good news for us, we have all the required virtio machinery. All we had to do, was
add a new device for vAccel and link the hypervisor with VaccelRT.&lt;/p&gt;
&lt;p&gt;The last bit required us to create the necessary Rust bindings for calling C from AWS Firecracker
which is written in Rust. This was actually a good exercise for us, since we plan to anyway
provide bindings for the vAccel API in more high-level languages.&lt;/p&gt;
&lt;p&gt;With all the components in place our stack looks like this:&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-vaccel-e2e"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel_v2/vaccele2e.png#center" alt="vAccel VM execution" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
vaccel-e2e
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The user application is consumes the vAccel API and links against VaccelRT. Inside the VM
the application uses the vAccel-virtio backend plugin. When a vAccel function is called, the
plugin will offload the request to &lt;code&gt;/dev/accel&lt;/code&gt; device which is the interface of the virtio
driver. Next, the virtio driver will forward the request to the vAccel-enabled AWS Firecracker
instance which will the host-residing VaccelRT. Finally, in the host side, VaccelRT will use
one of the available plugins, to execute the operation on the hardware accelerator.&lt;/p&gt;
&lt;p&gt;But how does this perform?&lt;/p&gt;
&lt;p&gt;We first grabbed a copy of jetson-inference, a rich repo full of ML inference models and example applications based on TensorRT. We patched it to be able to run on an x86 GPU (we had an NVIDIA RTX 2060 SUPER handy), and we built the vAccelRT backend for an image classification operation. To compare vAccel on AWS firecracker we patched the example imagenet-console application to properly calculate the time of execution and to account for more than 1 iteration. The average execution time for image classification on two sets of Image files (set &amp;ldquo;*_0.jpg&amp;rdquo; and &amp;ldquo;*_1.jpg) are shown below:&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-vaccel-bf-0"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel_v2/vaccel_bf_0.png#center" alt="vAccel VM execution" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
vaccel-bf-0
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The set above is sorted by the overhead percentage (GUEST vs HOST), while the set below, is sorted by Image size (in KB). One thing to note is that on all cases, the overhead of running an image classification operation in the guest compared to the host is less than 5%.&lt;/p&gt;
&lt;p&gt;
&lt;figure id="figure-vaccel-bf-1"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel_v2/vaccel_bf_1.png#center" alt="vAccel VM execution" loading="lazy" data-zoomable /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
vaccel-bf-1
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id="putting-it-all-together"&gt;Putting it all together&lt;/h2&gt;
&lt;p&gt;So, are you brave (or curious) enough to try it out yourself ? Full disclosure, vAccel is WiP, in terms of software development terms, the project should be considered in a pre-alpha phase. However, since we think the idea is useful, there might be someone willing to try it out. So here we go:&lt;/p&gt;
&lt;p&gt;TL;DR
The easy way to try vAccel on AWS Firecracker is to run the docker container bundled with all necessary libraries/tools etc. The only prerequisite is [Docker][https://github.com/NVIDIA/nvidia-container-runtime] &amp;amp; nvidia-container-runtime[https://github.com/NVIDIA/nvidia-container-runtime] installed.&lt;/p&gt;
&lt;p&gt;To fire a VM up try running:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;docker run -e LD_LIBRARY_PATH=/usr/local/lib -e VACCEL_BACKENDS=/usr/local/lib/libvaccel-jetson.so --rm -it --gpus all --privileged nubificus/jetson-runtime
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now what the above command does is the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;sets up a couple of env vars to let the container know where to find the necessary libraries&lt;/li&gt;
&lt;li&gt;runs in a privileged mode so that /dev/kvm is available to the container instance (we need to boot a VM in there ;))&lt;/li&gt;
&lt;li&gt;provides access to the GPU from the container.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The entrypoint for the above container image (&lt;code&gt;nubificus/jetson-runtime&lt;/code&gt;) simply starts a firecracker VM with a pre-built kernel &amp;amp; rootfs.img, available &lt;a href="https://github.com/nubificus/fc-x86-guest-build" target="_blank" rel="noopener"&gt;here&lt;/a&gt;. This repository contains the dockerfile from which these binaries have been produced. You can download ready-made binaries from the &lt;a href="https://github.com/nubificus/fc-x86-guest-build/releases/latest" target="_blank" rel="noopener"&gt;releases&lt;/a&gt; page.&lt;/p&gt;
&lt;p&gt;If (for any reason) you want to try out jetson-inference, without the AWS Firecracker VM boot, then just run the container with /bin/bash as an entrypoint, using the following command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;docker run --network=host --rm -it --gpus all --privileged -v nubificus/jetson-runtime /bin/bash
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="host"&gt;Host&lt;/h3&gt;
&lt;p&gt;The current available version of vAccelRT supports the jetson-inference plugin. Adding a new plugin is as easy as linking with vAccelRT and writing the glue code in the plugin directory &amp;ndash; more information should be available in the coming weeks!&lt;/p&gt;
&lt;p&gt;To use this plugin, the Host machine should have an NVIDIA GPU (supporting CUDA), the relevant drivers installed, as well as &lt;a href="https://github.com/dusty-nv/jetson-inference" target="_blank" rel="noopener"&gt;jetson-inference&lt;/a&gt; installed.&lt;/p&gt;
&lt;p&gt;The next step is to build &amp;amp; install &lt;a href="https://github.com/cloudkernels/vaccelrt" target="_blank" rel="noopener"&gt;vAccelRT&lt;/a&gt;, the glue that ties everything together. You can build it from source, or just install the binaries available from the &lt;a href="https://github.com/cloudkernels/vaccelrt/releases/latest" target="_blank" rel="noopener"&gt;releases&lt;/a&gt; page. Make sure you specify &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; to the folder where libvaccel.so is located, as well as to choose the necessary backend by setting &lt;code&gt;VACCEL_BACKENDS&lt;/code&gt; to &lt;code&gt;libvaccel-jetson.so&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-gdscript3" data-lang="gdscript3"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="n"&gt;LD_LIBRARY_PATH&lt;/span&gt;&lt;span class="o"&gt;=/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;export&lt;/span&gt; &lt;span class="n"&gt;VACCEL_BACKENDS&lt;/span&gt;&lt;span class="o"&gt;=/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;local&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;libvaccel&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;jetson&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;so&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id="vm"&gt;VM&lt;/h3&gt;
&lt;p&gt;To run a vAccel-enabled VM, we need four basic components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the AWS firecracker VMM (with the vAccel backend patch) &lt;a href="https://github.com/cloudkernels/firecracker" target="_blank" rel="noopener"&gt;github&lt;/a&gt; &lt;a href="https://github.com/cloudkernels/firecracker/releases/latest" target="_blank" rel="noopener"&gt;releases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;a firecracker guest Linux kernel supporting modules + the virtio-accel module &lt;a href="https://github.com/nubificus/fc-x86-guest-build" target="_blank" rel="noopener"&gt;github&lt;/a&gt; &lt;a href="https://github.com/nubificus/fc-x86-guest-build/releases/latest" target="_blank" rel="noopener"&gt;releases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;the vAccel runtime system (vAccelRT) for the Host and the guest &lt;a href="https://github.com/cloudkernels/vaccelrt" target="_blank" rel="noopener"&gt;github&lt;/a&gt; &lt;a href="https://github.com/cloudkernels/vaccelrt/releases/latest" target="_blank" rel="noopener"&gt;releases&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To facilitate the process of packing all these software components, we include links to binaries built from the respective github repositories.&lt;/p&gt;
&lt;p&gt;Grab, or build &lt;code&gt;vmlinux&lt;/code&gt; &amp;amp; the &lt;code&gt;rootfs.img&lt;/code&gt; from the links above, and use a template config for firecracker like the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;boot-source&amp;#34;: {
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;kernel_image_path&amp;#34;: &amp;#34;vmlinux&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;boot_args&amp;#34;: &amp;#34;console=ttyS0 reboot=k panic=1 pci=off loglevel=0 root=/dev/vda quiet&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; },
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;drives&amp;#34;: [
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;drive_id&amp;#34;: &amp;#34;rootfs&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;path_on_host&amp;#34;: &amp;#34;rootfs.img&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;is_root_device&amp;#34;: true,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;is_read_only&amp;#34;: false
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ],
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;network-interfaces&amp;#34;: [
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;iface_id&amp;#34;: &amp;#34;eth0&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;guest_mac&amp;#34;: &amp;#34;AA:FC:00:00:00:01&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;host_dev_name&amp;#34;: &amp;#34;tap&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; ],
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;crypto&amp;#34;: {
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;crypto_dev_id&amp;#34;: &amp;#34;vaccel0&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;host_crypto_dev&amp;#34;: &amp;#34;/dev/vaccel0&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; },
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;machine-config&amp;#34;: {
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;vcpu_count&amp;#34;: 1,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;mem_size_mib&amp;#34;: 1024,
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; &amp;#34;ht_enabled&amp;#34;: false
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; }
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Make sure vmlinux, rootfs.img are in the same directory as the invocation of the firecracker command. Also, ensure that you have set &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; and &lt;code&gt;VACCEL_BACKENDS&lt;/code&gt; correctly, and that you&amp;rsquo;ve downloaded the ML networks needed for inference. This step can be done using &lt;a href="https://github.com/dusty-nv/jetson-inference/blob/master/tools/download-models.sh" target="_blank" rel="noopener"&gt;this script&lt;/a&gt;. Just get this script and run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-gdscript3" data-lang="gdscript3"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="o"&gt;./&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sh&lt;/span&gt; &lt;span class="n"&gt;NO&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;and the models will be placed by default at &lt;code&gt;../data/networks&lt;/code&gt;. Change the path in the script if you need to. For the AWS Firecracker backend to work, we need the models in the same directory as the invocation of the firecracker binary, in a folder called networks.&lt;/p&gt;
&lt;p&gt;Now, we&amp;rsquo;re ready to fire up our VM:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;firecracker --api-sock /tmp/fc.sock --config-file config_vaccel.json --seccomp-level 0
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;aaand we get the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;2020-12-04T14:39:02.363414625 [anonymous-instance:WARN:src/devices/src/legacy/i8042.rs:126] Failed to trigger i8042 kbd interrupt (disabled by guest OS)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;2020-12-04T14:39:02.369542154 [anonymous-instance:WARN:src/devices/src/legacy/i8042.rs:126] Failed to trigger i8042 kbd interrupt (disabled by guest OS)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;SELinux: Could not open policy file &amp;lt;= /etc/selinux/targeted/policy/policy.32: No such file or directory
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;[ 0.970330] systemd[1]: Failed to find module &amp;#39;autofs4&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;[UNSUPP] Starting of Arbitrary Exec…Automount Point not supported.
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;Ubuntu 20.04.1 LTS vaccel-guest.nubificus.co.uk ttyS0
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;vaccel-guest login:
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Some harmless messages and a login prompt! Try &lt;code&gt;root&lt;/code&gt; for username (no password).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;vaccel-guest login: root
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;Welcome to Ubuntu 20.04.1 LTS (GNU/Linux 4.20.0 x86_64)
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; * Documentation: https://help.ubuntu.com
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; * Management: https://landscape.canonical.com
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt; * Support: https://ubuntu.com/advantage
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;This system has been minimized by removing packages and content that are
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;not required on a system that users do not log into.
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;To restore this content, you can run the &amp;#39;unminimize&amp;#39; command.
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;The programs included with the Ubuntu system are free software;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;the exact distribution terms for each program are described in the
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;individual files in /usr/share/doc/*/copyright.
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;applicable law.
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;root@vaccel-guest:~#
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Bear in mind, the rootfs.img is based on docker hub&amp;rsquo;s ubuntu latest.&lt;/p&gt;
&lt;p&gt;Now lets do some image classification! Try running the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-fallback" data-lang="fallback"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;root@vaccel-guest:~# ./classify images/airplane_1.jpg 1
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;Initialized session with id: 1
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;Image size: 115835B
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;classification tags: 21.973% warplane, military plane
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first time the execution might take longer, but any consecutive run will be significantly faster. This is because the acceleration backend (jetson-inference) needs to examine the hardware available, generate and load the necessary components for the model to run correctly.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s it! you just ran an image classification example for a JPG image, in an AWS Firecracker VM, talking vAccelRT, which, in turn, forwards this request to AWS Firecracker vAccel backend, calling vAccelRT, jetson-inference, talking to the GPU and voila!&lt;/p&gt;
&lt;h2 id="future-steps"&gt;Future steps&lt;/h2&gt;
&lt;p&gt;Argh! you made it this far? :D If you enjoyed playing with hardware
acceleration &amp;amp; Firecracker stay tuned to enjoy some more of this on aarch64
devices (yeap, the NVIDIA Jetson Nano!). Last but not least, we are in the
process of integrating vAccel-enabled AWS Firecracker instances to k8s and k3s
to facilitate deployment and scaling.&lt;/p&gt;
&lt;p&gt;Give us a shout at &lt;a href="mailto:team@cloudkernels.net"&gt;team@cloudkernels.net&lt;/a&gt; if you liked it, or visit the &lt;a href="https://vaccel.org" target="_blank" rel="noopener"&gt;vAccel&lt;/a&gt; website and drop us a note at &lt;a href="mailto:vaccel@nubificus.co.uk"&gt;vaccel@nubificus.co.uk&lt;/a&gt; for more info!&lt;/p&gt;</description></item><item><title>Hardware acceleration in the Age of Functions</title><link>/blog/vaccel/</link><pubDate>Mon, 01 Jun 2020 20:29:17 +0000</pubDate><guid>/blog/vaccel/</guid><description>&lt;p&gt;The debate on how to deploy applications, monoliths or micro services, is in
full swing. Part of this discussion relates to how the new paradigm
incorporates support for accessing accelerators, e.g. GPUs, FPGAs. That kind
of support has been made available to traditional programming models the last
couple of decades and its tooling has evolved to be stable and standardized.&lt;/p&gt;
&lt;p&gt;On the other hand, what does it mean for a serverless setup to access an
accelerator? Should the function invoked to classify an image, for instance,
link against the whole acceleration runtime and program the hardware device
itself? It seems quite counter-intuitive to create such bloated functions.&lt;/p&gt;
&lt;p&gt;Things get more complicated when we consider the low-level layers of the
service architecture. How does the system itself get access to the
acceleration hardware? Docker allows exposing a GPU device inside a container
for some time now, so serverless systems based on top of it can expose GPU
devices to running functions. Virtual Machine-based setups rely on the
monitor, e.g. QEMU or &lt;a href="https://firecracker-microvm.github.io/" target="_blank" rel="noopener"&gt;Firecracker&lt;/a&gt;, to expose acceleration devices to the
guest.&lt;/p&gt;
&lt;p&gt;There are several techniques used to expose a device from the host to a guest
VM. Passthrough mode exposes the hardware accelerator as is inside the guest.
This mode provides native performance using the accelerator from inside the VM,
however it does cause issues with sharing the device across multiple VMs. API
remoting, e.g. &lt;a href="http://rcuda.net/" target="_blank" rel="noopener"&gt;rCUDA&lt;/a&gt;, is another option, where requests are being
forwarded to the accelerator device over the network. Finally, there is the
option of paravirtual interfaces where the monitor exposes a generic device to
the guest, with a very simple API. Applications in the guest send requests to
the paravirtual device which are then passed to the hypervisor and dispatched
by the latter to an accelerator device on the host.&lt;/p&gt;
&lt;p&gt;VirtIO drivers are an example of such paravirtualized frameworks. VirtIO
exposes simple front-end device drivers to the guest, rather than emulating
complex devices and offloads the complexity of interacting with the hardware
to the back-end that lives in the Virtual Machine Monitor (VMM).&lt;/p&gt;
&lt;h3 id="virtio-crypto"&gt;virtio-crypto&lt;/h3&gt;
&lt;p&gt;One of the devices described in the VirtIO spec is the &lt;a href="https://github.com/gongleiarei/virtio" target="_blank" rel="noopener"&gt;virtio-crypto&lt;/a&gt;
&lt;a href="https://github.com/gongleiarei/virtio-crypto-linux-driver" target="_blank" rel="noopener"&gt;device&lt;/a&gt;. The guest chooses the cryptographic operation to perform and
passes a pointer to the data that will be manipulated. The actual operation is
offloaded through the VMM to the host crypto acceleration device.&lt;/p&gt;
&lt;p&gt;A VM is able to use a crypto device by using a combination of
&lt;em&gt;cryptodev&lt;/em&gt; and &lt;em&gt;virtio-crypto&lt;/em&gt;. Requests for encryption /
decryption originating from the VM, get forwarded to the backend, get injected
to the &lt;em&gt;cryptodev&lt;/em&gt; device and end up being handled by the host Linux
kernel. Figure 1 presents an overview of the &lt;em&gt;virtio-crypto&lt;/em&gt;
architecture.&lt;/p&gt;
&lt;figure id="figure-figure-1-virtio-crypto-architecture-overview"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel/virtio-crypto.png#center" alt="Figure 1: VirtIO-crypto architecture overview" loading="lazy" data-zoomable width="60%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 1: VirtIO-crypto architecture overview
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;In the context of micro-services (FaaS/Serverless) cryptographic operations are
quite common, presented to the user as language/library abstractions.
Integrating an off-loading mechanism of these CPU-intensive operations seems
like an interesting optimization. To showcase the potential of paravirtual
accelerated devices, we implemented a &lt;em&gt;virtio-crypto&lt;/em&gt; backend driver
for AWS Firecracker. Since &lt;em&gt;virtio-crypto&lt;/em&gt;&amp;rsquo;s frontend is already present
in the Linux kernel, all we had to do is implement the corresponding back-end
in the Firecracker code base. This effort was relatively straight-forward since
Firecracker already provides a number of VirtIO devices, e.g. net and block,
which means that all the machinery for communication with the guest was in
place.&lt;/p&gt;
&lt;p&gt;Figure 2 shows the performance our &lt;em&gt;virtio-crypto&lt;/em&gt; driver achieves (light
bars) compared to running the computation in the guest kernel using the
&lt;em&gt;cryptodev-linux&lt;/em&gt; driver (dark bars), when running the AES-CBC cipher.
Unfortunately, we have not been able to get our hands on a crypto acceleration
device, so &lt;em&gt;virtio-crypto&lt;/em&gt; is using the same &lt;em&gt;cryptodev-linux&lt;/em&gt;
device in the host (the CPU). This means that we do not actually accelerate the
operation, but our experiment is quite useful to see the VirtIO overhead of
offloading the operation to the host. As expected, the larger the block size of
the blob we are encrypting, the better we are able to hide the cost of moving
data from the userland of the guest to the kernel of the host.&lt;/p&gt;
&lt;figure id="figure-figure-2-host-and-guest-throughput-for-aes-cbc-128-vs-chunk-size"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel/aes_results.png#center" alt="Figure 2: Host and Guest Throughput for AES-CBC-128 vs chunk size" loading="lazy" data-zoomable width="80%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 2: Host and Guest Throughput for AES-CBC-128 vs chunk size
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;This is encouraging; once there is a hardware accelerator for computation,
acceleration capabilities are automatically exposed inside a Firecracker VM in
a secure way with reasonably low overhead. Which inevitably leads us to the
thought, &lt;em&gt;why only crypto?&lt;/em&gt; The &lt;em&gt;virtio-crypto&lt;/em&gt; example showcases a
simple interface through which we can achieve hardware acceleration, so why not
generalize this to other types of acceleration?&lt;/p&gt;
&lt;p&gt;This gave us the idea to define a simple, hardware-agnostic API to accelerate
any operation, as long as the host supports it. We believe that an API at this
granularity is the right abstraction for serverless frameworks, since it moves
the complexity of accelerating operations from the guest to the host.&lt;/p&gt;
&lt;h3 id="vaccel"&gt;vAccel&lt;/h3&gt;
&lt;p&gt;Let us consider a simple use-case: matrix multiplication. It is a common
operation, used in numerous applications, in HPC, Machine Learning, and Big
Data. In the generic case, the user running the application on a VM would
have to either have access to the GPU hardware and enjoy hardware acceleration,
or perform the operation on the CPU, wasting time and CPU cycles.&lt;/p&gt;
&lt;p&gt;Instead of passing through the GPU hardware, we choose a different
path: we introduce vAccel, a simple paravirtual framework that forwards
operation requests to the monitor, which, in turn, uses native calls to an
acceleration framework, taking advantage of the hardware capabilities of the
host.&lt;/p&gt;
&lt;p&gt;The vAccel framework allows workloads that execute on Virtual Machines to
offload compute-intensive functions to backends provided by the hypervisor. To
achieve this, the system presents a number of host-side accelerator functions
to the guest kernel, which are backed by hardware accelerators (FPGAs, GPUs,
specialized crypto engines etc.).&lt;/p&gt;
&lt;p&gt;vAccel consists of three main parts: the frontend driver, the backend driver
and the runtime. An overview of the system architecture is shown in Figure 3.&lt;/p&gt;
&lt;figure id="figure-figure-3-vaccel-architecture-overview"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel/vaccel.png#center" alt="Figure 3: vAccel architecture overview" loading="lazy" data-zoomable width="70%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 3: vAccel architecture overview
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Frontend and Backend drivers implement the transport layer. We base our
implementation on VirtIO, and follow the generic VirtIO spec, using a single
queue for control and data exchange.&lt;/p&gt;
&lt;p&gt;The runtime includes two components: a &lt;em&gt;host library&lt;/em&gt; that handles
offload requests, and a &lt;em&gt;guest library&lt;/em&gt; that intercepts the actual
offload-able user calls and creates those requests.&lt;/p&gt;
&lt;p&gt;The basic API is given below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-C" data-lang="C"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="k"&gt;typedef&lt;/span&gt; &lt;span class="kt"&gt;uint8_t&lt;/span&gt; &lt;span class="kt"&gt;vaccel_op_t&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="cm"&gt;/* Get available accelerate-able operations from the backend */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;vaccel_get_operations&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;uint8_t&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;available_operations&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="cm"&gt;/* Start a new session */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kt"&gt;vaccel_session_t&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nf"&gt;create_vaccel_session&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="cm"&gt;/* Invoke an acceleration operation */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;do_operation&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;vaccel_session_t&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;vaccel_op_t&lt;/span&gt; &lt;span class="n"&gt;operation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="cm"&gt;/* End a running session */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;destory_vaccel_session&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;vaccel_session_t&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To study the potential overhead of such an approach on a common function, we
deploy a generic QEMU/KVM VM on an x86 host, using an FPGA card as the
accelerator (&lt;a href="http://www.netlib.org/lapack/explore-html/db/dc9/group__single__blas__level3_gafe51bacb54592ff5de056acabd83c260.html" target="_blank" rel="noopener"&gt;SGEMM&lt;/a&gt;, implemented with OpenCL). We run the stencil on the host to
obtain a baseline, and then we execute the same benchmark on the vAccel-enabled
guest and capture the results.&lt;/p&gt;
&lt;figure id="figure-figure-4-sgemm-host--guest-results-vs-matrix-size"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel/sgemm.png#center" alt="Figure 4: SGEMM Host / Guest results vs matrix size" loading="lazy" data-zoomable width="80%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 4: SGEMM Host / Guest results vs matrix size
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Figure 4 presents the performance of SGEMM on a single core VM (light bars)
against the respective run on the host (dark bars) for various matrix sizes.
On the Y axis we plot the MFlops achieved by the SGEMM stencil, while on the X
axis we lay the size of the matrices tested. For large matrix sizes (&amp;gt;
128x128), the overhead perceived by the user is minimal, ranging from 16% to
even &amp;lt;3%.&lt;/p&gt;
&lt;h3 id="inference-at-the-edge"&gt;Inference at the edge&lt;/h3&gt;
&lt;p&gt;Let us now consider a more complicated scenario: image classification. From the
user perspective it is a simple operation: (i) provide an image as input, (ii)
define which model will be used to classify the image, (iii) wait for the
result. However, the system internals are a bit more complicated: the image has
to be preprocessed, fed to a pre-trained classification model, and mapped to a
given set of labels. This abstraction is already provided by common frameworks
such as Tensorflow, Caffe etc. However, these frameworks perform optimally with
direct access to hardware accelerators. Figure 5 presents the path to the
hardware accelerator from the VM&amp;rsquo;s userspace.&lt;/p&gt;
&lt;figure id="figure-figure-5-inference-use-case"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel/ml_legacy.png#center" alt="Figure 5: Inference use-case" loading="lazy" data-zoomable width="80%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 5: Inference use-case
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;We use vAccel to expose accelerated inference capabilities to a guest VM.
Specifically, we expose one basic function, image classification. The guest
simply issues a request with the image to be classified and the model to be
used for inference. The backend forwards this request to vAccel-runtime, which,
in turn, calls wrapper functions on top of the Tensorflow runtime to classify
the image. The result is copied back to the guest synchronously. Figure 6
presents the vAccel-enabled path.&lt;/p&gt;
&lt;figure id="figure-figure-6-inference-use-case-with-vaccel"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel/ml_vaccel.png#center" alt="Figure 6: Inference use-case with vAccel" loading="lazy" data-zoomable width="80%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 6: Inference use-case with vAccel
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Figure 7 plots the total execution time of an image classification operation
for various image sizes deployed on a generic QEMU/KVM VM on an &lt;a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-nano/" target="_blank" rel="noopener"&gt;NVIDIA Jetson
Nano&lt;/a&gt;. Dark bars indicate the time required to complete the operation on
the host, whereas light bars show the respective time spent on the guest.
Clearly, the overhead is minimal: the average overhead across all cases is 1%.&lt;/p&gt;
&lt;figure id="figure-figure-7-image-classification-with-vaccel"&gt;
&lt;div class="d-flex justify-content-center"&gt;
&lt;div class="w-100" &gt;&lt;img src="/images/vaccel/ml_results.png#center" alt="Figure 7: Image classification with vAccel" loading="lazy" data-zoomable width="80%" /&gt;&lt;/div&gt;
&lt;/div&gt;&lt;figcaption&gt;
Figure 7: Image classification with vAccel
&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;As execution moves to the Edge, following the Serverless paradigm, efficiency
is key to provide low power consumption, while at the same time increase the
quality and the diversity of services offered to the end user. Offloading
computation to specialized units is one of the most important aspects to
balance trade-offs related to resource utilization and energy-efficiency and
to minimize request-response latency.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;vAccel is being developed jointly by the &lt;a href="http://research.cslab.ece.ntua.gr" target="_blank" rel="noopener"&gt;Computing Systems Laboratory&lt;/a&gt; of
the &lt;a href="https://www.ntua.gr" target="_blank" rel="noopener"&gt;National Technical University of Athens&lt;/a&gt; and &lt;a href="https://nubificus.co.uk" target="_blank" rel="noopener"&gt;Nubificus LTD&lt;/a&gt;.
vAccel is open-source and WiP; we plan to provide an RFC for the frontend
driver to be upstreamed, as well as respective RFCs for the backends (QEMU,
Firecracker etc.).&lt;/em&gt;&lt;/p&gt;</description></item></channel></rss>