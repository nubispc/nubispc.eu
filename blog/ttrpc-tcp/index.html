<!DOCTYPE html>
<!-- This site was created with Hugo Blox. https://hugoblox.com -->
<!-- Last Published: February 16, 2026 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Hugo Blox Builder 5.9.7" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.26c458e6907dc03073573976b7f4044e.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css" integrity="" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.b2143a565ef733cce408eccf45aa36ba.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  



























  
  
  






  <meta name="author" content="Charalampos Mainas" />





  

<meta name="description" content="That’s what we thought when setting up a BERT-based hate speech classifier.
This was part of a broader experiment using
vAccel, our hardware acceleration
abstraction for AI inference across the Cloud-Edge-IoT continuum." />



<link rel="alternate" hreflang="en-us" href="/blog/ttrpc-tcp/" />
<link rel="canonical" href="/blog/ttrpc-tcp/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu_ea0c30c6d45d84f4.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu_86daab30bf3ca8be.png" />

<meta name="theme-color" content="#1565c0" />










  
  






<meta property="twitter:card" content="summary" />

  <meta property="twitter:site" content="@GetResearchDev" />
  <meta property="twitter:creator" content="@GetResearchDev" />
<meta property="twitter:image" content="/media/logo_hu_f553cb9eef83bc41.png" />



  

<meta property="og:type" content="website" />
<meta property="og:site_name" content="Nubificus" />
<meta property="og:url" content="/blog/ttrpc-tcp/" />
<meta property="og:title" content="“It’s just localhost. How slow could it be?” | Nubificus" />
<meta property="og:description" content="That’s what we thought when setting up a BERT-based hate speech classifier.
This was part of a broader experiment using
vAccel, our hardware acceleration
abstraction for AI inference across the Cloud-Edge-IoT continuum." /><meta property="og:image" content="/media/logo_hu_f553cb9eef83bc41.png" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2025-06-22T08:02:51&#43;01:00"
    />
  
  
    <meta property="article:modified_time" content="2025-06-22T08:02:51&#43;01:00">
  







  




  
  
  

  
  

  
  
  
  
  
    <script src="https://cdn.jsdelivr.net/gh/osano/cookieconsent@3.1.1/build/cookieconsent.min.js" integrity="sha512-yXXqOFjdjHNH1GND+1EO0jbvvebABpzGKD66djnUfiKlYME5HGMUJHoCaeE4D5PTG2YsSJf6dwqyUUvQvS0vaA==" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/osano/cookieconsent@3.1.1/build/cookieconsent.min.css" integrity="sha512-LQ97camar/lOliT/MqjcQs5kWgy6Qz/cCRzzRzUCfv0fotsCTC9ZHXaPQmJV8Xu/PVALfJZ7BDezl5lW3/qBxg==" crossorigin="anonymous">
  
  <script>
  window.addEventListener("load", function(){
    window.cookieconsent.initialise({
      "palette": {
        "popup": {
          "background": "#1565c0",
          "text": "rgb(255, 255, 255)"
        },
        "button": {
          "background": "rgb(255, 255, 255)",
          "text": "#1565c0"
        }
      },
      "theme": "classic",
      "content": {
        "message": "This website uses cookies to ensure you get the best experience on our website.",
        "dismiss": "Got it!",
        "link": "Learn more",
        "href": "https://www.cookiesandyou.com"
      }
    })});
  </script>



  
  <title>“It’s just localhost. How slow could it be?” | Nubificus</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="3b24f70840adb81b7e8c41f9db4ced9d" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.9e4214442a7711d35691acd58f6f6361.js"></script>

  




  <div class="page-header header--fixed">
  
  
  
  
  


<style>
.navbar-logo {
    height: 50px;
    display: flex;
    align-items: center;
}

.navbar-logo img {
    max-height: 45px;
    width: auto;
    display: block;
}

.navbar-logo .logo-dark {
    display: none;
}

body.colorscheme-dark .navbar-logo .logo-light,
body.dark .navbar-logo .logo-light {
    display: none;
}

body.colorscheme-dark .navbar-logo .logo-dark,
body.dark .navbar-logo .logo-dark {
    display: block;
}

.navbar-logo.logo-nubis,
.navbar-logo.logo-nubificus {
    display: none;
}
</style>

<script>
document.addEventListener('DOMContentLoaded', function() {
    var host = window.location.hostname;
    var isNubis = (host === 'nubis-pc.eu' || host === 'www.nubis-pc.eu');
    var cls = isNubis ? 'logo-nubis' : 'logo-nubificus';
    document.querySelectorAll('.navbar-logo.' + cls).forEach(function(el) {
        el.style.display = 'flex';
    });
    document.querySelectorAll('[data-brand]').forEach(function(el) {
        var role = el.getAttribute('data-brand');
        if (isNubis) {
            el.textContent = (role === 'primary') ? 'Nubis' : 'Nubificus';
        } else {
            el.textContent = (role === 'primary') ? 'Nubificus' : 'Nubis';
        }
    });
});
</script>










  


<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      
      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">
          <div class="navbar-logo logo-nubificus">
            <img class="logo-light" src="/media/logo-nubificus_hu_5bae3e7b5bc92c43.png" alt="Nubificus">
            <img class="logo-dark" src="/media/logo-nubificus-dark_hu_1887b26263e462ca.png" alt="Nubificus">
          </div>
          <div class="navbar-logo logo-nubis">
            <img class="logo-light" src="/media/logo-nubis.png" alt="Nubis PC">
            <img class="logo-dark" src="/media/logo-nubis-dark.png" alt="Nubis PC">
          </div>
        </a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">
          <div class="navbar-logo logo-nubificus">
            <img class="logo-light" src="/media/logo-nubificus_hu_5bae3e7b5bc92c43.png" alt="Nubificus">
            <img class="logo-dark" src="/media/logo-nubificus-dark_hu_1887b26263e462ca.png" alt="Nubificus">
          </div>
          <div class="navbar-logo logo-nubis">
            <img class="logo-light" src="/media/logo-nubis.png" alt="Nubis PC">
            <img class="logo-dark" src="/media/logo-nubis-dark.png" alt="Nubis PC">
          </div>
        </a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          

          

          
          
          
          

          
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/"><span>Home</span></a>
          </li>

          
          

          
          <li class="nav-item dropdown">
            <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Research</span><span class="caret"></span>
            </a>
            <div class="dropdown-menu">
              
                <a class="dropdown-item" href="/projects/"><span>Projects</span></a>
              
                <a class="dropdown-item" href="/publication"><span>Publications</span></a>
              
            </div>
          </li>

          
          

          
          <li class="nav-item dropdown">
            <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Solutions</span><span class="caret"></span>
            </a>
            <div class="dropdown-menu">
              
                <a class="dropdown-item" href="/solutions/edgelink/"><span>edgeLink</span></a>
              
                <a class="dropdown-item" href="/solutions/urunc/"><span>urunc</span></a>
              
                <a class="dropdown-item" href="/solutions/vaccel/"><span>vAccel</span></a>
              
            </div>
          </li>

          
          

          
          <li class="nav-item dropdown">
            <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Feed</span><span class="caret"></span>
            </a>
            <div class="dropdown-menu">
              
                <a class="dropdown-item" href="/post"><span>News</span></a>
              
                <a class="dropdown-item" href="/event"><span>Events</span></a>
              
            </div>
          </li>

          
          

          
          <li class="nav-item dropdown">
            <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>About</span><span class="caret"></span>
            </a>
            <div class="dropdown-menu">
              
                <a class="dropdown-item" href="/people"><span>People</span></a>
              
                <a class="dropdown-item" href="/contact"><span>Contact</span></a>
              
            </div>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        

        
        
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  













  

  
  
  
<div class="article-container pt-3">
  <h1>“It’s just localhost. How slow could it be?”</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Jun 22, 2025
  </span>
  

  

  

  
  
  
  

  
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>That’s what we thought when setting up a BERT-based hate speech classifier.
This was part of a broader experiment using
<a href="https://github.com/nubificus/vaccel" target="_blank" rel="noopener"><code>vAccel</code></a>, our hardware acceleration
abstraction for AI inference across the Cloud-Edge-IoT continuum.</p>
<p>We had offloading working locally (on the same physical host &amp; OS),
and started experimenting with our <a href="https://docs.vaccel.org/latest/plugins/available-plugins/transport-plugins/rpc-plugin" target="_blank" rel="noopener">transport
plugins</a>
to make sure everything works smoothly so that we can deploy that as part of a
distributed kubernetes setup. First thing to try was localhost, and we expected
communication to be lightning fast. Instead, we got&hellip; surprises.</p>
<h2 id="the-original-experiment">The original experiment</h2>
<h3 id="how-bert-works">How BERT works</h3>
<p>The BERT model (Bidirectional Encoder Representations from Transformers) is a
transformer-based architecture that maps input text to contextual embeddings.
In this example, we’re using a distilled BERT checkpoint, traced via
TorchScript (<code>cnn_trace.pt</code>), to classify short tweets into three categories:</p>
<ul>
<li>offensive-language</li>
<li>hate-speech</li>
<li>neither</li>
</ul>
<p>Each line of input (a tweet) goes through the following stages:</p>
<ul>
<li>
<p><em>Tokenization</em>
The tweet is split into word/subword tokens using a predefined vocabulary and
tokenizer (e.g. WordPiece). Each token is mapped to an integer ID.</p>
</li>
<li>
<p><em>Embedding + Encoding</em>
These token IDs are passed through BERT’s embedding layer and several
transformer encoder blocks, generating context-aware representations of each
token.</p>
</li>
<li>
<p><em>Classification Head</em>
For classification, we only use the embedding of the special [CLS] token (added
at the start). This vector is fed into a small feed forward layer that outputs
logits for each class.</p>
</li>
<li>
<p><em>Prediction</em>
The class with the highest logit is selected as the prediction.</p>
</li>
</ul>
<p>The model is serialized with TorchScript so that it can be loaded and run from
C++ or via runtime frameworks like vAccel. This avoids Python overhead and
allows seamless execution across backends (CPU, CUDA, remote offload, etc.).</p>
<p>So if we run this on a subset of an <a href="https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset" target="_blank" rel="noopener">example
dataset</a>
and see the following:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="gp">$</span> ./build-stock-cpu/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt
</span></span><span class="line"><span class="cl"><span class="go">Processing 100 lines from: build-local/tweets_100.txt
</span></span></span><span class="line"><span class="cl"><span class="go">== [Vocab Loaded] ==
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 5: Duration: 141.214 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 6: Duration: 115.528 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 7: Duration: 117.649 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 10: Duration: 69.3163 ms Prediction: neither
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 99: Duration: 117.06 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 100: Duration: 110.692 ms Prediction: hate-speech
</span></span></span><span class="line"><span class="cl"><span class="go">Average (after 4rd iteration): 92.07 ms
</span></span></span></code></pre></div><p>CPU execution on such models seems to take quite some time. If you enable GPU execution we get something really better:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="gp">$</span> ./build-stock/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt
</span></span><span class="line"><span class="cl"><span class="go">Processing 100 lines from: build-local/tweets_100.txt
</span></span></span><span class="line"><span class="cl"><span class="go">== [Vocab Loaded] ==
</span></span></span><span class="line"><span class="cl"><span class="go">== [Using GPU] ==
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 5: Duration: 7.98571 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 6: Duration: 7.81541 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 7: Duration: 7.76802 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 10: Duration: 8.22414 ms Prediction: neither
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 99: Duration: 7.76586 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 100: Duration: 7.8277 ms Prediction: hate-speech
</span></span></span><span class="line"><span class="cl"><span class="go">Average (after 4rd iteration): 7.88 ms
</span></span></span></code></pre></div><h3 id="how-vaccel-facilitates-the-execution">How vAccel facilitates the execution</h3>
<p>vAccel enables seamless interchange between <a href="https://docs.vaccel.org/latest/plugins/available-plugins/acceleration-plugins/" target="_blank" rel="noopener">hardware</a> and <a href="https://docs.vaccel.org/latest/plugins/available-plugins/transport-plugins/rpc-plugin/" target="_blank" rel="noopener">transport</a> plugins at
runtime. So given a port of this classifier to consume the vAccel API, all we
need to do is configure vAccel to use the CPU or the GPU plugin at runtime.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="gp">$</span> <span class="nb">export</span> <span class="nv">VACCEL_LOG_LEVEL</span><span class="o">=</span><span class="m">3</span>
</span></span><span class="line"><span class="cl"><span class="gp">$</span> <span class="nb">export</span> <span class="nv">VACCEL_PLUGINS</span><span class="o">=</span><span class="nv">$HOME</span>/vaccel-plugin-torch/build-cpu/src/libvaccel-torch.so
</span></span><span class="line"><span class="cl"><span class="gp">$</span> ./build-local/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt
</span></span><span class="line"><span class="cl"><span class="go">2025.06.22-13:44:38.82 - &lt;info&gt; vAccel 0.7.0-7-e67e52b6
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-13:44:38.82 - &lt;info&gt; Registered plugin torch 0.2.0-2-f80dd939-dirty
</span></span></span><span class="line"><span class="cl"><span class="go">Processing 100 lines from: build-local/tweets_100.txt
</span></span></span><span class="line"><span class="cl"><span class="go">== [Vocab Loaded] ==
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-13:44:38.86 - &lt;warn&gt; Path does not seem to have a `&lt;prefix&gt;://`
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-13:44:38.86 - &lt;warn&gt; Assuming build-local/cnn_trace.pt is a local path
</span></span></span><span class="line"><span class="cl"><span class="go">Created new model resource 1
</span></span></span><span class="line"><span class="cl"><span class="go">Initialized vAccel session 1
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 5: Duration: 142.267 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 6: Duration: 116.333 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 7: Duration: 117.776 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 10: Duration: 69.8447 ms Prediction: neither
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 99: Duration: 118.195 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 100: Duration: 111.499 ms Prediction: hate-speech
</span></span></span><span class="line"><span class="cl"><span class="go">Average (after 4rd iteration): 92.41 ms
</span></span></span></code></pre></div><p>and the equivalent GPU execution by just tweaking an environment variable:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="gp">$</span> <span class="nb">export</span> <span class="nv">VACCEL_LOG_LEVEL</span><span class="o">=</span><span class="m">3</span>
</span></span><span class="line"><span class="cl"><span class="gp">$</span> <span class="nb">export</span> <span class="nv">VACCEL_PLUGINS</span><span class="o">=</span><span class="nv">$HOME</span>/vaccel-plugin-torch/build-gpu/src/libvaccel-torch.so
</span></span><span class="line"><span class="cl"><span class="gp">$</span> ./build-local/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt
</span></span><span class="line"><span class="cl"><span class="go">2025.06.22-13:50:09.38 - &lt;info&gt; vAccel 0.7.0-7-e67e52b6
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-13:50:09.39 - &lt;info&gt; Registered plugin torch 0.2.0-2-f80dd939
</span></span></span><span class="line"><span class="cl"><span class="go">Processing 100 lines from: build-local/tweets_100.txt
</span></span></span><span class="line"><span class="cl"><span class="go">== [Vocab Loaded] ==
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-13:50:09.43 - &lt;warn&gt; Path does not seem to have a `&lt;prefix&gt;://`
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-13:50:09.43 - &lt;warn&gt; Assuming build-local/cnn_trace.pt is a local path
</span></span></span><span class="line"><span class="cl"><span class="go">Created new model resource 1
</span></span></span><span class="line"><span class="cl"><span class="go">Initialized vAccel session 1
</span></span></span><span class="line"><span class="cl"><span class="go">CUDA is available, switching to GPU mode
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 5: Duration: 8.38249 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 6: Duration: 8.20436 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 7: Duration: 8.13868 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 10: Duration: 8.22329 ms Prediction: neither
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 99: Duration: 8.17031 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 100: Duration: 8.1666 ms Prediction: hate-speech
</span></span></span><span class="line"><span class="cl"><span class="go">Average (after 4rd iteration): 8.27 ms
</span></span></span></code></pre></div><h3 id="table-summary-of-results">Table Summary of Results:</h3>
<table>
  <thead>
      <tr>
          <th>Configuration</th>
          <th>First inference (cold start)</th>
          <th>Average inference time [*]</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Stock PyTorch (CPU)</td>
          <td>531.083 ms</td>
          <td>92.07 ms</td>
      </tr>
      <tr>
          <td>vAccel (CPU)</td>
          <td>532.026 ms</td>
          <td>92.41 ms</td>
      </tr>
      <tr>
          <td>Stock PyTorch (GPU)</td>
          <td>507.915 ms</td>
          <td>7.88 ms</td>
      </tr>
      <tr>
          <td>vAccel (GPU)</td>
          <td>643.077 ms</td>
          <td>8.27 ms</td>
      </tr>
  </tbody>
</table>
<p>[*] <em>Excludes the first 4 lines to avoid cold-start effects.</em></p>
<p>So far so good. The overhead is specific, and relevant to the library calls we
do under the hood in vAccel and the actual copy of data when needed.</p>
<h3 id="remote-execution">Remote execution</h3>
<p>Given we can run this remotely over vAccel, we first test the execution on a
single node, for the sake of debugging.</p>
<p>First we spawn the agent:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="gp">$</span> <span class="nb">export</span> <span class="nv">VACCEL_PLUGINS</span><span class="o">=</span><span class="nv">$HOME</span>/vaccel-plugin-torch/build-cpu/src/libvaccel-torch.so
</span></span><span class="line"><span class="cl"><span class="gp">$</span> vaccel-rpc-agent -a unix:///tmp/bert.sock
</span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:01:10Z INFO  ttrpc::sync::server] server listen started
</span></span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:01:10Z INFO  ttrpc::sync::server] server started
</span></span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:01:10Z INFO  vaccel_rpc_agent] vAccel RPC agent started
</span></span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:01:10Z INFO  vaccel_rpc_agent] Listening on &#39;unix:///tmp/bert.sock&#39;, press Ctrl+C to exit
</span></span></span></code></pre></div><p>Then we specify the <code>RPC</code> plugin and point it to where the agent listens:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="gp">$</span> <span class="nb">export</span> <span class="nv">VACCEL_PLUGINS</span><span class="o">=</span>libvaccel-rpc.so
</span></span><span class="line"><span class="cl"><span class="gp">$</span> <span class="nb">export</span> <span class="nv">VACCEL_RPC_ADDRESS</span><span class="o">=</span>unix:///tmp/bert.sock
</span></span><span class="line"><span class="cl"><span class="gp">$</span> ./build-local/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt
</span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:02:36.25 - &lt;info&gt; vAccel 0.7.0-7-e67e52b6
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:02:36.27 - &lt;info&gt; Registered plugin rpc 0.2.0-1-eca9e440
</span></span></span><span class="line"><span class="cl"><span class="go">Processing 100 lines from: build-local/tweets_100.txt
</span></span></span><span class="line"><span class="cl"><span class="go">== [Vocab Loaded] ==
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:02:36.31 - &lt;warn&gt; Path does not seem to have a `&lt;prefix&gt;://`
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:02:36.31 - &lt;warn&gt; Assuming build-local/cnn_trace.pt is a local path
</span></span></span><span class="line"><span class="cl"><span class="go">Created new model resource 1
</span></span></span><span class="line"><span class="cl"><span class="go">Initialized vAccel session 1
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 5: Duration: 147.155 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 6: Duration: 115.631 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 7: Duration: 124.443 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 10: Duration: 68.7716 ms Prediction: neither
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 99: Duration: 123.694 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 100: Duration: 112.011 ms Prediction: hate-speech
</span></span></span><span class="line"><span class="cl"><span class="go">Average (after 4rd iteration): 92.29 ms
</span></span></span></code></pre></div><p>and the GPU equivalent execution:</p>
<p>Agent:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="gp">$</span> <span class="nb">export</span> <span class="nv">VACCEL_PLUGINS</span><span class="o">=</span><span class="nv">$HOME</span>/vaccel-plugin-torch/build-gpu/src/libvaccel-torch.so
</span></span><span class="line"><span class="cl"><span class="gp">$</span> vaccel-rpc-agent -a unix:///tmp/bert.sock
</span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:27:16Z INFO  ttrpc::sync::server] server listen started
</span></span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:27:16Z INFO  ttrpc::sync::server] server started
</span></span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:27:16Z INFO  vaccel_rpc_agent] vAccel RPC agent started
</span></span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:27:16Z INFO  vaccel_rpc_agent] Listening on &#39;unix:///tmp/bert.sock&#39;, press Ctrl+C to exit
</span></span></span></code></pre></div><p>And the classifier:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="gp">$</span> ./build-local/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt
</span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:27:28.47 - &lt;info&gt; vAccel 0.7.0-7-e67e52b6
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:27:28.49 - &lt;info&gt; Registered plugin rpc 0.2.0-1-eca9e440
</span></span></span><span class="line"><span class="cl"><span class="go">Processing 100 lines from: build-local/tweets_100.txt
</span></span></span><span class="line"><span class="cl"><span class="go">== [Vocab Loaded] ==
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:27:28.53 - &lt;warn&gt; Path does not seem to have a `&lt;prefix&gt;://`
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:27:28.53 - &lt;warn&gt; Assuming build-local/cnn_trace.pt is a local path
</span></span></span><span class="line"><span class="cl"><span class="go">Created new model resource 1
</span></span></span><span class="line"><span class="cl"><span class="go">Initialized vAccel session 1
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 5: Duration: 8.62251 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 6: Duration: 8.18224 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 7: Duration: 8.40555 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 10: Duration: 8.74234 ms Prediction: neither
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 99: Duration: 8.26042 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 100: Duration: 8.38511 ms Prediction: hate-speech
</span></span></span><span class="line"><span class="cl"><span class="go">Average (after 4rd iteration): 8.36 ms
</span></span></span></code></pre></div><p>We can see the overhead is negligible, almost identical to the local execution.
This is expected.</p>
<p>When we do that over a <code>TCP</code> socket though, we see a completely different result.</p>
<p>Again, we spawn the agent:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="gp">$</span> vaccel-rpc-agent -a tcp://0.0.0.0:8192
</span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:06:45Z INFO  ttrpc::sync::server] server listen started
</span></span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:06:45Z INFO  ttrpc::sync::server] server started
</span></span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:06:45Z INFO  vaccel_rpc_agent] vAccel RPC agent started
</span></span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:06:45Z INFO  vaccel_rpc_agent] Listening on &#39;tcp://0.0.0.0:8192&#39;, press Ctrl+C to exit
</span></span></span></code></pre></div><p>and point the <code>RPC</code> plugin to the <code>TCP</code> endpoint:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="gp">$</span> <span class="nb">export</span> <span class="nv">VACCEL_RPC_ADDRESS</span><span class="o">=</span>tcp://localhost:8192
</span></span><span class="line"><span class="cl"><span class="gp">$</span> ./build-local/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt
</span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:07:29.84 - &lt;info&gt; vAccel 0.7.0-7-e67e52b6
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:07:29.90 - &lt;info&gt; Registered plugin rpc 0.2.0-1-eca9e440
</span></span></span><span class="line"><span class="cl"><span class="go">Processing 100 lines from: build-local/tweets_100.txt
</span></span></span><span class="line"><span class="cl"><span class="go">== [Vocab Loaded] ==
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:07:29.95 - &lt;warn&gt; Path does not seem to have a `&lt;prefix&gt;://`
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:07:29.95 - &lt;warn&gt; Assuming build-local/cnn_trace.pt is a local path
</span></span></span><span class="line"><span class="cl"><span class="go">Created new model resource 1
</span></span></span><span class="line"><span class="cl"><span class="go">Initialized vAccel session 1
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 5: Duration: 187.425 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 6: Duration: 156.973 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 7: Duration: 164.083 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 10: Duration: 109.588 ms Prediction: neither
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 99: Duration: 159.926 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 100: Duration: 149.851 ms Prediction: hate-speech
</span></span></span><span class="line"><span class="cl"><span class="go">Average (after 4rd iteration): 131.94 ms
</span></span></span></code></pre></div><p>We see some overhead, which we could easily account to the network stack
(~130ms vs 90ms). Still a bit high, but one could mistake that for TCP/IP stack
traversals. However, if we do that using the GPU plugin, things get really
weird!</p>
<p>Agent spawn:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="gp">$</span> <span class="nb">export</span> <span class="nv">VACCEL_PLUGINS</span><span class="o">=</span><span class="nv">$HOME</span>/vaccel-plugin-torch/build-gpu/src/libvaccel-torch.so
</span></span><span class="line"><span class="cl"><span class="gp">$</span> vaccel-rpc-agent -a tcp://0.0.0.0:8192
</span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:09:56Z INFO  ttrpc::sync::server] server listen started
</span></span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:09:56Z INFO  ttrpc::sync::server] server started
</span></span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:09:56Z INFO  vaccel_rpc_agent] vAccel RPC agent started
</span></span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:09:56Z INFO  vaccel_rpc_agent] Listening on &#39;tcp://0.0.0.0:8192&#39;, press Ctrl+C to exit
</span></span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="gp">$</span> <span class="nb">export</span> <span class="nv">VACCEL_RPC_ADDRESS</span><span class="o">=</span>tcp://localhost:8192
</span></span><span class="line"><span class="cl"><span class="gp">$</span> ./build-local/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt
</span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:10:00.40 - &lt;info&gt; vAccel 0.7.0-7-e67e52b6
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:10:00.42 - &lt;info&gt; Registered plugin rpc 0.2.0-1-eca9e440
</span></span></span><span class="line"><span class="cl"><span class="go">Processing 100 lines from: build-local/tweets_100.txt
</span></span></span><span class="line"><span class="cl"><span class="go">== [Vocab Loaded] ==
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:10:00.46 - &lt;warn&gt; Path does not seem to have a `&lt;prefix&gt;://`
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:10:00.46 - &lt;warn&gt; Assuming build-local/cnn_trace.pt is a local path
</span></span></span><span class="line"><span class="cl"><span class="go">Created new model resource 1
</span></span></span><span class="line"><span class="cl"><span class="go">Initialized vAccel session 1
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 5: Duration: 1598.79 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 6: Duration: 50.5106 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 7: Duration: 1896.81 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 10: Duration: 90.4444 ms Prediction: neither
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 99: Duration: 90.4848 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 100: Duration: 90.3126 ms Prediction: hate-speech
</span></span></span><span class="line"><span class="cl"><span class="go">Average (after 4rd iteration): 124.18 ms
</span></span></span></code></pre></div><h3 id="updated-table-summary-of-results">Updated Table Summary of Results:</h3>
<table>
  <thead>
      <tr>
          <th>Configuration</th>
          <th>Average inference time [*]</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Stock PyTorch (CPU)</td>
          <td>92.07 ms</td>
      </tr>
      <tr>
          <td>vAccel (CPU)</td>
          <td>92.41 ms</td>
      </tr>
      <tr>
          <td>Stock PyTorch (GPU)</td>
          <td>7.88 ms</td>
      </tr>
      <tr>
          <td>vAccel (GPU)</td>
          <td>8.27 ms</td>
      </tr>
      <tr>
          <td>vAccel remote <code>UNIX</code> (CPU)</td>
          <td>92.29 ms</td>
      </tr>
      <tr>
          <td>vAccel remote <code>UNIX</code> (GPU)</td>
          <td>8.36 ms</td>
      </tr>
      <tr>
          <td>vAccel remote <code>TCP</code> (CPU)</td>
          <td>131.94 ms</td>
      </tr>
      <tr>
          <td>vAccel remote <code>TCP</code> (GPU)</td>
          <td>124.18 ms</td>
      </tr>
  </tbody>
</table>
<p>How would that be even possible? 124 ms vs 8.36 ms for the GPU execution?</p>
<h2 id="a-curious-latency-bump">A Curious Latency Bump</h2>
<p>When calling the BERT classifier from the client via <code>ttrpc</code>, we saw minimal
overhead for <code>UNIX</code> sockets (&lt;~5%) compared to native execution. This is
expected as the data path for RPC in vAccel uses copies. When running over TCP
sockets, we saw an almost <strong>10x difference</strong> (~100ms vs ~10ms). This got us
thinking what could have gone wrong in the plugin&hellip; The code is identical, the
only thing that is different is the kind of socket&hellip;</p>
<p>The classifier code itself was fast (inference ~7-9ms), over <code>UNIX</code> sockets it
was almost the same (~9-10ms); but with TCP sockets we were getting ~100ms.</p>
<h2 id="setting-the-stage">Setting the Stage</h2>
<p>We isolated the issue to the transport mechanism (<code>ttrpc-rust</code>) so we wrote a
small microbenchmark: a <code>ttrpc</code> program exchanging empty <code>protobuf</code> messages in a
tight loop.</p>
<p>The goal was to measure raw round-trip latency—no ML, no I/O, just the
transport.</p>
<p>You can find the benchmark here:
<a href="https://github.com/nubificus/ttrpc-rs-benchmark" target="_blank" rel="noopener">ttrpc-rs-benchmark</a></p>
<h2 id="reproducing-the-problem">Reproducing the Problem</h2>
<p>Here&rsquo;s how we tested:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="gp">$</span> git clone https://github.com/nubificus/ttrpc-rs-benchmark
</span></span><span class="line"><span class="cl"><span class="gp">$</span> <span class="nb">cd</span> ttrpc-rs-benchmark
</span></span><span class="line"><span class="cl"><span class="gp">$</span> cargo build --release
</span></span><span class="line"><span class="cl"><span class="gp">$</span> ./target/release/ttrpc-benchmark
</span></span><span class="line"><span class="cl"><span class="go">Running ttrpc-rust latency benchmark with 1000 iterations...
</span></span></span><span class="line"><span class="cl"><span class="go"></span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="go">Testing Unix sockets...
</span></span></span><span class="line"><span class="cl"><span class="go">Unix Socket Results:
</span></span></span><span class="line"><span class="cl"><span class="go">  Min:     58.029µs
</span></span></span><span class="line"><span class="cl"><span class="go">  Average: 73.217µs
</span></span></span><span class="line"><span class="cl"><span class="go">  Max:     887.728µs
</span></span></span><span class="line"><span class="cl"><span class="go">  P99:     116.979µs
</span></span></span><span class="line"><span class="cl"><span class="go"></span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="go">Testing TCP sockets...
</span></span></span><span class="line"><span class="cl"><span class="go">TCP Socket Results:
</span></span></span><span class="line"><span class="cl"><span class="go">  Min:     81.40514ms
</span></span></span><span class="line"><span class="cl"><span class="go">  Average: 82.004085ms
</span></span></span><span class="line"><span class="cl"><span class="go">  Max:     83.021974ms
</span></span></span><span class="line"><span class="cl"><span class="go">  P99:     82.465309ms
</span></span></span><span class="line"><span class="cl"><span class="go"></span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="go">Comparison:
</span></span></span><span class="line"><span class="cl"><span class="go">  Unix sockets are 1120.01x faster than TCP
</span></span></span></code></pre></div><p>That’s even worse that what we&rsquo;ve seen with vAccel.</p>
<h2 id="the-nagle-surprise">The Nagle Surprise</h2>
<p>We generated flamegraphs for both paths, and one thing stood out: <code>send()</code> was
stalling on the TCP path.</p>
<p>Digging deeper, we figured out the culprit: <strong>Nagle&rsquo;s algorithm</strong>.</p>
<p>Nagle tries to reduce small-packet overhead by coalescing writes; but that’s
poison for latency-sensitive communication over TCP. Especially when the
protocol uses small messages.</p>
<h2 id="disabling-nagle--without-touching-code">Disabling Nagle — Without Touching Code</h2>
<p>Unfortunately, <code>ttrpc-rust</code> does not expose a socket config option. But we
prefer not to patch the library.</p>
<p>So we wrote a preloadable shared object, <code>nodelay.so</code>, that intercepts <code>socket()</code> and <code>setsockopt()</code> to enforce <code>TCP_NODELAY</code>.</p>
<p>You can get it from <a href="https://github.com/nubificus/nodelay" target="_blank" rel="noopener">here</a>, build it like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">git clone https://github.com/nubificus/nodelay
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> nodelay
</span></span><span class="line"><span class="cl">make
</span></span></code></pre></div><p>and preload it like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="nv">LD_PRELOAD</span><span class="o">=</span>./nodelay.so ./target/release/ttrpc-benchmark
</span></span></code></pre></div><p>This reduces the TCP latency and brings it in par with <code>UNIX</code>-socket latency.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="gp">$</span> <span class="nv">LD_PRELOAD</span><span class="o">=</span>../nodelay/nodelay.so ./target/release/ttrpc-benchmark
</span></span><span class="line"><span class="cl"><span class="go">Running ttrpc-rust latency benchmark with 1000 iterations...
</span></span></span><span class="line"><span class="cl"><span class="go"></span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="go">Testing Unix sockets...
</span></span></span><span class="line"><span class="cl"><span class="go">Unix Socket Results:
</span></span></span><span class="line"><span class="cl"><span class="go">  Min:     55.985µs
</span></span></span><span class="line"><span class="cl"><span class="go">  Average: 69.387µs
</span></span></span><span class="line"><span class="cl"><span class="go">  Max:     373.772µs
</span></span></span><span class="line"><span class="cl"><span class="go">  P99:     101.441µs
</span></span></span><span class="line"><span class="cl"><span class="go"></span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="go">Testing TCP sockets...
</span></span></span><span class="line"><span class="cl"><span class="go">[hook] TCP_NODELAY enabled on socket 16
</span></span></span><span class="line"><span class="cl"><span class="go">[hook] TCP_NODELAY enabled on socket 12
</span></span></span><span class="line"><span class="cl"><span class="go">TCP Socket Results:
</span></span></span><span class="line"><span class="cl"><span class="go">  Min:     81.323µs
</span></span></span><span class="line"><span class="cl"><span class="go">  Average: 92.432µs
</span></span></span><span class="line"><span class="cl"><span class="go">  Max:     420.38µs
</span></span></span><span class="line"><span class="cl"><span class="go">  P99:     126.688µs
</span></span></span><span class="line"><span class="cl"><span class="go"></span><span class="err">
</span></span></span><span class="line"><span class="cl"><span class="err"></span><span class="go">Comparison:
</span></span></span><span class="line"><span class="cl"><span class="go">  Unix sockets are 1.33x faster than TCP
</span></span></span></code></pre></div><h2 id="running-the-original-example-using-the-nodelayso-hack">Running the original example using the <code>nodelay.so</code> hack</h2>
<p>Keeping the same settings as our last execution attempt, only now using the <code>nodelay.so</code>, we see the following:</p>
<p>Agent spawn:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="gp">$</span> <span class="nv">LD_PRELOAD</span><span class="o">=</span>../nodelay/nodelay.so vaccel-rpc-agent -a tcp://0.0.0.0:8192
</span></span><span class="line"><span class="cl"><span class="go">[hook] TCP_NODELAY enabled on socket 3
</span></span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:14:49Z INFO  ttrpc::sync::server] server listen started
</span></span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:14:49Z INFO  ttrpc::sync::server] server started
</span></span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:14:49Z INFO  vaccel_rpc_agent] vAccel RPC agent started
</span></span></span><span class="line"><span class="cl"><span class="go">[2025-06-22T15:14:49Z INFO  vaccel_rpc_agent] Listening on &#39;tcp://0.0.0.0:8192&#39;, press Ctrl+C to exit
</span></span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-console" data-lang="console"><span class="line"><span class="cl"><span class="gp">$</span> <span class="nv">LD_PRELOAD</span><span class="o">=</span>../nodelay/nodelay.so ./build-local/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt
</span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:16:19.66 - &lt;info&gt; vAccel 0.7.0-7-e67e52b6
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:16:19.68 - &lt;info&gt; Registered plugin rpc 0.2.0-1-eca9e440
</span></span></span><span class="line"><span class="cl"><span class="go">Processing 100 lines from: build-local/tweets_100.txt
</span></span></span><span class="line"><span class="cl"><span class="go">== [Vocab Loaded] ==
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:16:19.72 - &lt;warn&gt; Path does not seem to have a `&lt;prefix&gt;://`
</span></span></span><span class="line"><span class="cl"><span class="go">2025.06.22-15:16:19.72 - &lt;warn&gt; Assuming build-local/cnn_trace.pt is a local path
</span></span></span><span class="line"><span class="cl"><span class="go">Created new model resource 1
</span></span></span><span class="line"><span class="cl"><span class="go">[hook] TCP_NODELAY enabled on socket 3
</span></span></span><span class="line"><span class="cl"><span class="go">Initialized vAccel session 1
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 5: Duration: 8.61344 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 6: Duration: 8.4136 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 7: Duration: 8.69745 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 10: Duration: 8.75759 ms Prediction: neither
</span></span></span><span class="line"><span class="cl"><span class="go">[snipped]
</span></span></span><span class="line"><span class="cl"><span class="go">Line 99: Duration: 8.80859 ms Prediction: offensive-language
</span></span></span><span class="line"><span class="cl"><span class="go">Line 100: Duration: 8.30694 ms Prediction: hate-speech
</span></span></span><span class="line"><span class="cl"><span class="go">Average (after 4rd iteration): 8.50 ms
</span></span></span></code></pre></div><h3 id="final-table-summary-of-results">Final Table Summary of Results:</h3>
<table>
  <thead>
      <tr>
          <th>Configuration</th>
          <th>Average inference time [*]</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Stock PyTorch (CPU)</td>
          <td>92.07 ms</td>
      </tr>
      <tr>
          <td>vAccel (CPU)</td>
          <td>92.41 ms</td>
      </tr>
      <tr>
          <td>Stock PyTorch (GPU)</td>
          <td>7.88 ms</td>
      </tr>
      <tr>
          <td>vAccel (GPU)</td>
          <td>8.27 ms</td>
      </tr>
      <tr>
          <td>vAccel remote <code>UNIX</code> (CPU)</td>
          <td>92.29 ms</td>
      </tr>
      <tr>
          <td>vAccel remote <code>UNIX</code> (GPU)</td>
          <td>8.36 ms</td>
      </tr>
      <tr>
          <td>vAccel remote <code>TCP</code> (CPU)</td>
          <td>131.94 ms</td>
      </tr>
      <tr>
          <td>vAccel remote <code>TCP</code> (GPU)</td>
          <td>124.18 ms</td>
      </tr>
      <tr>
          <td>vAccel remote <code>TCP_NODELAY</code> (CPU)</td>
          <td>92.55 ms</td>
      </tr>
      <tr>
          <td>vAccel remote <code>TCP_NODELAY</code> (GPU)</td>
          <td>8.50 ms</td>
      </tr>
  </tbody>
</table>
<h2 id="takeaways">Takeaways</h2>
<ul>
<li>Even on localhost, <code>TCP</code> can be surprisingly slow if Nagle&rsquo;s algorithm is enabled.</li>
<li><code>UNIX</code> sockets avoid these issues entirely; but come with deployment trade-offs.</li>
<li>Flamegraphs are a powerful way to uncover unexpected bottlenecks.</li>
<li><code>LD_PRELOAD</code> is still a useful hack for tweaking behaviors without code changes.</li>
</ul>
<p>For AI workloads that rely on tight client-server loops (like ML inference
offloading), these small optimizations matter.</p>
<h2 id="resources">Resources</h2>
<ul>
<li><a href="https://github.com/nubificus/ttrpc-rs-benchmark" target="_blank" rel="noopener">ttrpc-rs-benchmark</a></li>
<li><a href="https://github.com/nubificus/nodelay" target="_blank" rel="noopener"><code>nodelay.so</code></a></li>
<li><a href="https://brooker.co.za/blog/2024/05/09/nagle.html" target="_blank" rel="noopener">Dan Brooker&rsquo;s excellent post on Nagle</a></li>
<li><a href="https://github.com/nubificus/vaccel" target="_blank" rel="noopener">vAccel</a></li>
<li><a href="https://github.com/brendangregg/Flamegraph" target="_blank" rel="noopener">flamegraph</a></li>
</ul>
<h2 id="appendix-i--flamegraphs">Appendix I &ndash; Flamegraphs</h2>
<p>To produce a flamegraph follow these steps:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo apt install linux-tools-common linux-tools-<span class="k">$(</span>uname -r<span class="k">)</span> linux-tools-generic
</span></span><span class="line"><span class="cl">git clone https://github.com/brendangregg/Flamegraph
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> Flamegraph
</span></span><span class="line"><span class="cl">sudo perf record -F <span class="m">99</span> -g -- ./my-program --arg1 myarg1 etc..
</span></span><span class="line"><span class="cl">sudo perf script &gt; out.perf
</span></span><span class="line"><span class="cl">./stackcollapse-perf.pl out.perf &gt; out.folded
</span></span><span class="line"><span class="cl">./flamegraph.pl out.folded &gt; flamegraph.svg
</span></span></code></pre></div><h2 id="appendix-ii--hardware-testbed">Appendix II &ndash; Hardware Testbed</h2>
<h3 id="hardware-testbed-summary">Hardware Testbed Summary</h3>
<table>
  <thead>
      <tr>
          <th>Component</th>
          <th>Specification</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>OS</td>
          <td>Ubuntu 24.04.2 LTS (noble)</td>
      </tr>
      <tr>
          <td>CPU</td>
          <td>AMD Ryzen 5 2600, 6 cores / 12 threads @ 3.4 GHz</td>
      </tr>
      <tr>
          <td>RAM</td>
          <td>64 GB DDR4</td>
      </tr>
      <tr>
          <td>GPU</td>
          <td>NVIDIA GeForce RTX 2060 SUPER (8 GB GDDR6)</td>
      </tr>
      <tr>
          <td>CUDA Toolkit</td>
          <td>12.0 (nvcc 12.0.140)</td>
      </tr>
      <tr>
          <td>CUDA Driver</td>
          <td>12.8 (Driver Version: 570.133.20)</td>
      </tr>
      <tr>
          <td>GPU Usage</td>
          <td>vaccel-rpc-agent (228 MiB GPU memory used during tests)</td>
      </tr>
  </tbody>
</table>
<h3 id="description">Description</h3>
<p>All benchmarks were executed on a modern workstation equipped with a 6-core AMD
Ryzen CPU and 64 GB of system memory. For GPU-accelerated runs, the machine
used an NVIDIA RTX 2060 SUPER with 8 GB of VRAM, running CUDA 12.8. The vAccel
RPC agent utilized a small portion of GPU memory during execution. Tests were
conducted under Ubuntu 24.04.2 with minimal background processes to ensure
measurement consistency.</p>
<h2 id="appendix-iii--ttrpc-rust--tiny-transport-rpc-in-rust">Appendix III &ndash; <code>ttrpc-rust</code> – Tiny Transport RPC in Rust</h2>
<p>As part of the latency benchmarking setup, we used
<a href="https://github.com/containerd/ttrpc-rust" target="_blank" rel="noopener"><code>ttrpc-rust</code></a>, a minimalist
transport abstraction for low-latency RPC-style communication. <code>ttrpc-rust</code> is
the <code>Rust</code> version of <a href="https://github.com/containerd/ttrpc" target="_blank" rel="noopener"><code>ttrpc</code></a>. <code>ttrpc</code>
is <a href="https://grpc.io/" target="_blank" rel="noopener">GRPC</a> for low-memory environments.</p>
<p>By default, ttrpc-rust supports:</p>
<ul>
<li>Unix domain sockets (fast, local IPC)</li>
<li><code>AF_VSOCK</code> sockets</li>
</ul>
<p>For our experiments, we extended it in a <a href="https://github.com/nubificus/ttrpc-rust/tree/0.8.0%2Bvaccel" target="_blank" rel="noopener">downstream
fork</a> to also
support <code>TCP</code> sockets. In addition to this, we also added an environment
variable to control the <code>TCP_NODELAY</code> feature, so that we don&rsquo;t have to do the
<code>nodelay.so</code> hack. Use with the following variable set:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-gdscript3" data-lang="gdscript3"><span class="line"><span class="cl"><span class="k">export</span> <span class="n">TTRPC_TCP_NODELAY_ENABLED</span><span class="o">=</span><span class="mi">1</span>
</span></span></code></pre></div>
    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/vaccel/">Vaccel</a>
  
  <a class="badge badge-light" href="/tag/nagle/">Nagle</a>
  
  <a class="badge badge-light" href="/tag/tcp/">TCP</a>
  
  <a class="badge badge-light" href="/tag/ttrpc/">TTRPC</a>
  
  <a class="badge badge-light" href="/tag/rust/">Rust</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=%2Fblog%2Fttrpc-tcp%2F&amp;text=%E2%80%9CIt%E2%80%99s&#43;just&#43;localhost.&#43;How&#43;slow&#43;could&#43;it&#43;be%3F%E2%80%9D" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=%2Fblog%2Fttrpc-tcp%2F&amp;t=%E2%80%9CIt%E2%80%99s&#43;just&#43;localhost.&#43;How&#43;slow&#43;could&#43;it&#43;be%3F%E2%80%9D" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
        
      
      <li>
        <a href="mailto:?subject=%E2%80%9CIt%E2%80%99s%20just%20localhost.%20How%20slow%20could%20it%20be%3F%E2%80%9D&amp;body=%2Fblog%2Fttrpc-tcp%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=%2Fblog%2Fttrpc-tcp%2F&amp;title=%E2%80%9CIt%E2%80%99s&#43;just&#43;localhost.&#43;How&#43;slow&#43;could&#43;it&#43;be%3F%E2%80%9D" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="whatsapp://send?text=%E2%80%9CIt%E2%80%99s&#43;just&#43;localhost.&#43;How&#43;slow&#43;could&#43;it&#43;be%3F%E2%80%9D%20%2Fblog%2Fttrpc-tcp%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=%2Fblog%2Fttrpc-tcp%2F&amp;title=%E2%80%9CIt%E2%80%99s&#43;just&#43;localhost.&#43;How&#43;slow&#43;could&#43;it&#43;be%3F%E2%80%9D" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>
























  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2026 Nubificus LTD. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Hugo Blox Builder</a> — the free, <a href="https://github.com/HugoBlox/hugo-blox-builder" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.5faf09821dbe513ca103e87bafd52766.js"></script>




  

  
  

  






  <script src="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js" integrity="" crossorigin="anonymous"></script>



































<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>










<script src="/en/js/wowchemy.min.e166a4f802c5f09237cc72919197d83b.js"></script>



  <script src="/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js" type="module"></script>




  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.9c0e895144aef5a693008b5c5d450147.js" type="module"></script>


















</body>
</html>
