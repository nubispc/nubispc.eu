<!doctype html><!-- This site was created with Hugo Blox. https://hugoblox.com --><!-- Last Published: February 20, 2026 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 5.9.7"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/vendor-bundle.min.26c458e6907dc03073573976b7f4044e.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.1aac88f67649a5938f00c9b82441a209.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><script async src="https://www.googletagmanager.com/gtag/js?id=G-KSC25ZE341"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(e,t){gtag("event","click",{event_category:"outbound",event_label:e,transport_type:"beacon",event_callback:function(){t!=="_blank"&&(document.location=e)}}),console.debug("Outbound link clicked: "+e)}function onClickCallback(e){if(e.target.tagName!=="A"||e.target.host===window.location.host)return;trackOutboundLink(e.target,e.target.getAttribute("target"))}gtag("js",new Date),gtag("config","G-KSC25ZE341",{anonymize_ip:!0}),gtag("set",{cookie_flags:"SameSite=None;Secure"}),document.addEventListener("click",onClickCallback,!1)</script><meta name=author content="Charalampos Mainas"><meta name=description content="
In our previous posts,
we walked through the process of configuring various low-level container
runtimes in Knative using the RuntimeClass feature of K8s. We
detailed the setup for
isolation mechanisms like
gVisor, with a special
focus on Kata and its associated hypervisors, including AWS
Firecracker and
QEMU.  Additionally, we
delved into the capabilities of unikernels, showcasing the
power of urunc in the serverless realm."><link rel=alternate hreflang=en-us href=/blog/knative-runtime-eval/><link rel=canonical href=/blog/knative-runtime-eval/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu_997e0522978c46c6.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu_409bdbda4e241619.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@nubificus"><meta property="twitter:creator" content="@nubificus"><meta property="twitter:image" content="/media/logo_hu_866fdf07312224c.png"><meta property="og:type" content="website"><meta property="og:site_name" content="Nubificus"><meta property="og:url" content="/blog/knative-runtime-eval/"><meta property="og:title" content="Optimizing Performance with Unikernels: Exploring Container Runtimes for Serverless Workloads with Knative Benchmarking | Nubificus"><meta property="og:description" content="
In our previous posts,
we walked through the process of configuring various low-level container
runtimes in Knative using the RuntimeClass feature of K8s. We
detailed the setup for
isolation mechanisms like
gVisor, with a special
focus on Kata and its associated hypervisors, including AWS
Firecracker and
QEMU.  Additionally, we
delved into the capabilities of unikernels, showcasing the
power of urunc in the serverless realm."><meta property="og:image" content="/media/logo_hu_866fdf07312224c.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2023-11-24T11:28:46+00:00"><meta property="article:modified_time" content="2023-11-24T11:28:46+00:00"><script src=https://cdn.jsdelivr.net/gh/osano/cookieconsent@3.1.1/build/cookieconsent.min.js integrity="sha512-yXXqOFjdjHNH1GND+1EO0jbvvebABpzGKD66djnUfiKlYME5HGMUJHoCaeE4D5PTG2YsSJf6dwqyUUvQvS0vaA==" crossorigin=anonymous></script><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/osano/cookieconsent@3.1.1/build/cookieconsent.min.css integrity="sha512-LQ97camar/lOliT/MqjcQs5kWgy6Qz/cCRzzRzUCfv0fotsCTC9ZHXaPQmJV8Xu/PVALfJZ7BDezl5lW3/qBxg==" crossorigin=anonymous><script>window.addEventListener("load",function(){window.cookieconsent.initialise({palette:{popup:{background:"#1565c0",text:"rgb(255, 255, 255)"},button:{background:"rgb(255, 255, 255)",text:"#1565c0"}},theme:"classic",content:{message:"This website uses cookies to ensure you get the best experience on our website.",dismiss:"Got it!",link:"Learn more",href:"https://www.cookiesandyou.com"}})})</script><title>Optimizing Performance with Unikernels: Exploring Container Runtimes for Serverless Workloads with Knative Benchmarking | Nubificus</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=58e42d9a6ac7084a0d4bc46e856c14d3><script src=/js/wowchemy-init.min.9e4214442a7711d35691acd58f6f6361.js></script><div class="page-header header--fixed"><style>.navbar-logo img{max-height:45px;width:auto;display:block}.navbar-logo .logo-dark{display:none}body.colorscheme-dark .navbar-logo .logo-light,body.dark .navbar-logo .logo-light{display:none}body.colorscheme-dark .navbar-logo .logo-dark,body.dark .navbar-logo .logo-dark{display:block}.navbar-logo.logo-nubis,.navbar-logo.logo-nubificus{display:none}</style><script>document.addEventListener("DOMContentLoaded",function(){var e=window.location.hostname,t=e==="nubis-pc.eu"||e==="www.nubis-pc.eu",n=t?"logo-nubis":"logo-nubificus";document.querySelectorAll(".navbar-logo."+n).forEach(function(e){e.style.display="flex"}),document.querySelectorAll("[data-brand]").forEach(function(e){var n=e.getAttribute("data-brand");t?e.textContent=n==="primary"?"Nubis":"Nubificus":e.textContent=n==="primary"?"Nubificus":"Nubis"})})</script><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/><div class="navbar-logo logo-nubificus"><img class=logo-light src=/media/logo-nubificus_hu_7be4fa6b19286a64.png alt=Nubificus>
<img class=logo-dark src=/media/logo-nubificus-dark_hu_bd80a6d6121ee2ff.png alt=Nubificus></div><div class="navbar-logo logo-nubis"><img class=logo-light src=/media/logo-nubis.png alt="Nubis PC">
<img class=logo-dark src=/media/logo-nubis-dark.png alt="Nubis PC"></div></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/><div class="navbar-logo logo-nubificus"><img class=logo-light src=/media/logo-nubificus_hu_7be4fa6b19286a64.png alt=Nubificus>
<img class=logo-dark src=/media/logo-nubificus-dark_hu_bd80a6d6121ee2ff.png alt=Nubificus></div><div class="navbar-logo logo-nubis"><img class=logo-light src=/media/logo-nubis.png alt="Nubis PC">
<img class=logo-dark src=/media/logo-nubis-dark.png alt="Nubis PC"></div></a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/><span>Home</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Research</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/projects/><span>Projects</span></a>
<a class=dropdown-item href=/publication><span>Publications</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Solutions</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/solutions/edgelink/><span>edgeLink</span></a>
<a class=dropdown-item href=/solutions/urunc/><span>urunc</span></a>
<a class=dropdown-item href=/solutions/vaccel/><span>vAccel</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Feed</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/post><span>News</span></a>
<a class=dropdown-item href=/event><span>Events</span></a></div></li><li class=nav-item><a class="nav-link active" href=/blog><span>Blog</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>About</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/people><span>People</span></a>
<a class=dropdown-item href=/contact><span>Contact</span></a></div></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".navbar-nav .nav-item.dropdown");e.forEach(function(t){t.addEventListener("mouseenter",function(){e.forEach(function(e){if(e!==t){e.classList.remove("show");var n=e.querySelector(".dropdown-menu");n&&n.classList.remove("show")}}),t.classList.add("show");var n=t.querySelector(".dropdown-menu");n&&n.classList.add("show")}),t.addEventListener("mouseleave",function(){t.classList.remove("show");var e=t.querySelector(".dropdown-menu");e&&e.classList.remove("show")})})})</script></div><div class=page-body><article class=article><div class=article-container><a class=blog-back-link href=javascript:void(0) onclick=blogGoBack()><i class="fas fa-arrow-left"></i> Back to Blog</a></div><div class="article-container pt-3"><h1>Optimizing Performance with Unikernels: Exploring Container Runtimes for Serverless Workloads with Knative Benchmarking</h1><div class=article-metadata><span class=article-date>Nov 24, 2023</span></div></div><div class=article-container><div class=article-style><!-- [Knative](https://knative.dev/docs/concepts/#what-is-knative) --><p>In our <a href=/posts/knative-diverse-deployments/>previous</a> <a href=/posts/urunc>posts</a>,
we walked through the process of configuring various low-level container
runtimes in Knative using the <code>RuntimeClass</code> feature of K8s. We
<a href=/posts/knative-diverse-deployments/#gvisor-deployment>detailed</a> the setup for
isolation mechanisms like
<a href=/posts/knative-diverse-deployments/#gvisor-deployment>gVisor</a>, with a special
focus on Kata and its associated hypervisors, including <a href=/posts/knative-diverse-deployments/#gvisor-deployment>AWS
Firecracker</a> and
<a href=/posts/knative-diverse-deployments/#kata-qemu>QEMU</a>. Additionally, we
<a href=/posts/urunc>delved</a> into the capabilities of unikernels, showcasing the
power of <code>urunc</code> in the serverless realm.</p><p>Now, you might be wondering: What&rsquo;s the real advantage beyond ensuring the
security isolation of workloads? Why choose one mechanism over another? And why
even dive into this conversation?</p><p>This post aims to provide insights into these questions, shedding light on the
considerations and factors at play in the dynamic landscape of low-level
container runtimes for Knative workloads.</p><h3 id=overview>Overview</h3><p>Despite achieving security isolation through sandboxed container runtimes like
Kata-containers or gVisor, it is crucial to acknowledge that running
containerized workloads in a VM, isolated from the host kernel, can introduce
significant overhead to the final execution. In serverless architectures,
where the cost of a function is directly tied to deployment time, this
factor becomes a key consideration.</p><p>Moreover, optimizing the use of hardware resources by activating them only when
necessary contributes to a &lsquo;greener&rsquo; cloud solution, reducing <a href="https://github.com/Green-Software-Foundation/sci/blob/main/Software_Carbon_Intensity/Software_Carbon_Intensity_Specification.md#:~:text=be%20expanded%20to%3A-,SCI%20%3D%20%28O%20%2B%20M%29%20per%20R,-Operational%20emissions" target=_blank rel=noopener>Software Carbon
Intensity</a>. Conventional practices, like booting an entire OS and unnecessary
libraries, can be counterproductive in terms of software stack complexity and
performance. To this end, we try to combine isolated execution of
user-workloads with optimal resource utilization and performance efficiency in
a serverless context.</p><p>To validate the above hypothesis we present an initial, high-level performance
analysis of Knative using:</p><ul><li>generic container runtimes (<code>runc</code>)</li><li>sandboxed container runtimes (<code>kata-containers</code>, <code>gVisor</code>)</li><li><code>urunc</code>, our own unikernel container runtime</li></ul><p>We base our measurements on our own
<a href=https://github.com/nubificus/kperf target=_blank rel=noopener>modified</a> version of
<a href=https://github.com/knative-extensions/kperf target=_blank rel=noopener><code>kperf</code></a>.</p><p>Our primary focus revolved around examining function-spawning latency,
response time, and scalability. To achieve this, we developed
<a href=https://github.com/nubificus/kperf-metrics-scripts/blob/main/get-metrics.py target=_blank rel=noopener>scripts</a> utilizing <code>kperf</code> to specifically measure these aspects. Below, we showcase
two experiments conducted using <code>kperf</code>.</p><ul><li>The initial set of measurements pertain to &ldquo;cold-start&rdquo; latencies,
representing the duration required for a function to spawn and respond to a request.</li><li>The second set, illustrates average latencies when concurrently spawning
multiple services.</li></ul><p>By presenting these findings, our aim is to provide users with critical metrics
that aid in selecting the most efficient low-level container runtime,
optimizing their serverless workload performance.</p><p><strong>NOTE</strong>: <code>kperf</code> development was stale for a while and Sep 15th the maintainers archived the
repo, focusing on a <a href=https://github.com/knative/serving/tree/main/test/performance target=_blank rel=noopener>different
approach</a> for
benchmarking <code>Knative</code>. We plan to gather metrics related to container runtimes
with this tool as well.</p><h3 id=architecture-overview>Architecture overview</h3><figure id=figure-figure-1-knative-serving-stock-components-and-workflow><div class="d-flex justify-content-center"><div class=w-100><img src=/images/knative/knative-runc.png#center alt="Figure 1: Knative Serving stock components and workflow." loading=lazy data-zoomable width=80%></div></div><figcaption>Figure 1: Knative Serving stock components and workflow.</figcaption></figure><p>Figure 1 depicts a typical <code>Knative</code> setup on a k8s cluster. Boxes in light
blue show the <code>Knative</code> components, while the ingress controller is assumed to
be provided by the infrastructure. Since <code>Knative</code> function pods are
essentially containers, cloud vendors <em>refrain</em> from serving multiple tenants on
shared hardware due the limitations this technology imposes on <a href=https://thenewstack.io/interview-google-gvisor-and-the-challenge-of-securing-multitenant-containers/ target=_blank rel=noopener>isolating</a> user
workloads. Consequently, these vendors opt to offer dedicated bare-metal or
virtualized infrastructure specifically for serverless tenants.</p><p>A solution to the issue above, could be to sandbox the user workload (the
Serverless Function) in an enclave that protects the host (and the rest of the
tenants) from potentially malicious code. Figure 2 shows an alternative setup
to the default, where the function pods are being spawned using sandboxed
container runtimes such as kata-containers (left) and gVisor (right).</p><figure id=figure-figure-2-knative-serving-with-sandboxed-container-runtimes><div class="d-flex justify-content-center"><div class=w-100><img src=/images/knative/knative-sandboxed.png#center alt="Figure 2: Knative Serving with sandboxed container runtimes." loading=lazy data-zoomable width=100%></div></div><figcaption>Figure 2: Knative Serving with sandboxed container runtimes.</figcaption></figure><p>This mechanism has merits and shortcomings:</p><p>cons:</p><ul><li>the sandbox spawn is on the critical path (slower cold boot times)</li><li>the code executing in the user-container runs virtualized (no straightforward
access to hardware accelerators)</li></ul><p>pros:</p><ul><li>the code executing in the user-container runs virtualized: it is much harder
for malicious user code to escape to the host and/or access sensitive data
from the system or other co-located tenants.</li></ul><h4 id=user-workloads>User workloads</h4><p>Another point of discussion is the type of workloads in a typical serverless
setup. What do users run, or better, how complicated is the code that users run
in a common serverless application?</p><p><a href=https://dl.acm.org/doi/10.1145/2451116.2451167 target=_blank rel=noopener>Unikernels</a>
have come a long way since their inception several years ago, where
they showed great potential (e.g. millisecond boot times, ultra low OS memory
consumption) but were difficult to use, hard to debug or limited in
functionality. Today, with the advent of approaches to run unmodified Linux
binaries as unikernels, or automate the building process they seem ready for
prime time.</p><p>In an effort to combine the isolation characteristics of virtualization /
sandboxed container runtimes and the lightweight nature of unikernels, we couple
unikernels and containers with <a href=https://github.com/nubificus/urunc target=_blank rel=noopener><code>urunc</code></a>
and deploy Knative functions on top of that, bringing the system management
overhead for a secure, isolated, and efficient serverless deployment to the
bare minimum.</p><p>To validate our assumptions, we measure end-to-end service latencies across various
container runtimes, including: <code>generic</code>, <code>gvisor</code>, <code>kata-qemu</code>, <code>kata-fc</code>, <code>kata-rs</code>, <code>kata-clh</code>,
and <code>urunc</code>.</p><p>We assume the bare minimum in terms of application (user-container), a simple HTTP reply program
written in
<a href=https://github.com/nubificus/helloworld-knative/blob/main/hello.go target=_blank rel=noopener>go</a> for
the generic and sandboxed functions and in
<a href=https://github.com/nubificus/app-httpreply/tree/nbfc-knative target=_blank rel=noopener>C</a> for the
unikernel function.</p><p>The individual steps that comprise the service latency measured are shown below:</p><ul><li><code>kperf</code> issues the request that reaches the ingress controller</li><li>the request traverses the networking stack of k8s and reaches the <code>activator</code></li><li>the <code>activator</code> triggers the deployment of a function pod</li><li>upon the creation of the function pod, the request reaches <code>queue-proxy</code></li><li><code>queue-proxy</code> forwards the request to the <code>user-container</code>,</li><li>the <code>user-container</code> replies to <code>kperf</code>.</li></ul><p>Figure 4 visualizes these steps in a sequence diagram, presenting which
components interact with each other, as well as the time spent at each part for
the flow.</p><figure id=figure-figure-4-end-to-end-request-servicing-on-knative-cold-instantiation><div class="d-flex justify-content-center"><div class=w-100><img src=/images/knative/knative-workflow.png#center alt="Figure 4: End-to-end request servicing on Knative (cold instantiation)." loading=lazy data-zoomable width=100%></div></div><figcaption>Figure 4: End-to-end request servicing on Knative (cold instantiation).</figcaption></figure><p>In what follows, we briefly describe the hardware and software components that
comprise our experimental setup, and present the measurements we captured.</p><h3 id=experimental-testbed>Experimental testbed</h3><p>For our measurements, we use a bare metal server from
<a href=https://www.hetzner.com/dedicated-rootserver/ax161/ target=_blank rel=noopener>Hetzner</a> with 32 physical
cores and 128GB RAM. Detailed specifications are shown in the table below:</p><table><thead><tr><th style=text-align:center>CPU</th><th style=text-align:center>Memory</th><th style=text-align:center>Storage</th><th style=text-align:center>Network</th><th style=text-align:left>OS</th></tr></thead><tbody><tr><td style=text-align:center>AMD EPYC 7502P &ldquo;Rome&rdquo; (32 Cores) Zen2</td><td style=text-align:center>4 x 32 GB DDR4 ECC</td><td style=text-align:center>960 GB NVMe SSD Datacenter Edition</td><td style=text-align:center>Internal (single server setup)</td><td style=text-align:left>Ubuntu 22.04.3 LTS (jammy)</td></tr></tbody></table><p>The software stack we use for our tests is focused around the requirements for
a generic k8s plus Knative <a href=https://knative.dev/docs/install/yaml-install/serving/install-serving-with-yaml/#prerequisites target=_blank rel=noopener>setup</a>, along with the sandboxed container runtimes and <code>urunc</code>.
Detailed components and versions are shown in the table below:</p><table><thead><tr><th style=text-align:center>Linux</th><th style=text-align:center>runc</th><th style=text-align:center>containerd</th><th style=text-align:center>nerdctl</th><th style=text-align:center>kubelet</th><th style=text-align:center>calico</th><th style=text-align:center>kata-containers</th><th style=text-align:center>kperf</th><th style=text-align:center>Knative serving</th></tr></thead><tbody><tr><td style=text-align:center>v5.15.0</td><td style=text-align:center>v1.1.9</td><td style=text-align:center>v1.7.5</td><td style=text-align:center>v0.20.0</td><td style=text-align:center>v1.28.2</td><td style=text-align:center>v3.26.0</td><td style=text-align:center>v3.2.0</td><td style=text-align:center>v20231115-78dabcb</td><td style=text-align:center>v1.12.0</td></tr></tbody></table><p>To enhance kperf, we initiated a tailored development process by forking and constructing our
unique iteration. Among the fundamental modifications crucial for efficient result collection was
the <a href=https://github.com/knative-extensions/kperf/commit/191436bfa78e4fedfb7c020e89d8373b099ef917 target=_blank rel=noopener>redirection</a>
of <code>GET</code> requests to the ingress controller with the appropriate headers
and the implementation of <a href=https://github.com/knative-extensions/kperf/commit/fca4a218acd166ff4d10cd2c65e9eacd8257a1c4 target=_blank rel=noopener>timeouts</a> inserted specifically for clients.
This was imperative to alleviate the issue of non-responsive services caused by
hardware constraints, notably the burden of spawning an excessive number of services.</p><p>With these modifications implemented, we effectively utilized kperf&rsquo;s <a href=https://github.com/knative-extensions/kperf/blob/main/docs/examples.md#scale-from-zero-and-measure-knative-service-latency target=_blank rel=noopener><code>scale</code></a>
command to gather latency statistics for both single-service function instances
and multiple services spawned simultaneously.
Moreover, the incorporation of a helper
<a href=https://github.com/nubificus/kperf-metrics-scripts/blob/main/get-metrics.py target=_blank rel=noopener>script</a>
facilitated an automated
process for retrieving this data and dynamically adjusting the low-level
runtime of workloads as well as the number of concurrently
spawned services.</p><p>Furthermore, we undertook another aspect of our work aimed at enhancing the accuracy of retrieved metrics.
This involved disabling the CPU frequency scaler, responsible for dynamically modifying the CPU cores&rsquo; frequency.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>cpupower frequency-set --governor performance
</span></span></code></pre></div><p>By disable frequency scaling, we are essentially ensuring that the CPU operates
at a constant frequency leading to more stable and accurate metric retrieval.</p><p>At the same time, limiting the CPU cores&rsquo; <code>turbo</code> feature, allows us to capture reproducible metrics.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>echo &#34;0&#34; | sudo tee /sys/devices/system/cpu/cpufreq/boost
</span></span></code></pre></div><h3 id=evaluation>Evaluation</h3><p>We plot our findings and present an initial analysis of Knative performance
over the various container runtimes we consider for our tests.</p><h4 id=service-response-latency-single-instance>Service Response Latency (single instance)</h4><p>We first test the total time needed for a single request to reach a defined,
but not provisioned, <code>Knative</code> service. Essentially, this metric represents how
much time the user will wait for a service when they first access it. Usually,
the dominating part of this time is the <em>cold-boot</em> time of the function.</p><table><thead><tr><th style=text-align:center>Parameter name</th><th style=text-align:center>Value</th><th style=text-align:center>Metric</th><th style=text-align:center>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>timeout</code></td><td style=text-align:center>3</td><td style=text-align:center>sec</td><td style=text-align:center>Duration to wait for Knative Service to be ready</td></tr><tr><td style=text-align:center><code>time-interval</code></td><td style=text-align:center>90</td><td style=text-align:center>sec</td><td style=text-align:center>The time interval of each scale up</td></tr><tr><td style=text-align:center><code>iterations</code></td><td style=text-align:center>30</td><td style=text-align:center>-</td><td style=text-align:center>nr of consecutive runs of the same test</td></tr><tr><td style=text-align:center><code>scale-client-timeout</code></td><td style=text-align:center>100</td><td style=text-align:center>sec</td><td style=text-align:center>time to give up on the operation (non-responsive service)</td></tr><tr><td style=text-align:center><code>stable-window</code></td><td style=text-align:center>25</td><td style=text-align:center>sec</td><td style=text-align:center>&ldquo;<a href=https://knative.dev/docs/serving/autoscaling/scale-bounds/#stable-window target=_blank rel=noopener>time window</a> over which metrics are averaged to provide the input for scaling decisions&rdquo;</td></tr></tbody></table><p><code>kperf</code> reports this metric as <em>Service Response Latency</em>. The <a href="https://github.com/nubificus/kperf#:~:text=default%20%22/home/ubuntu/.config/kperf/config.yaml" target=_blank rel=noopener>parameters</a> we
used for setting up <code>kperf</code> are shown in the table above.</p><table><thead><tr><th style=text-align:center>Service Latency</th><th style=text-align:center>Average</th><th style=text-align:center>Minimum</th><th style=text-align:left>Maximum</th></tr></thead><tbody><tr><td style=text-align:center>generic</td><td style=text-align:center><strong>1.20</strong></td><td style=text-align:center>0.92</td><td style=text-align:left>1.23</td></tr><tr><td style=text-align:center>gvisor</td><td style=text-align:center><strong>2.21</strong></td><td style=text-align:center>2.15</td><td style=text-align:left>2.23</td></tr><tr><td style=text-align:center>kata-qemu</td><td style=text-align:center><strong>2.21</strong></td><td style=text-align:center>2.10</td><td style=text-align:left>2.23</td></tr><tr><td style=text-align:center>kata-fc</td><td style=text-align:center><strong>2.21</strong></td><td style=text-align:center>2.21</td><td style=text-align:left>2.22</td></tr><tr><td style=text-align:center>urunc</td><td style=text-align:center><strong>1.21</strong></td><td style=text-align:center>1.19</td><td style=text-align:left>1.23</td></tr></tbody></table><p>The table above, summarizes the absolute service latency for the various
container runtimes in our test. We plot these results in Figure 4.</p><figure id=figure-figure-4-service-response-latency-as-a-function-of-the-various-container-runtimes><div class="d-flex justify-content-center"><div class=w-100><img src=/images/knative/svc_latency.png#center alt="Figure 4: Service Response Latency as a function of the various container runtimes." loading=lazy data-zoomable width=100%></div></div><figcaption>Figure 4: Service Response Latency as a function of the various container runtimes.</figcaption></figure><p>Figure 4 depicts the service response latency for the sandboxed container
runtimes and <code>urunc</code>. The horizontal lines show the average response latency,
while the vertical lines depict the minimum (lower) and maximum (higher)
response latency captured during our test.</p><p>An expected observation in Figure 4 is that the sandboxed container runtimes
require 2-2.5 seconds for servicing a request. The parameters for
each one vary depending on the sandbox technology used, but on average a request will
be served in approximately 2.21 seconds.</p><p>Moreover, using the generic container runtime, we see that a request is being
served in approximately 1.20 seconds.</p><p>Additionally, Figure 4 shows that the behavior of <code>urunc</code> is on par with the generic
container runtime (<code>runc</code>). The performance of our early version of <code>urunc</code> is
comparable to <code>runc</code> (~1.21s on average vs 1.20s for <code>runc</code>).</p><p>Finally, the maximum service request latency for <code>runc</code> and <code>urunc</code> do not
exceed 5% of the total latency.</p><p>Figure 5 plots the 99th percentile of Service latency for the various runtimes,
proving that <code>urunc</code> can sustain low latency even for slower requests.</p><figure id=figure-figure-5-service-response-latency-for-the-99th-percentile-as-a-function-of-the-various-container-runtimes><div class="d-flex justify-content-center"><div class=w-100><img src=/images/knative/svc_latency_99th.png#center alt="Figure 5: Service Response Latency for the 99th percentile as a function of the various container runtimes." loading=lazy data-zoomable width=100%></div></div><figcaption>Figure 5: Service Response Latency for the 99th percentile as a function of the various container runtimes.</figcaption></figure><p>The takeaway message from Figures 4 and 5 is that unikernels can achieve the
same or (yet to be proven) better performance than generic containers when it
comes to serverless functions!</p><h4 id=concurrent-servicing-multiple-instances>Concurrent servicing (multiple instances)</h4><p>In this test we want to assess the footprint and responsiveness of a <code>Knative</code>
service by scaling to a large number of instances.</p><table><thead><tr><th style=text-align:center>Parameter name</th><th style=text-align:center>Value</th><th style=text-align:center>Metric</th><th style=text-align:center>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>timeout</code></td><td style=text-align:center>3</td><td style=text-align:center>sec</td><td style=text-align:center>Duration to wait for Knative Service to be ready</td></tr><tr><td style=text-align:center><code>time-interval</code></td><td style=text-align:center>95</td><td style=text-align:center>sec</td><td style=text-align:center>The time interval of each scale up</td></tr><tr><td style=text-align:center><code>iterations</code></td><td style=text-align:center>15</td><td style=text-align:center>-</td><td style=text-align:center>nr of consecutive runs of the same test</td></tr><tr><td style=text-align:center><code>scale-client-timeout</code></td><td style=text-align:center>120</td><td style=text-align:center>sec</td><td style=text-align:center>time to give up on the operation (non-responsive service)</td></tr><tr><td style=text-align:center><code>stable-window</code></td><td style=text-align:center>300</td><td style=text-align:center>sec</td><td style=text-align:center>&ldquo;<a href=https://knative.dev/docs/serving/autoscaling/scale-bounds/#stable-window target=_blank rel=noopener>time window</a> over which metrics are averaged to provide the input for scaling decisions&rdquo;</td></tr></tbody></table><p>The parameters we used for setting up <code>kperf</code> can be found in the table above.</p><p>One distinction in the parameters for the single-instance scale metrics involves the extension of the stable-window duration.
This extension ensures an extended lifespan for functions/pods, thereby guaranteeing that the required number of services remains
concurrently operational.</p><figure id=figure-figure-5-service-response-latency-for-various-container-runtimes-when-servicing-multiple-parallel-requests><div class="d-flex justify-content-center"><div class=w-100><img src=/images/knative/conc_latency.png#center alt="Figure 5: Service Response Latency for various container runtimes when servicing multiple parallel requests." loading=lazy data-zoomable width=100%></div></div><figcaption>Figure 5: Service Response Latency for various container runtimes when servicing multiple parallel requests.</figcaption></figure><p>Figure 5 presents the average service response latency (sec) for each container
runtime as a function of the number of concurrent instances of the service. From this figure, some interesting conclusions can be drawn:</p><ul><li><code>generic</code>, <code>gvisor</code>, and <code>urunc</code> exhibit similar behavior when increasing the
number of instances</li><li><code>urunc</code> and <code>generic</code> appear identical in terms of response latency even when
scaling to 250 services.</li><li><code>gvisor</code> exhibits approximately an average of +1.5sec latency compared to
<code>urunc</code>. This accounts for 2x the latency for instances up to 125.</li><li><code>kata-*</code>&rsquo;s overhead ranges from 2x to 3x latency compared to <code>urunc</code> up to
125 instances and approximately 1.5x for more instances.</li></ul><p>Although the target number of services is defined, some of the instances did
not manage to produce a response after 125. Figure 6 summarizes
how many actual instances responded in our test for the various runtimes.</p><figure id=figure-figure-6-number-of-instances-spawned-for-various-container-runtimes-as-a-function-of-the-target-services-requested><div class="d-flex justify-content-center"><div class=w-100><img src=/images/knative/scale_instances_spawned.png#center alt="Figure 6: Number of Instances spawned for various container runtimes as a function of the target services requested." loading=lazy data-zoomable width=80%></div></div><figcaption>Figure 6: Number of Instances spawned for various container runtimes as a function of the target services requested.</figcaption></figure><h4 id=pushing-the-scaling-limits>Pushing the scaling limits</h4><p>To explore the maximum amount of services supported on the testbed hardware, we increase the number of services to 500 for <code>urunc</code> and capture:</p><ul><li>the number of actual services spawned</li><li>the service response latency in this context.</li></ul><p>We compare these results with the generic container runtime.</p><figure id=figure-figure-7-number-of-instances-spawned-target-500-for-various-container-runtimes-higher-is-better><div class="d-flex justify-content-center"><div class=w-100><img src=/images/knative/scale_instances.png#center alt="Figure 7: Number of Instances spawned (target 500) for various container runtimes (higher is better)." loading=lazy data-zoomable width=50%></div></div><figcaption>Figure 7: Number of Instances spawned (target 500) for various container runtimes (higher is better).</figcaption></figure><p>Figure 7 plots the number of spawned instances for the generic container
runtime and <code>urunc</code> with a target of 500. The achievable number for each
container runtime is shown in the plot. We can see that <code>urunc</code> is able to
spawn as much instances as <code>runc</code>, enabling the sandboxing of user code without
imposing any additional overhead in terms of memory / CPU footprint.</p><figure id=figure-figure-7-service-response-latency-for-multiple-concurrent-instances-500-for-various-container-runtimes-lower-is-better><div class="d-flex justify-content-center"><div class=w-100><img src=/images/knative/scale_instances_svc.png#center alt="Figure 7: Service Response Latency for multiple concurrent instances (500) for various container runtimes (lower is better)." loading=lazy data-zoomable width=50%></div></div><figcaption>Figure 7: Service Response Latency for multiple concurrent instances (500) for various container runtimes (lower is better).</figcaption></figure><p>To validate that the services are responsive and to assess the latency imposed
by so many services running at the same time, we plot the Average response
latency for these services. Figure 7 shows that <code>urunc</code> is able to sustain low
response latency compared to the generic container runtime, even with ~450
instances running.</p><h4 id=conclusions-and-next-steps>Conclusions and next steps</h4><p>In this post, we went through an initial evaluation of container runtimes that
sandbox the user code in a serverless environment such as Knative. We presented
the rationale of sandboxing serverless functions, following up from our
previous posts regarding <a href=/posts/knative-diverse-deployments>alternative container runtimes</a> for
Knative and <a href=/posts/urunc><code>urunc</code></a>, our own container runtime for unikernels.</p><p>Isolating the user code from the platform allows cloud vendors to better
utilize their infrastructure and enables users to share infrastructure without
compromising their isolation requirements. Especially in the context of
serverless computing, unikernels seem like a great fit, combining the isolation
principles of Virtual Machines, without the overhead and management burden of
generic, full virtualization stacks.</p><p>There are still a few ways to go for the wider adoption of unikernels, but
still, we are working towards this direction and, hopefully, we get to see
unikernels in production soon!</p><p>Our plan is to continue de-mystifying quirks and issues with serverless
platforms and focus on evaluating our approach on devices with lower compute
capabilities, as well as add hardware acceleration to the mix!</p></div><div class=article-tags><a class="badge badge-light" href=/tag/knative/>Knative</a>
<a class="badge badge-light" href=/tag/container/>Container</a>
<a class="badge badge-light" href=/tag/runtimes/>Runtimes</a>
<a class="badge badge-light" href=/tag/kubernetes/>Kubernetes</a>
<a class="badge badge-light" href=/tag/urunc/>Urunc</a>
<a class="badge badge-light" href=/tag/unikernels/>Unikernels</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=%2Fblog%2Fknative-runtime-eval%2F&amp;text=Optimizing+Performance+with+Unikernels%3A+Exploring+Container+Runtimes+for+Serverless+Workloads+with+Knative+Benchmarking" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=%2Fblog%2Fknative-runtime-eval%2F&amp;t=Optimizing+Performance+with+Unikernels%3A+Exploring+Container+Runtimes+for+Serverless+Workloads+with+Knative+Benchmarking" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Optimizing%20Performance%20with%20Unikernels%3A%20Exploring%20Container%20Runtimes%20for%20Serverless%20Workloads%20with%20Knative%20Benchmarking&amp;body=%2Fblog%2Fknative-runtime-eval%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=%2Fblog%2Fknative-runtime-eval%2F&amp;title=Optimizing+Performance+with+Unikernels%3A+Exploring+Container+Runtimes+for+Serverless+Workloads+with+Knative+Benchmarking" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Optimizing+Performance+with+Unikernels%3A+Exploring+Container+Runtimes+for+Serverless+Workloads+with+Knative+Benchmarking%20%2Fblog%2Fknative-runtime-eval%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=%2Fblog%2Fknative-runtime-eval%2F&amp;title=Optimizing+Performance+with+Unikernels%3A+Exploring+Container+Runtimes+for+Serverless+Workloads+with+Knative+Benchmarking" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div></div></article><script>function blogGoBack(){document.referrer&&document.referrer.indexOf("/blog")!==-1?history.back():window.location.href="/blog/"}</script></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2026 Nubificus LTD. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target=_blank rel=noopener>Hugo Blox Builder</a> — the free, <a href=https://github.com/HugoBlox/hugo-blox-builder target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.6782e3f6a4f06ea4766df038c45ecf40.js></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.b95a7e109243f29d04930ae8cb49a756.js type=module></script><script src=/en/js/wowchemy.min.ed487406ecbb80985462ba7a97b6f2d2.js></script><script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.9c0e895144aef5a693008b5c5d450147.js type=module></script></body></html>