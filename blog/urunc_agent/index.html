<!doctype html><!-- This site was created with Hugo Blox. https://hugoblox.com --><!-- Last Published: February 20, 2026 --><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 5.9.7"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/vendor-bundle.min.26c458e6907dc03073573976b7f4044e.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.1aac88f67649a5938f00c9b82441a209.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Charalampos Mainas"><meta name=description content="Over the past few years, Large Language Models (LLMs) have changed how we
interact with computers. Instead of navigating interfaces or reading
documentation, we simply describe what we want in natural language."><link rel=alternate hreflang=en-us href=/blog/urunc_agent/><link rel=canonical href=/blog/urunc_agent/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu_997e0522978c46c6.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu_409bdbda4e241619.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@GetResearchDev"><meta property="twitter:creator" content="@GetResearchDev"><meta property="twitter:image" content="/media/logo_hu_866fdf07312224c.png"><meta property="og:type" content="website"><meta property="og:site_name" content="Nubificus"><meta property="og:url" content="/blog/urunc_agent/"><meta property="og:title" content="AI Agents? Not on my host | Nubificus"><meta property="og:description" content="Over the past few years, Large Language Models (LLMs) have changed how we
interact with computers. Instead of navigating interfaces or reading
documentation, we simply describe what we want in natural language."><meta property="og:image" content="/media/logo_hu_866fdf07312224c.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2026-02-11T00:22:12+00:00"><meta property="article:modified_time" content="2026-02-11T00:22:12+00:00"><script src=https://cdn.jsdelivr.net/gh/osano/cookieconsent@3.1.1/build/cookieconsent.min.js integrity="sha512-yXXqOFjdjHNH1GND+1EO0jbvvebABpzGKD66djnUfiKlYME5HGMUJHoCaeE4D5PTG2YsSJf6dwqyUUvQvS0vaA==" crossorigin=anonymous></script><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/osano/cookieconsent@3.1.1/build/cookieconsent.min.css integrity="sha512-LQ97camar/lOliT/MqjcQs5kWgy6Qz/cCRzzRzUCfv0fotsCTC9ZHXaPQmJV8Xu/PVALfJZ7BDezl5lW3/qBxg==" crossorigin=anonymous><script>window.addEventListener("load",function(){window.cookieconsent.initialise({palette:{popup:{background:"#1565c0",text:"rgb(255, 255, 255)"},button:{background:"rgb(255, 255, 255)",text:"#1565c0"}},theme:"classic",content:{message:"This website uses cookies to ensure you get the best experience on our website.",dismiss:"Got it!",link:"Learn more",href:"https://www.cookiesandyou.com"}})})</script><title>AI Agents? Not on my host | Nubificus</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=4a9f1d5cc5b9f1724e1c65b952ed16b5><script src=/js/wowchemy-init.min.9e4214442a7711d35691acd58f6f6361.js></script><div class="page-header header--fixed"><style>.navbar-logo img{max-height:45px;width:auto;display:block}.navbar-logo .logo-dark{display:none}body.colorscheme-dark .navbar-logo .logo-light,body.dark .navbar-logo .logo-light{display:none}body.colorscheme-dark .navbar-logo .logo-dark,body.dark .navbar-logo .logo-dark{display:block}.navbar-logo.logo-nubis,.navbar-logo.logo-nubificus{display:none}</style><script>document.addEventListener("DOMContentLoaded",function(){var e=window.location.hostname,t=e==="nubis-pc.eu"||e==="www.nubis-pc.eu",n=t?"logo-nubis":"logo-nubificus";document.querySelectorAll(".navbar-logo."+n).forEach(function(e){e.style.display="flex"}),document.querySelectorAll("[data-brand]").forEach(function(e){var n=e.getAttribute("data-brand");t?e.textContent=n==="primary"?"Nubis":"Nubificus":e.textContent=n==="primary"?"Nubificus":"Nubis"})})</script><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/><div class="navbar-logo logo-nubificus"><img class=logo-light src=/media/logo-nubificus_hu_7be4fa6b19286a64.png alt=Nubificus>
<img class=logo-dark src=/media/logo-nubificus-dark_hu_bd80a6d6121ee2ff.png alt=Nubificus></div><div class="navbar-logo logo-nubis"><img class=logo-light src=/media/logo-nubis.png alt="Nubis PC">
<img class=logo-dark src=/media/logo-nubis-dark.png alt="Nubis PC"></div></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/><div class="navbar-logo logo-nubificus"><img class=logo-light src=/media/logo-nubificus_hu_7be4fa6b19286a64.png alt=Nubificus>
<img class=logo-dark src=/media/logo-nubificus-dark_hu_bd80a6d6121ee2ff.png alt=Nubificus></div><div class="navbar-logo logo-nubis"><img class=logo-light src=/media/logo-nubis.png alt="Nubis PC">
<img class=logo-dark src=/media/logo-nubis-dark.png alt="Nubis PC"></div></a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/><span>Home</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Research</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/projects/><span>Projects</span></a>
<a class=dropdown-item href=/publication><span>Publications</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Solutions</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/solutions/edgelink/><span>edgeLink</span></a>
<a class=dropdown-item href=/solutions/urunc/><span>urunc</span></a>
<a class=dropdown-item href=/solutions/vaccel/><span>vAccel</span></a></div></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Feed</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/post><span>News</span></a>
<a class=dropdown-item href=/event><span>Events</span></a></div></li><li class=nav-item><a class="nav-link active" href=/blog><span>Blog</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>About</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/people><span>People</span></a>
<a class=dropdown-item href=/contact><span>Contact</span></a></div></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".navbar-nav .nav-item.dropdown");e.forEach(function(t){t.addEventListener("mouseenter",function(){e.forEach(function(e){if(e!==t){e.classList.remove("show");var n=e.querySelector(".dropdown-menu");n&&n.classList.remove("show")}}),t.classList.add("show");var n=t.querySelector(".dropdown-menu");n&&n.classList.add("show")}),t.addEventListener("mouseleave",function(){t.classList.remove("show");var e=t.querySelector(".dropdown-menu");e&&e.classList.remove("show")})})})</script></div><div class=page-body><article class=article><div class=article-container><a class=blog-back-link href=javascript:void(0) onclick=blogGoBack()><i class="fas fa-arrow-left"></i> Back to Blog</a></div><div class="article-container pt-3"><h1>AI Agents? Not on my host</h1><div class=article-metadata><span class=article-date>Feb 11, 2026</span></div></div><div class=article-container><div class=article-style><p>Over the past few years, Large Language Models (LLMs) have changed how we
interact with computers. Instead of navigating interfaces or reading
documentation, we simply describe what we want in natural language.</p><p>The next step, agentic AI, goes further: Agents do not just respond to
questions; they <em>act</em>. They write code, build projects, run tests, and
can execute programs directly on our systems. And that is where things become
interesting, and risky!</p><p>An AI agent is no longer &ldquo;just a chatbot&rdquo;. It becomes an
autonomous program executing arbitrary instructions on our system.
That raises a simple but important question:</p><blockquote><p>Would we run arbitrary, dynamically generated, unaudited code directly on our host system?</p></blockquote><p>Probably not. Yet this is exactly what we allow when we let agents execute
commands without isolation.</p><p>Let&rsquo;s be safe and isolate AI agent execution using microVMs, but with
the familiar container. workflows. In this post, we will see how we can
<code>docker build</code> and <code>docker run</code> a microVM and run the agent inside it.</p><p><a href=#isolating-ai-agents-execution-with-urunc>Jump to the instructions</a></p><h2 id=the-threat-model-treat-agents-as-untrusted-code>The Threat Model: Treat Agents as Untrusted Code</h2><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/urunc/evil_agent.png alt loading=lazy data-zoomable width=100%></div></div></figure><p>Initially, LLMs were used in a simple request-response pattern: we asked
questions and received answers. In software development, those answers often
included code snippets or shell commands that we manually copied from the
browser and pasted into our editor or terminal.</p><p>AI agents remove the middle (hu)man. Instead of suggesting commands, they
execute them. Instead of proposing code, they write it directly to disk.
However, this also removes the audit step (if anyone was actually doing it) when
copying commands or code from the browser.</p><p>As a result, agents can execute the instructions they receive directly.
They can read and write
files, execute arbitrary shell commands, create and run new applications,
modify system configuration and even interact with external services; all
without explicit human review. And that can turn ugly very quickly.</p><p>Agents do not rely only on user input. They also consume content from external
sources (repositories, documentation pages, forums, blogs, etc.). They cannot
reliably distinguish between safe and harmful instructions; they simply follow
what appears relevant in the current context. Therefore, what an agent executes
on our machine can be highly influenced from external (and sometimes) malicious
sources.</p><p>An example of the above scenario is <a href=https://x.com/theonejvo/status/2015892980851474595 target=_blank rel=noopener>a recent backdoor in a Skill for
openclaw</a>. In fact, <a href=https://snyk.io/blog/toxicskills-malicious-ai-agent-skills-clawhub/ target=_blank rel=noopener>Snyk
published research
</a>showing
that 36.82% of AI agent &ldquo;skills&rdquo; contained at least one security flaw. Even
setting aside the <a href=https://www.theregister.com/2026/02/05/openclaw_skills_marketplace_leaky_security/ target=_blank rel=noopener>security hole called
openclaw</a>,
both <a href=https://claude.com/blog/cowork-research-preview target=_blank rel=noopener>Anthropic</a> and
<a href=https://openai.com/index/hardening-atlas-against-prompt-injection/ target=_blank rel=noopener>OpenAI</a>
have publicly acknowledged that prompt injection attacks remain a real and
unresolved challenge in agent security.</p><p>From a systems perspective, once an agent can execute code, it effectively
becomes an untrusted program running on our system. This is not fundamentally
different from the cloud model, where users submit arbitrary workloads and
cloud providers must isolate and protect the infrastructure and other tenants
from potentially malicious or buggy code.</p><p>Some agents attempt to mitigate this risk by restricting file access,
limiting which commands can be executed, or running them inside some form of sandbox.
However, these controls are often
implemented at the application level. They may be bypassed due to bugs,
or misconfigurations, or simply because the agent itself is closed-source and
opaque. In practice, we are asked to trust that the agent will respect the
boundaries we configure.</p><p>But cloud providers do not rely on trust when running untrusted workloads. They
enforce strong isolation boundaries (from containers to VMs). If they do so,
we should not let AI agents run freely on our host systems either.</p><h2 id=workload-isolation>Workload isolation</h2><p>Fortunately, isolating untrusted workloads is a well-studied problem and there
are various mechanisms we can use.</p><h3 id=containers>Containers</h3><p>Containers have become the de facto packaging and deployment mechanism for
cloud applications. They restrict an application&rsquo;s access to the host
using Linux kernel features such as namespaces, cgroups,
capabilities, seccomp and others. They are lightweight, easy to use and
distribute, and therefore a good candidate for packaging and creating a restricted
execution environment for an AI agent.</p><p>However, all containers on a host share the same kernel.</p><p>While this may not pose a threat in some scenarios, it can be a serious risk
under the threat model described above. A single vulnerability in the kernel or
container runtime can potentially lead to container escapes.</p><h3 id=virtual-machines>Virtual Machines</h3><p>Virtual Machines (VMs) provide the strongest isolation boundary available on a single
host. Using hardware virtualization features, hypervisors create an environment
where a separate operating system can boot. Applications inside the VM interact
only with the guest kernel, not the host kernel. This creates a much stronger
separation.</p><p>Traditionally, VMs came with performance overhead and slow boot times.
but the introduction of microVMs in recent years has changed that.
Unlike traditional VMs, microVMs make use of specific devices and configurations
to decrease their size and overhead. This has led to the adoption of microVMs
in serverless and multi-tenant cloud environments.</p><p>On the other hand, (micro)VMs typically increase the operational complexity and
their day-to-day workflow does not match the container UX. (Micro)VMs require a
kernel and a root filesystem to boot, and tasks like attaching volumes and
setting up networking with Internet access typically involve extra steps.</p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/urunc/app_containers_vms.svg alt loading=lazy data-zoomable width=100%></div></div></figure><h3 id=sandboxed-containers>Sandboxed Containers</h3><p>Of course, people have recognized that containers are great for packaging and
deploying applications, but microVMs offer stronger isolation. As a result,
solutions that combine these technologies, as well as other software-based
sandboxing approaches, have emerged. These solutions are referred to as
sandboxed container runtimes and they aim to provide stronger isolation while
preserving the familiar container workflows and tooling.</p><p>In the case of microVMs, <a href=https://katacontainers.io/ target=_blank rel=noopener>Kata Containers</a> is a
container runtime that, instead of directly spawning a container on the host,
spins up a microVM and runs the container inside it. <a href=https://katacontainers.io/ target=_blank rel=noopener>Kata
Containers</a> provide its own Linux kernel and root
filesystem, but users can configure the runtime to use a custom kernel and/or
rootfs.</p><p>In the case of software-based sandboxes, <a href=https://gvisor.dev/ target=_blank rel=noopener>gVisor</a> is a
container runtime that spawns an application kernel alongside the container.
This application kernel mediates between the container and the host system: it
intercepts system calls from the container, implements as many as possible
itself, and forwards the rest (for example, I/O-related calls) to the host.</p><p>In this post, we will discuss the new kid on the block in sandboxed container
runtimes: <a href=https://urunc.io/ target=_blank rel=noopener>urunc</a>. Unlike the runtimes mentioned above,
<code>urunc</code> is designed to isolate only the untrusted parts of a deployment. In a
Kubernetes context, this means that instead of running the entire pod in a
sandbox, only the untrusted parts run in a sandbox inside the pod,
alongside trusted components that run as regular containers.</p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=/images/urunc/runtimes.svg alt loading=lazy data-zoomable width=100%></div></div></figure><h2 id=the-spawning-time-of-containers-with-vm-level-isolation-and-minimal-overhead>The spawning time of containers with VM level isolation and minimal overhead</h2><p>The idea behind <code>urunc</code> is that the sandbox should be as small as possible and
contain only the untrusted parts of a deployment. Therefore, <code>urunc</code> does not
require any additional components running inside or alongside the sandbox.
Subsequently, it can support both software- and VM-based sandboxes, along with a
variety of guest types, from unikernels to more general-purpose kernels like
Linux and BSD. Think of it as spawning a microVM using the container&rsquo;s rootfs ,
with the container entrypoint running as <code>init</code>.</p><p>The sandbox, that <code>urunc</code> creates, runs as a Linux container and it
integrates seamlessly with container workflows. We can create, start containers
as simply as <code>docker run</code>, get network access as a container, and
attach volumes as with any other container.</p><p>Thanks to its design <code>urunc</code> can achieve comparable spawn times to normal
containers and introduces minimal overhead. Check out the past <a href="https://github.com/urunc-dev/urunc?tab=readme-ov-file#publications-and-talks" target=_blank rel=noopener>talks and
publications</a>
for comparisons with other sandboxed container runtimes.</p><p>In the context of agents, <code>urunc</code> can be used in two ways:
a) as a sandbox for the entire agent. or b) as a sandbox for a
specific application executions triggered by the agent.</p><p>In this post we will describe the first approach and run an agent inside a
<code>urunc</code> container. Stay tuned for the second approach.</p><p>Let&rsquo;s try it out.</p><h2 id=isolating-ai-agents-execution-with-urunc>Isolating AI agents execution with urunc</h2><p>To showcase how we can use <code>urunc</code> to isolate an AI agent, we will use as an
example <a href=https://opencode.ai/ target=_blank rel=noopener>opencode</a>, but the instructions below can be
adapted for any agent. Overall, we just need to set up the <code>urunc</code> environment,
build the container images, and run them.</p><h3 id=step-0-setting-up-the-environment>Step 0: Setting up the environment</h3><p>Assuming we already have a working docker / containerd installation, we can install
<code>urunc</code> and its shim as easily as fetching the binaries from the latest release:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># or for the latest release</span>
</span></span><span class=line><span class=cl><span class=nv>URUNC_VERSION</span><span class=o>=</span><span class=k>$(</span>curl -L -s -o /dev/null -w <span class=s1>&#39;%{url_effective}&#39;</span> <span class=s2>&#34;https://github.com/urunc-dev/urunc/releases/latest&#34;</span> <span class=p>|</span> grep -oP <span class=s2>&#34;v\d+\.\d+\.\d+&#34;</span> <span class=p>|</span> sed <span class=s1>&#39;s/v//&#39;</span><span class=k>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nv>URUNC_BINARY_FILENAME</span><span class=o>=</span><span class=s2>&#34;urunc_static_</span><span class=k>$(</span>dpkg --print-architecture<span class=k>)</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>wget -q https://github.com/urunc-dev/urunc/releases/download/v<span class=nv>$URUNC_VERSION</span>/<span class=nv>$URUNC_BINARY_FILENAME</span>
</span></span><span class=line><span class=cl>chmod +x <span class=nv>$URUNC_BINARY_FILENAME</span>
</span></span><span class=line><span class=cl>sudo mv <span class=nv>$URUNC_BINARY_FILENAME</span> /usr/local/bin/urunc
</span></span></code></pre></div><p>And for <code>containerd-shim-urunc-v2</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nv>CONTAINERD_BINARY_FILENAME</span><span class=o>=</span><span class=s2>&#34;containerd-shim-urunc-v2_static_</span><span class=k>$(</span>dpkg --print-architecture<span class=k>)</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>wget -q https://github.com/urunc-dev/urunc/releases/download/v<span class=nv>$URUNC_VERSION</span>/<span class=nv>$CONTAINERD_BINARY_FILENAME</span>
</span></span><span class=line><span class=cl>chmod +x <span class=nv>$CONTAINERD_BINARY_FILENAME</span>
</span></span><span class=line><span class=cl>sudo mv <span class=nv>$CONTAINERD_BINARY_FILENAME</span> /usr/local/bin/containerd-shim-urunc-v2
</span></span></code></pre></div><p>For more detailed installation instructions see the <a href=https://urunc.io/installation/ target=_blank rel=noopener>installation guide of
<code>urunc</code></a>. The guide contains also instructions
to download the <a href=https://urunc.io/installation/#option-3-install-from-latest-artifacts-tip-of-the-main-branch target=_blank rel=noopener>latest build of the <code>main</code>
branch</a>.</p><p>As previously mentioned, <code>urunc</code> supports <a href=https://urunc.io/#current-support-of-unikernels-and-vmsandbox-monitors target=_blank rel=noopener>a variety of software- and VM-based
sandboxes</a>.
For simplicity, this post focuses on a VM-based sandbox using Linux and QEMU.
We will also use <a href=https://virtio-fs.gitlab.io/ target=_blank rel=noopener>virtiofs</a> to share data
between the host and the VM. As a result, we will need to install QEMU and
virtiofsd.</p><p>We can install them either via our distribution’s package manager or by
downloading pre-built artifacts from the
<a href=https://github.com/urunc-dev/monitors-build target=_blank rel=noopener>monitors-build</a> repository. For
more details, refer to the <a href=https://urunc.io/installation/#step-2-install-all-supported-monitors target=_blank rel=noopener>respective section of the installation
guide</a></p><h3 id=step-1-creating-the-container-image>Step 1: Creating the Container image</h3><p>In this step, we define the environment in which the agent will run. We will
do that with a <code>Containerfile</code>. For example, we can create a Go dev environment
based on the container image of opencode with the following <code>Containerfile</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>FROM ghcr.io/anomalyco/opencode
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>RUN apk add git go
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>WORKDIR /app
</span></span></code></pre></div><p>We can build the container image with:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>docker build -f Containerfile -t go-dev-opencode:normal .
</span></span></code></pre></div><p>To make the container image above compatible with <code>urunc</code>, we need to include a
Linux kernel for the VM and set a few <code>urunc</code> -specific OCI annotations. In
addition, to configure the execution environment inside the VM (UID/GID, working
directory, etc.), we will use a custom <code>init</code> called
<a href=https://github.com/nubificus/urunit target=_blank rel=noopener>urunit</a>.
This is optional but recommended. As mentioned earlier, <code>urunc</code> does not require
any additional components inside the sandbox; however, if our workload expects
to run in a specific setup (UID/GID, working directory, etc.), someone needs to
set this up, and that is what <code>urunit</code> provides.</p><p>While this may sound like a lot of work, we can handle it without installing
extra tooling by using <a href=https://github.com/nubificus/bunny target=_blank rel=noopener>bunny</a>.
It is a
buildkit frontend that works directly with
<code>docker build</code>.
Bunny can parse two types of files: a) the standard <code>Containerfile</code>
and b) a YAML-based file specific to <code>bunny</code> called
<a href="https://github.com/nubificus/bunny?tab=readme-ov-file#the-bunnyfile" target=_blank rel=noopener>bunnyfile</a>.
Using either type, we can add the kernel, <code>urunit</code> and set the required annotations.</p><h4 id=bunny-with-containerfile>Bunny with Containerfile</h4><p>For simplicity we have created a <code>bunny</code> variant that can get an existing
<code>Containerfile</code> and build an image compatible with <code>urunc</code>.
To use it we simply need to prepend the following line in <code>Containerfile</code>:
<code>#syntax=harbor.nbfc.io/nubificus/bunny:containerfile</code>.</p><p>The new <code>Containerfile</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>#syntax=harbor.nbfc.io/nubificus/bunny:containerfile
</span></span><span class=line><span class=cl>FROM ghcr.io/anomalyco/opencode
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>RUN apk add git go
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>WORKDIR /app
</span></span></code></pre></div><p>We can build it exactly as before with:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>docker build -f Containerfile -t go-dev-opencode:containerfile .
</span></span></code></pre></div><h4 id=bunny-with-bunnyfile>Bunny with bunnyfile</h4><p>For users who want more control over the kernel and the <code>init</code>, we recommend
using the <code>bunnyfile</code> format. This approach involves two build steps, one for
the base container and one using <code>bunny</code> to make the image <code>urunc</code> compatible.
The first steps uses the standard <code>Containerfile</code>, while the second uses a
<code>bunnyfile</code>.</p><p>We assume that the first step has been done using the <code>Containerfile</code>
above. For the second build, we can use the following <code>bunnyfile</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>#syntax=harbor.nbfc.io/nubificus/bunny:latest
</span></span><span class=line><span class=cl>version: v0.1
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>platforms:
</span></span><span class=line><span class=cl>  framework: linux
</span></span><span class=line><span class=cl>  monitor: qemu
</span></span><span class=line><span class=cl>  architecture: x86
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>rootfs:
</span></span><span class=line><span class=cl>  from: go-dev-opencode:normal
</span></span><span class=line><span class=cl>  type: raw
</span></span><span class=line><span class=cl>  include:
</span></span><span class=line><span class=cl>  - from: harbor.nbfc.io/nubificus/urunit:latest
</span></span><span class=line><span class=cl>    source: /urunit
</span></span><span class=line><span class=cl>    destination: /urunit
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>kernel:
</span></span><span class=line><span class=cl>  from: local
</span></span><span class=line><span class=cl>  path: kernel
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>entrypoint: [&#34;/urunit&#34;]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>cmd: [&#34;opencode&#34;]
</span></span></code></pre></div><p>In the above <code>bunnyfile</code>, we specify:</p><ul><li>a <code>urunc</code> container image targeting Linux over Qemu in x86 architecture.</li><li>with a rootfs based on the container image we created before appending the
<code>urunit</code> binary from the latest <code>urunit</code> container image.</li><li>with a kernel which resides locally under the name <code>kernel</code>.</li><li>with <code>/urunit</code> and <code>opencode</code> as entrypoint and cmd respectively.</li></ul><p>We can build it simply with docker:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>docker build -f bunnyfile -t go-dev-opencode:bunnyfile .
</span></span></code></pre></div><h3 id=step-2-running-the-container>Step 2: Running the container</h3><p>To run the containers we built in the previous steps, we can simply:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>docker run -m 1024M --rm -it go-dev-opencode:normal
</span></span></code></pre></div><p>and opencode TUI will open.</p><p>Running a <code>urunc</code> container follows the same workflow; simply add the <code>--runtime io.containerd.urunc.v2</code> cli option when starting a container from either a
bunnyfile-based or Containerfile-based image.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>docker run -m 1024M --rm --runtime io.containerd.urunc.v2 -it go-dev-opencode:bunnyfile
</span></span></code></pre></div><p>That&rsquo;s it! We have created a VM that can run arbitrary workloads using the root
filesystem we defined when building the container image.</p><blockquote><p>Hint: Use <a href=https://opencode.ai/docs/web/ target=_blank rel=noopener>the web interface of opencode</a> and expose
its port to the host with <code>-p &lt;PORT>:&lt;PORT></code> to avoid issues with the console.</p></blockquote><h3 id=step-3-sharing-data-with-the-host>Step 3: Sharing data with the host</h3><p>In a normal container we can share a host directory with the container with:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>docker run -m 1024M --rm -v ${PWD}/mydir:/mydir -it go-dev-opencode:bunnyfile
</span></span></code></pre></div><p>In a <code>urunc</code> container nothing changes, except of specifying the runtime:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>docker run -m 1024M --rm --runtime io.containerd.urunc.v2 -v ${PWD}/mydir:/mydir -it go-dev-opencode:bunnyfile
</span></span></code></pre></div><p>and now we have <code>mydir</code> in the VM&rsquo;s rootfs and everything is shared, but use
with caution.</p><h2 id=what-urunc-protects-us-from>What <code>urunc</code> protects us from</h2><p>Like other sandboxed runtimes, <code>urunc</code> isolates untrusted code from the host
system. As a result, <code>urunc</code> will protect our host&rsquo;s filesystem, kernel and
other processes, making a container escape to the host significantly more difficult.</p><p>However, isolation is not magic. If we explicitly share data or resources with
a <code>urunc</code> container, that data is no longer protected. Untrusted code can still
delete files, leak data, or misuse whatever access we grant it.</p><p><code>urunc</code> provides a strong boundary, but the security policy is still controlled
by us (the users).</p><h2 id=hands-on-example>Hands-on example</h2><p>As an example, we captured an end-to-end execution of opencode running inside
a QEMU VM with <code>urunc</code>. In this demo, we use the container images built earlier
and instruct opencode to create a Go server that responds to HTTP requests with
random messages.We also share a directory with the <code>urunc</code>container; within
that directory, opencode initializes a Git repository and commits the changes.</p><script src=https://asciinema.org/a/787412.js id=asciicast-787412 async></script><h2 id=final-thoughts>Final Thoughts</h2><p>AI agents blur the line between &ldquo;tool&rdquo; and &ldquo;program&rdquo;. Once they execute
code, they should be treated as untrusted workloads.</p><p>Containers made deployment easy, but for agentic execution, a shared kernel
might not be enough. Virtual machines provide the right boundary and with
sandboxed container runtimes like <code>urunc</code>, they can be managed as easily as
containers.</p><p>If agents are going to run code on our system, they should not run on our host.</p><p>Use them with caution.</p></div><div class=article-tags><a class="badge badge-light" href=/tag/unikernels/>Unikernels</a>
<a class="badge badge-light" href=/tag/container-runtimes/>Container Runtimes</a>
<a class="badge badge-light" href=/tag/containers/>Containers</a>
<a class="badge badge-light" href=/tag/urunc/>Urunc</a>
<a class="badge badge-light" href=/tag/k8s/>K8s</a>
<a class="badge badge-light" href=/tag/kubernetes/>Kubernetes</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=%2Fblog%2Furunc_agent%2F&amp;text=AI+Agents%3F+Not+on+my+host" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=%2Fblog%2Furunc_agent%2F&amp;t=AI+Agents%3F+Not+on+my+host" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=AI%20Agents%3F%20Not%20on%20my%20host&amp;body=%2Fblog%2Furunc_agent%2F" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=%2Fblog%2Furunc_agent%2F&amp;title=AI+Agents%3F+Not+on+my+host" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=AI+Agents%3F+Not+on+my+host%20%2Fblog%2Furunc_agent%2F" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=%2Fblog%2Furunc_agent%2F&amp;title=AI+Agents%3F+Not+on+my+host" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div></div></article><script>function blogGoBack(){document.referrer&&document.referrer.indexOf("/blog")!==-1?history.back():window.location.href="/blog/"}</script></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2026 Nubificus LTD. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target=_blank rel=noopener>Hugo Blox Builder</a> — the free, <a href=https://github.com/HugoBlox/hugo-blox-builder target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.6782e3f6a4f06ea4766df038c45ecf40.js></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.b95a7e109243f29d04930ae8cb49a756.js type=module></script><script src=/en/js/wowchemy.min.ed487406ecbb80985462ba7a97b6f2d2.js></script><script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.9c0e895144aef5a693008b5c5d450147.js type=module></script></body></html>