[{"authors":null,"categories":null,"content":"About Charalampos (Babis) Mainas is a Systems Researcher and OS \u0026amp; Hypervisor Engineer with a PhD candidate status at the National Technical University of Athens (HSCNL, NTUA). He focuses on low-level systems programming, including Linux kernel development, hypervisors (KVM, Xen), and unikernel runtime ecosystems.\nHe leads the design and development of the cloud-native container runtime urunc and the building and packaging tool bunny, enabling seamless build-and-run workflows for unikernels and single-application kernels. His interests span kernel mechanisms, OS-adjacent tooling, and integration of cutting-edge systems technologies, contributing deep expertise in performance-sensitive and isolation-oriented environments.\nResearch Interests Babis is deeply interested in low-level aspects of computing systems, such as kernels, hypervisors, and software that tightly interacts with them. He has significant hands-on experience with the Linux kernel and the two most widely used hypervisors – KVM and Xen.\nA significant portion of his work has been dedicated to unikernels, including porting applications, libraries, and language runtimes with an emphasis on enhancing their compatibility with existing technologies. Through his work on bunny and urunc, he enables users to simply docker build and docker run unikernels.\nPersonal Website For more detailed and up-to-date information, visit: https://cmainas.gitlab.io/\n","date":1756920600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1756920600,"objectID":"6f68f447524ce812a1bb067581c71730","permalink":"/author/charalampos-mainas/","publishdate":"2025-09-03T00:00:00Z","relpermalink":"/author/charalampos-mainas/","section":"authors","summary":"About Charalampos (Babis) Mainas is a Systems Researcher and OS \u0026 Hypervisor Engineer with a PhD candidate status at the National Technical University of Athens (HSCNL, NTUA). He focuses on low-level systems programming, including Linux kernel development, hypervisors (KVM, Xen), and unikernel runtime ecosystems.\n","tags":null,"title":"Charalampos Mainas","type":"authors"},{"authors":null,"categories":null,"content":"About I am a Researcher in Computer Systems and I am currently working on the various levels of the systems software stack to attack issues related to performance, scalability, power-efficiency and security in modern systems.\nSince 2015 I have been affiliated with UK \u0026amp; EU firms, building \u0026amp; architecting solutions for efficient execution of workloads in the Cloud and at the Edge. I have been involved in many parts of the systems software stack, including device drivers, memory management, network/block layers etc.\nPreviously, I was a postdoctoral researcher at CSLab, NTUA (2014), where I got my Dipl. Eng. (2006) and my PhD (2013).\n","date":1756209300,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1756209300,"objectID":"fefb8a9c567832dafe1af915cfe4586b","permalink":"/author/anastassios-nanos/","publishdate":"2025-08-26T00:00:00Z","relpermalink":"/author/anastassios-nanos/","section":"authors","summary":"About I am a Researcher in Computer Systems and I am currently working on the various levels of the systems software stack to attack issues related to performance, scalability, power-efficiency and security in modern systems.\n","tags":null,"title":"Anastassios Nanos","type":"authors"},{"authors":null,"categories":null,"content":"About Anastasia Mallikopoulou is a Systems Engineer with expertise in observability stacks, runtime benchmarking, distributed instrumentation, and applied machine learning. She brings experience ranging from AI-driven analytics (medical imaging, multimodal ML) to performance profiling for containerised and sandboxed environments.\nShe reinforces system reliability, monitoring, data-quality assessment, and anomaly detection workflows with a focus on practical observability solutions for complex distributed systems.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5e620ba7dbb6105447a0838d41ed4fc0","permalink":"/author/anastasia-mallikopoulou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/anastasia-mallikopoulou/","section":"authors","summary":"About Anastasia Mallikopoulou is a Systems Engineer with expertise in observability stacks, runtime benchmarking, distributed instrumentation, and applied machine learning. She brings experience ranging from AI-driven analytics (medical imaging, multimodal ML) to performance profiling for containerised and sandboxed environments.\n","tags":null,"title":"Anastasia Mallikopoulou","type":"authors"},{"authors":null,"categories":null,"content":"About Konstantinos Papazafeiropoulos is a Systems Researcher and Virtualization \u0026amp; Systems Engineer with a PhD candidate status at the National Technical University of Athens (CSlab, NTUA). He brings strong expertise in operating-system internals, virtualization (QEMU/KVM, unikernels), resource elasticity, and secure containerisation techniques.\nHis contributions include systems research on memory management, isolation primitives, and high-performance serverless/edge acceleration frameworks. He strengthens low-level runtime design, secure execution environments, and OS-level optimisation.\nKonstantinos leads the design and development of the vAccel framework, which enables hardware acceleration for serverless and edge computing environments.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"19af861cc5661261ab68a6e88be5a2e4","permalink":"/author/konstantinos-papazafeiropoulos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/konstantinos-papazafeiropoulos/","section":"authors","summary":"About Konstantinos Papazafeiropoulos is a Systems Researcher and Virtualization \u0026 Systems Engineer with a PhD candidate status at the National Technical University of Athens (CSlab, NTUA). He brings strong expertise in operating-system internals, virtualization (QEMU/KVM, unikernels), resource elasticity, and secure containerisation techniques.\n","tags":null,"title":"Konstantinos Papazafeiropoulos","type":"authors"},{"authors":null,"categories":null,"content":"About Maria Gkoutha is a Systems \u0026amp; Software Engineer who contributes to systems-level development, testbed engineering, and experimentation with heterogeneous edge/cloud architectures. Her work spans resource-management frameworks, hardware-accelerated multimedia pipelines (vAccel), and software prototyping across diverse platforms.\nShe supports integration, testing, and performance evaluation tasks across the full stack, with a focus on practical engineering solutions for cloud and edge computing environments.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5b145166e8c1be858440d4c1759b79b6","permalink":"/author/maria-gkoutha/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/maria-gkoutha/","section":"authors","summary":"About Maria Gkoutha is a Systems \u0026 Software Engineer who contributes to systems-level development, testbed engineering, and experimentation with heterogeneous edge/cloud architectures. Her work spans resource-management frameworks, hardware-accelerated multimedia pipelines (vAccel), and software prototyping across diverse platforms.\n","tags":null,"title":"Maria Gkoutha","type":"authors"},{"authors":null,"categories":null,"content":"Maria Rafaela Gkeka is a Systems Researcher and HW acceleration Engineer specialising in embedded systems, FPGA-based acceleration, hardware-software co-design and real-time sensor pipelines.\nHer research background includes SLAM architectures, adversarial robustness and performance-optimised compute primitives. She contributes strong capabilities in low-level driver development, heterogeneous interface integration and high-fidelity data acquisition. Currently pursuing her PhD.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fc25d1310dfe37e71394eeca8ed39406","permalink":"/author/maria-rafaela-gkeka/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/maria-rafaela-gkeka/","section":"authors","summary":"Maria Rafaela Gkeka is a Systems Researcher and HW acceleration Engineer specialising in embedded systems, FPGA-based acceleration, hardware-software co-design and real-time sensor pipelines.\n","tags":null,"title":"Maria Rafaela Gkeka","type":"authors"},{"authors":null,"categories":null,"content":"About Panagiotis Mavrikos is a Software Engineer specializing in DevOps, with expertise in containerization, orchestration, and modern deployment practices. He contributes to DevOps automation, CI/CD pipelines, and infrastructure provisioning with a focus on Kubernetes, Docker, and serverless technologies.\nHis work spans cluster maintenance, deployment automation, and supporting stable environments for software testing and runtime distribution. He is passionate about innovative software solutions and dedicated to reinforcing operational reliability and scalable deployment processes.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"89059452415eeddffc1112bf2d8935f3","permalink":"/author/panagiotis-mavrikos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/panagiotis-mavrikos/","section":"authors","summary":"About Panagiotis Mavrikos is a Software Engineer specializing in DevOps, with expertise in containerization, orchestration, and modern deployment practices. He contributes to DevOps automation, CI/CD pipelines, and infrastructure provisioning with a focus on Kubernetes, Docker, and serverless technologies.\n","tags":null,"title":"Panagiotis Mavrikos","type":"authors"},{"authors":[""],"categories":null,"content":" edgeLink develops integration layers that unify workload and device orchestration across multiple tiers of infrastructure — from cloud clusters to embedded IoT endpoints. Zero-Trust Cloud-Native Edge-First Hardware-Rooted Identity Architecture Overview edgeLink Continuum Cloud Cloud Control Plane GitOps \u0026amp; policy management\nK8s Orchestration Workload scheduling \u0026amp; scaling\nObservability Monitoring \u0026amp; fleet analytics\nOTA \u0026amp;\nTelemetry Edge Edge Gateway Local orchestration \u0026amp; caching\nZero-Trust Broker DICE/EAT attestation\nvAccel Runtime HW acceleration \u0026amp; offload\nSecure\nChannel Device MCU / SoC Constrained endpoints\nEmbedded Linux Lightweight runtimes\nSensors \u0026amp; Actuators Field data collection\nSmart Devices AI-capable endpoints\nOur work ensures alignment with Kubernetes and GitOps-based operational models, while extending orchestration semantics to devices that cannot run a full container orchestration stack. By leveraging zero-trust principles and cloud-native orchestration, edgeLink enables scalable, secure management of heterogeneous IoT devices at the edge — from provisioning to runtime monitoring and firmware updates. Key Capabilities Distributed Offloading Workload routing and compute offloading via vAccel, enabling hardware acceleration across distributed infrastructure.\nDeclarative Configuration Policy-driven resource management with GitOps-aligned declarative configuration for consistent deployments.\nHardware-Rooted Identity Secure device onboarding using DICE/EAT attestation, establishing hardware-rooted trust from first boot.\nSecure OTA Updates Continuous and cryptographically verified over-the-air updates for both firmware and application workloads.\nReproducible Supply Chains End-to-end reproducible software supply chains for embedded and edge systems, ensuring build integrity.\nMulti-Tenant Management Cloud-aligned device repurposing mechanisms with multi-tenant isolation for shared infrastructure.\nUse Cases Industrial IoT Manage and orchestrate thousands of industrial sensors and controllers across factory floors with zero-touch provisioning and continuous monitoring.\nSmart Infrastructure Deploy and maintain heterogeneous edge devices across smart buildings, campuses, and urban infrastructure with cloud-native workflows.\nTelco Edge Extend Kubernetes orchestration to far-edge telecom nodes, enabling consistent workload management from core to RAN.\nPrecision Agriculture Securely onboard and manage fleets of agricultural sensors and actuators with firmware lifecycle management across remote locations.\nExplore edgeLink Discover the full technical documentation, architecture guides, and getting started resources.\nVisit edgeLink Website ","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"3a147df4ca9403fb7be055a0f5591b87","permalink":"/solutions/edgelink/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/solutions/edgelink/","section":"solutions","summary":"Zero-touch, zero-trust device onboarding and lifecycle management for IoT at the edge","tags":[],"title":"edgeLink","type":"solutions"},{"authors":[""],"categories":null,"content":" urunc bridges the gap between traditional unikernels and containerized environments, bringing ultra-lightweight, immutable execution to cloud-native workflows. OCI-Compatible CRI Support Lightweight VM-Isolated Architecture Overview urunc Stack App OCI Image Unikernel packaged as container\nbunny CLI Build \u0026amp; package tooling\nContainer Registry Standard OCI distribution\nOCI\nSpec Runtime urunc Runtime CRI-compatible unikernel launcher\ncontainerd Shim Container runtime integration\nKubernetes CRI K8s pod scheduling\nVMM\nAPI VMM Firecracker Lightweight microVM\nQEMU Full-featured VMM\nSolo5 (hvt/spt) Minimal sandbox monitor\nRather than spawning simple processes, urunc uses Virtual Machine Monitors or sandbox monitors to launch unikernels, maintaining strong isolation with minimal overhead. Its un-opinionated design enables straightforward integration of new unikernel frameworks without porting overhead, while developers and administrators use familiar container workflows to package, deliver, deploy, and manage unikernels. Key Capabilities OCI Compatibility Unikernels are packaged inside standard OCI-compatible images, enabling use of existing container registries and tooling.\nCRI Integration Full compatibility with Kubernetes Container Runtime Interface for seamless pod scheduling and orchestration.\nVM-Level Isolation Strong security boundaries via lightweight VM monitors, minimizing the attack surface with single-application kernels.\nMinimal Overhead Ultra-lightweight runtimes with deterministic performance, suitable for real-time and resource-constrained edge environments.\nMulti-Framework Support Un-opinionated design supports Rumprun, Unikraft, MirageOS, Mewz, and Linux guests without porting overhead.\nFast Instantiation Millisecond-level boot times enable serverless and event-driven workloads with near-instant cold starts.\nSupported Platforms Unikernel Monitor Architecture Storage Rumprun Solo5-hvt, Solo5-spt x86, aarch64 Block / Devmapper Unikraft QEMU, Firecracker x86 Initrd, 9pfs MirageOS QEMU, Solo5-hvt, Solo5-spt x86, aarch64 Block / Devmapper Mewz QEMU x86 In-memory Linux QEMU, Firecracker x86, aarch64 Initrd, Block, 9pfs, Virtiofs Use Cases Microservices Lightweight single-application OS reduces overhead in microservice architectures, offering stronger isolation than traditional containers.\nServerless / FaaS Millisecond boot times and minimal footprint make unikernels ideal for event-driven, short-lived function execution.\nEdge Computing Resource-constrained environments benefit from unikernel efficiency, enabling cloud-native execution on devices with limited memory and compute.\nSensitive Environments VM-based isolation with a minimized attack surface provides strong security guarantees for workloads handling sensitive data.\nExplore urunc Get started with tutorials, Kubernetes integration guides, and performance benchmarks.\nVisit urunc Website ","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"0198ed138c28a71495455d969a4bb3a9","permalink":"/solutions/urunc/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/solutions/urunc/","section":"solutions","summary":"A lightweight container runtime that brings unikernels into cloud-native workflows","tags":[],"title":"urunc","type":"solutions"},{"authors":[""],"categories":null,"content":" vAccel is a lightweight, modular framework that exposes hardware acceleration functionality to workloads in virtualized or otherwise isolated environments — write once, accelerate anywhere. Hardware-Agnostic Plugin Architecture Lightweight Multi-Language Architecture Overview vAccel Framework App Application Code User workload (C, Python, Go, Rust)\nvAccel API Unified operations interface\nML / Compute Ops TF, Torch, image, BLAS, exec\nUnified\nAPI Plugin Accel Plugins TensorFlow, Torch, TVM\nTransport Plugins RPC, VirtIO for remote exec\nUtility Plugins Exec, MBench, NoOp\nDriver\nLayer HW GPUs NVIDIA, AMD, Intel\nTPUs / NPUs Neural accelerators\nFPGAs Custom fabric acceleration\nRemote Backends Network-attached accelerators\nvAccel provides a unified abstraction layer that allows applications to offload compute-intensive operations — such as image processing, machine-learning inference, and cryptographic functions — to a wide range of accelerators without requiring platform-specific code. Built to integrate seamlessly with existing runtimes, vAccel supports local and remote acceleration backends, offering a consistent API across GPUs, FPGAs, NPUs, and custom devices. Its design emphasises portability, minimal overhead, and interoperability with containerised and unikernel-based workloads. Key Capabilities Hardware Agnostic Write accelerated code once and deploy on any backend — GPUs, TPUs, NPUs, or FPGAs — without vendor lock-in or driver-level integration.\nPlugin Architecture Modular plugin system supporting acceleration (TensorFlow, PyTorch, TVM), transport (RPC, VirtIO), and utility backends.\nRemote Acceleration Transparent remote execution via RPC and VirtIO transport plugins, enabling acceleration from VMs and constrained environments.\nMulti-Language Bindings Native bindings for C, Python, Go, and Rust, with a unified operations API across all supported languages.\nMinimal Overhead Lightweight framework with negligible performance penalty, designed for highly constrained serverless and edge environments.\nRuntime Integration Seamless interoperability with containers, unikernels (via urunc), and Kubernetes for cloud-native accelerated workloads.\nPlugin Ecosystem Category Plugin Operations Acceleration TensorFlow Image classification, object detection, model inference Acceleration PyTorch Tensor ops, model inference, jit forward Acceleration TVM Compiled model execution, cross-platform inference Transport RPC Network-based remote acceleration dispatch Transport VirtIO VM-to-host hardware passthrough Utility Exec, MBench, NoOp Generic exec, benchmarking, testing Use Cases ML Inference at the Edge Run TensorFlow and PyTorch models on edge devices with transparent offloading to available accelerators, no code changes needed.\nServerless Acceleration Enable hardware acceleration in serverless functions and lightweight VMs without requiring direct driver access or GPU passthrough.\nImage \u0026amp; Video Processing Offload compute-intensive image classification, object detection, and video analytics to heterogeneous accelerators at scale.\nKubernetes GPU Sharing Deploy accelerated workloads as standard K8s pods with vAccel managing hardware multiplexing across tenants.\nExplore vAccel Dive into the documentation, API reference, tutorials, and plugin development guides.\nVisit vAccel Docs ","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"490ff98dd23461f42c90749741e93cbe","permalink":"/solutions/vaccel/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/solutions/vaccel/","section":"solutions","summary":"Seamless hardware acceleration for cloud and edge workloads via a portable, hardware-agnostic API","tags":[],"title":"vAccel","type":"solutions"},{"authors":[""],"categories":null,"content":"NUBIS contribution: NUBIS develops the secure and efficient interoperable execution of workloads across the IoT-Edge-Cloud continuum.\n","date":1798761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1798761600,"objectID":"baba9c92a6b1c210df2d61900fde203d","permalink":"/projects/empyrean/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/projects/empyrean/","section":"projects","summary":"Hyper-distributed computing with federated IoT and edge resources using AI-driven multi-agent decision-making","tags":["ongoing"],"title":"EMPYREAN","type":"projects"},{"authors":[""],"categories":null,"content":"NUBIS contribution: As WP6 leader, NUBIS orchestrates the integration of all TexTailes digital tools into a coherent portal. Additionally, NUBIS leads the API integration effort with ECHOES and ECCCH.\n","date":1798761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1798761600,"objectID":"3cb582851c439c50ee371e97212f8765","permalink":"/projects/textailes/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/projects/textailes/","section":"projects","summary":"Standardized non-destructive AI-driven tools for textile digitization within ECCCH","tags":["ongoing"],"title":"TEXTaiLES","type":"projects"},{"authors":[""],"categories":null,"content":"NUBIS contribution: NUBIS contributes expertise in cloud-native computing and edge infrastructure, supporting the deployment of advanced computing continuum solutions across heterogeneous 5G testbeds.\n","date":1796083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1796083200,"objectID":"080d81341d282488d6af37dd7ebb36b0","permalink":"/projects/ambitious/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/projects/ambitious/","section":"projects","summary":"Advanced computing continuum solutions boosting digitalization across European regions through AI, 5G, and IoT","tags":["ongoing"],"title":"AMBITIOUS","type":"projects"},{"authors":null,"categories":null,"content":"Over the past few years, Large Language Models (LLMs) have changed how we interact with computers. Instead of navigating interfaces or reading documentation, we simply describe what we want in natural language.\nThe next step, agentic AI, goes further: Agents do not just respond to questions; they act. They write code, build projects, run tests, and can execute programs directly on our systems. And that is where things become interesting, and risky!\nAn AI agent is no longer “just a chatbot”. It becomes an autonomous program executing arbitrary instructions on our system. That raises a simple but important question:\nWould we run arbitrary, dynamically generated, unaudited code directly on our host system?\nProbably not. Yet this is exactly what we allow when we let agents execute commands without isolation.\nLet’s be safe and isolate AI agent execution using microVMs, but with the familiar container. workflows. In this post, we will see how we can docker build and docker run a microVM and run the agent inside it.\nJump to the instructions\nThe Threat Model: Treat Agents as Untrusted Code Initially, LLMs were used in a simple request-response pattern: we asked questions and received answers. In software development, those answers often included code snippets or shell commands that we manually copied from the browser and pasted into our editor or terminal.\nAI agents remove the middle (hu)man. Instead of suggesting commands, they execute them. Instead of proposing code, they write it directly to disk. However, this also removes the audit step (if anyone was actually doing it) when copying commands or code from the browser.\nAs a result, agents can execute the instructions they receive directly. They can read and write files, execute arbitrary shell commands, create and run new applications, modify system configuration and even interact with external services; all without explicit human review. And that can turn ugly very quickly.\nAgents do not rely only on user input. They also consume content from external sources (repositories, documentation pages, forums, blogs, etc.). They cannot reliably distinguish between safe and harmful instructions; they simply follow what appears relevant in the current context. Therefore, what an agent executes on our machine can be highly influenced from external (and sometimes) malicious sources.\nAn example of the above scenario is a recent backdoor in a Skill for openclaw. In fact, Snyk published research showing that 36.82% of AI agent “skills” contained at least one security flaw. Even setting aside the security hole called openclaw, both Anthropic and OpenAI have publicly acknowledged that prompt injection attacks remain a real and unresolved challenge in agent security.\nFrom a systems perspective, once an agent can execute code, it effectively becomes an untrusted program running on our system. This is not fundamentally different from the cloud model, where users submit arbitrary workloads and cloud providers must isolate and protect the infrastructure and other tenants from potentially malicious or buggy code.\nSome agents attempt to mitigate this risk by restricting file access, limiting which commands can be executed, or running them inside some form of sandbox. However, these controls are often implemented at the application level. They may be bypassed due to bugs, or misconfigurations, or simply because the agent itself is closed-source and opaque. In practice, we are asked to trust that the agent will respect the boundaries we configure.\nBut cloud providers do not rely on trust when running untrusted workloads. They enforce strong isolation boundaries (from containers to VMs). If they do so, we should not let AI agents run freely on our host systems either.\nWorkload isolation Fortunately, isolating untrusted workloads is a well-studied problem and there are various mechanisms we can use.\nContainers Containers have become the de facto packaging and deployment mechanism for cloud applications. They restrict an application’s access to the host using Linux kernel features such as namespaces, cgroups, capabilities, seccomp and others. They are lightweight, easy to use and distribute, and therefore a good candidate for packaging and creating a restricted execution environment for an AI agent.\nHowever, all containers on a host share the same kernel.\nWhile this may not pose a threat in some scenarios, it can be a serious risk under the threat model described above. A single vulnerability in the kernel or container runtime can potentially lead to container escapes.\nVirtual Machines Virtual Machines (VMs) provide the strongest isolation boundary available on a single host. Using hardware virtualization features, hypervisors create an environment where a separate operating system can boot. Applications inside the VM interact only with the guest kernel, not the host kernel. This creates a much stronger separation.\nTraditionally, VMs came with performance overhead and slow boot times. but …","date":1770769332,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1770769332,"objectID":"4a9f1d5cc5b9f1724e1c65b952ed16b5","permalink":"/blog/urunc_agent/","publishdate":"2026-02-11T00:22:12Z","relpermalink":"/blog/urunc_agent/","section":"blog","summary":"Over the past few years, Large Language Models (LLMs) have changed how we interact with computers. Instead of navigating interfaces or reading documentation, we simply describe what we want in natural language.\n","tags":["unikernels","container runtimes","containers","urunc","k8s","kubernetes"],"title":"AI Agents? Not on my host","type":"blog"},{"authors":[""],"categories":null,"content":"NUBIS contribution: As WP3 leader, NUBIS orchestrated the development of efficient mechanisms for optimizing application execution, resource utilization and energy consumption across the whole continuum.\n","date":1767225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767225600,"objectID":"7a490c6c624eb1e35b3aa01ef53a2861","permalink":"/projects/mlsysops/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/projects/mlsysops/","section":"projects","summary":"AI-driven framework for autonomic system management and application deployment across the cloud-edge continuum","tags":["completed"],"title":"MLSysOps","type":"projects"},{"authors":[""],"categories":null,"content":"NUBIS contribution: Through the DROP proposal, NUBIS develops a distributed resource offloading mechanism leveraging the vAccel framework for ESP32-based IoT devices. DROP enables real-time processing capabilities for resource-constrained devices while reducing latency and improving system efficiency, fostering innovation in logistics, smart home, and industrial automation.\n","date":1767225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767225600,"objectID":"0f3bc7de6ef95a4c1dac2053740b8d9d","permalink":"/projects/nephele-drop/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/projects/nephele-drop/","section":"projects","summary":"Distributed resource offloading leveraging vAccel for ESP32-based IoT devices, enabling real-time processing on resource-constrained hardware","tags":["completed"],"title":"NEPHELE: DROP","type":"projects"},{"authors":[""],"categories":null,"content":"NUBIS contribution: As WP3 leader, NUBIS designed and developed a lightweight, ultra-performant SMO for Desire6G, able to scale out compute-intensive parts of service decomposition as standalone, scalable serverless functions. Additionally, NUBIS enabled interoperable hardware acceleration mechanisms for full data-plane programmability.\n","date":1764547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1764547200,"objectID":"345518ae3ac928e5badf8a67c3822a2f","permalink":"/projects/desire6g/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/projects/desire6g/","section":"projects","summary":"Zero-touch, AI-native control/management/orchestration for eXtreme URLLC in beyond-5G","tags":["completed"],"title":"DESIRE6G","type":"projects"},{"authors":null,"categories":null,"content":"From Containers to KServe and vAccel Container images have become the standard unit of software packaging and deployment. They’re everywhere: in the cloud, on edge devices, and even in AI inference pipelines. Yet, despite the ubiquity of OCI (Open Container Initiative) registries and image formats, there hasn’t been a clean, lightweight C library for fetching and unpacking OCI images.\nWe’ve built exactly that: a minimalist C client library for OCI registries, designed for embedding in systems software, unikernel runtimes, and edge-native inference frameworks. The inspiration was ModelPack, a recent CNCF sandbox project that establishes open standards for packaging, distributing and running AI artifacts in the cloud-native environment.\nWhy a C OCI Client? Most existing OCI tooling, such as skopeo, ORAS, or Docker, is written in Go or Python. While these tools are excellent for command-line and automation tasks, they’re heavy, dynamically linked, and not suitable for embedding into low-level runtimes or constrained environments.\nWhen you want to pull container artifacts directly into a C codebase, say, a unikernel launcher, an inference runtime, or an edge orchestrator, your options are limited. You’d need to shell out to external tools or reimplement the OCI registry protocol from scratch.\nThat’s where our OCI Client Library comes in.\nAbout the Project The library provides a clean, almost dependency-free API for interacting with OCI-compliant registries. It can:\nRetrieve manifests (including multi-architecture ones) Download layer blobs by digest Extract .tar.gz layers to a filesystem Even handle fetch authentication tokens from registries (yes, even for public registry repos, you need an auth token!) Under the hood, it uses libcurl, cJSON, and libarchive, but all those details are hidden. Applications link against a single, self-contained library and call a handful of (we hope) intuitive functions.\nA Simpler API The library’s design philosophy is straightforward: fetching and unpacking container images should be as simple as fetching a file.\nHere’s what using it looks like:\noci_client_init(); char *token = fetch_token(\u0026#34;https://harbor.nbfc.io\u0026#34;, \u0026#34;models/resnet101-v2.7\u0026#34;); // get a token char *manifest = fetch_manifest(\u0026#34;https://harbor.nbfc.io\u0026#34;, // registry \u0026#34;models/resnet101-v2.7\u0026#34;, // repo \u0026#34;tvm\u0026#34;, // tag \u0026#34;amd64\u0026#34;, // arch \u0026#34;linux\u0026#34;, // os token); // auth struct OciLayer *layers; int n = oci_manifest_parse_layers(manifest, \u0026amp;layers); // helper function to parse the layers // Get the layers one by one, and extract them for (int i = 0; i \u0026lt; n; i++) { struct Memory *blob = fetch_blob(\u0026#34;https://harbor.nbfc.io\u0026#34;, \u0026#34;models/resnet101-v2.7\u0026#34;, layers[i].digest, token, NULL); extract_tar_gz(layers[i].filename, \u0026#34;output\u0026#34;); } oci_layers_free(layers, n); // cleanup oci_client_cleanup(); // cleanup That’s all it takes to pull and extract an OCI image layer-by-layer in native C code.\nIntegration with KServe and vAccel This library isn’t just for experiments, it’s becoming a key part of our cloud-native acceleration stack. Specifically, in vAccel we refactored the vAccel Resource handling to allow:\nlocal files (as before), remote files (from URIs), to fetch models/TVM shared objects etc. multiple files (either archives or compressed archives), to account for TF saved models The plan is to add an extra, OCI option, to vAccel, to facilitate software delivery of models to vAccel instances (agents or applications).\nThis functionality, will enable efficient model fetching in KServe deployments that use vAccel. Thus, instead of relying on KServe’s STORAGE_URI and side-car containers to fetch the models and make them available to the inference service container, we just specify the OCI URI (oci://harbor.nbfc.io/models/resnet101-v2.7:tvm) and the binary artifact is available to vAccel as a Resource, ready to be loaded by the relevant plugin/backend.\nAdditionally, to leverage KServe’s simplified workflow, we could patch the code to allow for a custom side-car that just fetches the model like that, without relying on heavy-weight containers in Python/Go. This way, we make deployments faster, more portable, and suitable for edge devices with limited resources.\nWhy store models in OCI Registries ? Storing models as OCI artifacts transforms them into first-class, verifiable, and portable software units, aligning ML deployment with modern DevOps and GitOps practices. Using OCI registries to store ML models provides several important benefits:\nImmutability: Once pushed, layers and manifests are immutable. This ensures that models cannot be tampered with after release. Verification \u0026amp; Trust: Tools like cosign allow signing and verifying models, ensuring integrity and origin. Provenance: Registry manifests track digests, timestamps, and annotations, making it easy to track model versions. Compatibility: OCI is an open standard, widely supported across cloud providers, edge runtimes, and orchestration systems. Layered Storage \u0026amp; Reuse: Common dependencies can …","date":1761436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1761436800,"objectID":"922962c97938e54ec44ebc2d70678e05","permalink":"/blog/fetch_models_in_c/","publishdate":"2025-10-26T00:00:00Z","relpermalink":"/blog/fetch_models_in_c/","section":"blog","summary":"From Containers to KServe and vAccel Container images have become the standard unit of software packaging and deployment. They’re everywhere: in the cloud, on edge devices, and even in AI inference pipelines. Yet, despite the ubiquity of OCI (Open Container Initiative) registries and image formats, there hasn’t been a clean, lightweight C library for fetching and unpacking OCI images.\n","tags":["Kserve","vAccel","AI","Inference","Models","C","OCI"],"title":"A Lightweight C Library for Fetching OCI Artifacts","type":"blog"},{"authors":null,"categories":null,"content":"When a Release Breaks Your CI We’re a small engineering team. Everyone’s busy! Some days we’re deep in container runtime dev, other days we’re debugging transport layers for vAccel or measuring latency for torch model execution offloading across Edge devices. What we don’t have is a dedicated team for CI maintenance.\nSo when our GitHub Actions runners went down again, we heard the familiar chorus:\n“CI is down.”\n“Is the cluster up?”\n“Did someone change something?”\nNobody had. The culprit was subtler and (once more) frustrating.\nGitHub follows a 30-day policy to update the runner software. Specifically:\nAny updates released for the software, including major, minor, or patch releases, are considered as an available update. If you do not perform a software update within 30 days, the GitHub Actions service will not queue jobs to your runner. In addition, if a critical security update is required, the GitHub Actions service will not queue jobs to your runner until it has been updated.\nBut if you’re like us, running self-hosted GH runners on containers, then the auto-update feature of the runner does not scale well.\nAs a result, when v2.328.0 was released, the older v2.327.1 became unsupported. Our self-hosted runners, built with the old version, were simply not receiving jobs by GitHub.\nJust a broken CI.\nWe were wasting hours each time this happened: manually updating runner versions, rebuilding images, and redeploying. We needed a fix that was automatic, self-contained, and didn’t require human babysitting.\nThe Key Insight: Use Dependabot as a Build Trigger Dependabot is best known for keeping dependencies up to date. It can bump versions of Go packages, rust crates, container images, and even GitHub Actions.\nThat last part, GitHub Actions, turned out to be our way out.\nWhat if we asked Dependabot to track the version of actions/runner,\nand used its pull request as the trigger to rebuild our runner images?\nThat single insight became the foundation of a hands-free CI maintenance pipeline.\nWe will follow-up on another post about our CI setup, as we have upgraded to ARC since our last post about it. Runner images are mostly based on some-natalie’s kubernoodles.\nStep 1: The Tracker We added a minimal file to .github/workflows:\n# _track_runner.yml uses: actions/runner@v2.329.0 Then we configured Dependabot to watch that folder:\n# .github/dependabot.yml version: 2 updates: - package-ecosystem: \u0026#34;github-actions\u0026#34; directory: \u0026#34;/.github/workflows\u0026#34; schedule: interval: \u0026#34;daily\u0026#34; That’s it.\nWhenever GitHub publishes a new actions/runner release, Dependabot opens a PR like:\nchore(deps): Bump actions/runner from v2.327.1 to v2.329.0\nThat PR became our signal: the moment Dependabot does its job, we rebuild our runners automatically.\nStep 2: The Trigger Workflow When Dependabot’s PR gets issued, it triggers a lightweight workflow that extracts the runner version and dispatches a full image rebuild.\n# .github/workflows/build-trigger.yml on: pull_request: branches: [ main ] paths: - \u0026#34;.github/workflows/_track_runner.yml\u0026#34; jobs: extract_version: runs-on: ubuntu-latest outputs: runner-version: ${{ steps.extract.outputs.version }} steps: - uses: actions/checkout@v5 with: ref: ${{ github.event.pull_request.head.ref }} fetch-depth: 0 - id: extract run: | version=$(grep -oE \u0026#39;actions/runner@v[0-9]+\\.[0-9]+\\.[0-9]+\u0026#39; .github/workflows/_track_runner.yml | cut -d@ -f2 | sed \u0026#39;s/^v//\u0026#39;) echo \u0026#34;version=$version\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT build_base: needs: extract_version uses: ./.github/workflows/build-latest.yml with: runner-version: ${{ needs.extract_version.outputs.runner-version }} runner: \u0026#39;[\u0026#34;base\u0026#34;,\u0026#34;dind\u0026#34;,\u0026#34;2204\u0026#34;]\u0026#39; runner-archs: \u0026#39;[\u0026#34;amd64\u0026#34;,\u0026#34;arm64\u0026#34;,\u0026#34;arm\u0026#34;]\u0026#39; dockerfiles: \u0026#39;[\u0026#34;jammy-base\u0026#34;,\u0026#34;noble-base\u0026#34;]\u0026#39; secrets: inherit build_custom: needs: [extract_version,build_base] uses: ./.github/workflows/build-latest.yml with: runner-version: ${{ needs.extract_version.outputs.runner-version }} runner: \u0026#39;[\u0026#34;base\u0026#34;,\u0026#34;dind\u0026#34;,\u0026#34;2204\u0026#34;]\u0026#39; runner-archs: \u0026#39;[\u0026#34;amd64\u0026#34;,\u0026#34;arm64\u0026#34;]\u0026#39; dockerfiles: \u0026#39;[\u0026#34;jammy-tf\u0026#34;,\u0026#34;jammy-torch\u0026#34;,\u0026#34;jammy-tvm\u0026#34;,\u0026#34;jammy-opencv\u0026#34;]\u0026#39; secrets: inherit auto_merge: needs: [build_base,build_custom] if: github.actor == \u0026#39;dependabot[bot]\u0026#39; runs-on: ubuntu-latest steps: - name: Auto-merge Dependabot PR run: | gh pr merge ${{ github.event.pull_request.number }} --rebase --delete-branch --admin --repo ${{ github.repository }} env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} That small workflow is the bridge: it detects the change, parses the version, and tells our builder to get to work.\nStep 3: Building \u0026amp; Signing Custom Runner Images Our main workflow, build-latest.yml, handles the heavy lifting:\nMulti-arch container image builds for amd64, arm64, and arm Image publishing to our Harbor registry cosign signing for supply chain integrity Example output images:\nharbor.nbfc.io/nubificus/runner-images/jammy-base:amd64-d3aa6e9 harbor.nbfc.io/nubificus/runner-images/jammy-base:arm64-d3aa6e9 We sign these images using cosign in keyless mode (using GH’s OIDC) and we merge the per-arch images into a …","date":1761177600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1761177600,"objectID":"4eb0d494e9986e8ca23859a130b64f24","permalink":"/blog/gh_runner_updates/","publishdate":"2025-10-23T00:00:00Z","relpermalink":"/blog/gh_runner_updates/","section":"blog","summary":"When a Release Breaks Your CI We’re a small engineering team. Everyone’s busy! Some days we’re deep in container runtime dev, other days we’re debugging transport layers for vAccel or measuring latency for torch model execution offloading across Edge devices. What we don’t have is a dedicated team for CI maintenance.\n","tags":["GitHub Actions","Dependabot","CI/CD","Automation","Harbor","Cosign","Containers"],"title":"Keeping Our GitHub Runners Alive with Dependabot","type":"blog"},{"authors":null,"categories":null,"content":"","date":1760918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1760918400,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"/people/","publishdate":"2025-10-20T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"People","type":"landing"},{"authors":["Charalampos Mainas"],"categories":null,"content":"Further Reading urunc - A Container Runtime for Unikernels WebAssembly and urunc Integration ","date":1756920600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1756920600,"objectID":"8ee5858b289ec6bfc578d0faa6658800","permalink":"/event/containerdays2025/","publishdate":"2025-09-03T00:00:00Z","relpermalink":"/event/containerdays2025/","section":"event","summary":"Unikernel Deployment Made Easy - A CRI-Compatible Runtime for Secure Cloud Deployments","tags":["unikernels","container-runtime","kubernetes","security","urunc"],"title":"ContainerDays Hamburg 2025: Unikernel Deployment Made Easy: A CRI-Compatible Runtime for Secure Cloud Deployments","type":"event"},{"authors":["Charalampos Mainas"],"categories":null,"content":"","date":1756389e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1756389e3,"objectID":"2f878e7f46404ce40d0f7ebf8d250340","permalink":"/event/containerplumbing2025/","publishdate":"2025-08-28T00:00:00Z","relpermalink":"/event/containerplumbing2025/","section":"event","summary":"urunc: A novel container runtime that runs lightweight VMs as containers, offering VM-grade isolation with low overhead and seamless Kubernetes integration.","tags":["containers","unikernels","Kubernetes","CRI","sandboxing","urunc"],"title":"Container Plumbing Days - urunc: A Container runtime for unikernels and single application kernels","type":"event"},{"authors":["Anastassios Nanos","Charalampos Mainas"],"categories":null,"content":"","date":1756209300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1756209300,"objectID":"43b1317d865bf826907a48b7a1c61763","permalink":"/event/oss2025/","publishdate":"2025-08-26T00:00:00Z","relpermalink":"/event/oss2025/","section":"event","summary":"Open-source system for integrating resource-constrained IoT devices like ESP32-based MCUs into Kubernetes-managed environments with secure onboarding and OTA management.","tags":["IoT","Kubernetes","OTA","Security","CNIOT"],"title":"OSS 2025 - Cloud Native IoT: OTA Updates and Device Repurposing","type":"event"},{"authors":null,"categories":null,"content":"If you’ve ever tried to debug a PyTorch program on an ARM64 system using Valgrind, you might have stumbled on something really odd: “Why does it take so long?”. And if you’re like us, you would probably try to run it locally, on a Raspberry pi, to see what’s going on… And the madness begins!\nTL;DR, as you probably figured out from the title of this post, it’s a counter-intuitive experience: the more cores your machine has, the slower your (torch) code seems to run under Valgrind. Shouldn’t more cores mean more speed? Let’s dive into why that’s not always the case ;)\nThe background In an effort to improve our testing infrastructure for vAccel and make it more robust, we started cleaning up our examples, unifying the build \u0026amp; test scripts and started adding more elaborate test cases for both the library and the plugins. Valgrind provides a quite decent experience for this, especially to catch multi-arch errors, memory leaks and dangling pointers (something quite common when writing in C :D).\nThe issue While adding the Valgrind mode of execution in our tests for the vAccel plugins, we noticed something really weird in the Torch case. The test was taking forever!\nFigure 1: Build \u0026amp; Test run on amd64 Specifically, while the equivalent amd64 was taking roughly 4 and a half minutes (Figure 1), the arm64 run was taking nearly an hour (53 minutes) – see Figure 2.\nFigure 2: Why is it taking sooo long? Debugging The first thing that came to mind was that there’s something wrong with our infrastructure. We run self-hosted Github runners, with custom container images that support the relevant software components we need for each plugin/case. We run those on our infra, a set of VMs running on top of diverse low-end bare-metal machines, both amd64 and arm64. The arm64 runners run on a couple of Jetson AGX Orins, with 8 cores and 32GB of RAM.\nAnd what’s the first thing to try (especially when debugging on arm64? A Raspberry Pi of course!\nSo getting the runner container image on a Raspberry Pi 5, with 8GB of RAM, spinning up the container, building the library and the plugin, all took roughly 10 minutes. And we’re ready for the test:\n# ninja run-examples-valgrind -C build-container ninja: Entering directory `build-container\u0026#39; [0/1] Running external command run-examples-valgrind (wrapped by meson to set env) Arch is 64bit : true [snipped] Running examples with plugin \u0026#39;libvaccel-torch.so\u0026#39; + valgrind --leak-check=full --show-leak-kinds=all --track-origins=yes --errors-for-leak-kinds=all --max-stackframe=3150000 --keep-debuginfo=yes --error-exitcode=1 --suppressions=/home/ananos/develop/vaccel-plugin-torch/scripts/common/config/valgrind.supp --main-stacksize=33554432 --max-stackframe=4000000 --suppressions=/home/ananos/develop/vaccel-plugin-torch/scripts/config/valgrind.supp /home/runner/artifacts/bin/torch_inference /home/runner/artifacts/share/vaccel/images/example.jpg https://s3.nbfc.io/torch/mobilenet.pt /home/runner/artifacts/share/vaccel/labels/imagenet.txt ==371== Memcheck, a memory error detector ==371== Copyright (C) 2002-2024, and GNU GPL\u0026#39;d, by Julian Seward et al. ==371== Using Valgrind-3.25.1 and LibVEX; rerun with -h for copyright info ==371== Command: /home/runner/artifacts/bin/torch_inference /home/runner/artifacts/share/vaccel/images/example.jpg https://s3.nbfc.io/torch/mobilenet.pt /home/runner/artifacts/share/vaccel/labels/imagenet.txt ==371== 2025.07.10-20:48:01.91 - \u0026lt;debug\u0026gt; Initializing vAccel 2025.07.10-20:48:01.93 - \u0026lt;info\u0026gt; vAccel 0.7.1-9-b175578f 2025.07.10-20:48:01.93 - \u0026lt;debug\u0026gt; Config: 2025.07.10-20:48:01.93 - \u0026lt;debug\u0026gt; plugins = libvaccel-torch.so 2025.07.10-20:48:01.93 - \u0026lt;debug\u0026gt; log_level = debug 2025.07.10-20:48:01.93 - \u0026lt;debug\u0026gt; log_file = (null) 2025.07.10-20:48:01.93 - \u0026lt;debug\u0026gt; profiling_enabled = false 2025.07.10-20:48:01.93 - \u0026lt;debug\u0026gt; version_ignore = false 2025.07.10-20:48:01.94 - \u0026lt;debug\u0026gt; Created top-level rundir: /run/user/0/vaccel/ZpNkGT 2025.07.10-20:48:47.87 - \u0026lt;info\u0026gt; Registered plugin torch 0.2.1-3-0b1978fb [snipped] 2025.07.10-20:48:48.07 - \u0026lt;debug\u0026gt; Downloading https://s3.nbfc.io/torch/mobilenet.pt 2025.07.10-20:48:53.18 - \u0026lt;debug\u0026gt; Downloaded: 2.4 KB of 13.7 MB (17.2%) | Speed: 474.96 KB/sec 2025.07.10-20:48:54.93 - \u0026lt;debug\u0026gt; Downloaded: 13.7 MB of 13.7 MB (100.0%) | Speed: 2.01 MB/sec 2025.07.10-20:48:54.95 - \u0026lt;debug\u0026gt; Download completed successfully 2025.07.10-20:48:55.04 - \u0026lt;debug\u0026gt; session:1 Registered resource 1 2025.07.10-20:48:56.37 - \u0026lt;debug\u0026gt; session:1 Looking for plugin implementing torch_jitload_forward operation 2025.07.10-20:48:56.37 - \u0026lt;debug\u0026gt; Returning func from hint plugin torch [snipped] CUDA not available, running in CPU mode Success! Result Tensor : Output tensor =\u0026gt; type:7 nr_dims:2 size: 4000 B Prediction: banana [snipped] ==371== HEAP SUMMARY: ==371== in use at exit: 339,636 bytes in 3,300 blocks ==371== total heap usage: 1,779,929 allocs, 1,776,629 frees, 405,074,676 bytes allocated ==371== ==371== LEAK SUMMARY: ==371== definitely lost: 0 bytes in 0 blocks ==371== indirectly lost: 0 bytes …","date":1752217371,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752217371,"objectID":"843f06b1a6c23245ef03bb3746290d10","permalink":"/blog/torch-arm-debug/","publishdate":"2025-07-11T08:02:51+01:00","relpermalink":"/blog/torch-arm-debug/","section":"blog","summary":"If you’ve ever tried to debug a PyTorch program on an ARM64 system using Valgrind, you might have stumbled on something really odd: “Why does it take so long?”. And if you’re like us, you would probably try to run it locally, on a Raspberry pi, to see what’s going on… And the madness begins!\n","tags":["vaccel","Torch","Slowdown","multi-cores"],"title":"When More Cores Means Less Speed: Debugging PyTorch with Valgrind on ARM","type":"blog"},{"authors":[""],"categories":null,"content":"We are thrilled to announce the release of urunc v0.6.0, a feature-packed update that pushes unikernel-native infrastructure even closer to everyday container workflows.\nThis release marks a major milestone: urunc is now an official CNCF Sandbox Project, a strong signal of our commitment to open collaboration, cloud-native principles, and production-grade reliability.\nThis release adds support for WASM Mewz unikernels, and significantly, for single-app, Linux-based containers by transforming them into minimal Linux VMs. You can now:\nPivot and execute the monitor process from a freshly created root filesystem (chroot) Share the container’s root filesystem with the guest using 9pfs Mount volumes into the container’s rootfs Boot single-application, Linux-based guests, not just unikernels Set CLI options at runtime Pass environment variables into the guest It also enhances logging and error handling, introduces more intuitive CLI semantics, and brings a polished documentation redesign, complete with a brand new logo and improved usability.\nBreaking Changes The useDMBlock annotation has been replaced with mountRootfs, offering a unified way to mount the container rootfs using block devices or shared filesystems. Unikraft CLI handling now reserves argv[0] for consistency with kraftkit’s behavior. CI/CD \u0026amp; Internals Moved entirely to GitHub-hosted runners Added automated docs publishing, better license checking, and spellchecks (code \u0026amp; docs) Improved isolation and test reproducibility by removing org-wide secret dependencies Refactored CI workflows to facilitate external (non-org) PRs Full Changelog\nDocs, Governance \u0026amp; Community We’re especially excited about the community-building steps in this release. In addition to technical improvements, we’ve:\nIntroduced clear project governance Published a dedicated security policy Refreshed the README with: Roadmap links Contribution guidelines OpenSSF best practices badge A public Slack channel to join the conversation We invite developers, researchers, and users to collaborate, share feedback, and help shape the future of Urunc.\nCNCF Milestone Becoming a CNCF Sandbox Project marks an important milestone for urunc. This recognition opens up new collaboration opportunities and validates our vision: to bring secure, low-overhead, unikernel-native execution to cloud-native environments through familiar interfaces and open standards.\nFor more information, demos, tutorials, and contribution details, visit: https://urunc.io\nTry it out Run Urunc in Kubernetes Use Knative with Urunc Boot OCI Containers as VMs Join the Slack \u0026amp; Get Involved Contact info@urunc.io\nGitHub: github.com/urunc-dev/urunc\n","date":1751500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1751500800,"objectID":"b09e0854e9efd76476712cb6d3874023","permalink":"/post/2025-07-03-urunc-v060-released-linux-guests-volume-mounts-cncf-sandbox/","publishdate":"2025-07-03T00:00:00Z","relpermalink":"/post/2025-07-03-urunc-v060-released-linux-guests-volume-mounts-cncf-sandbox/","section":"post","summary":"We are thrilled to announce the release of urunc v0.6.0, a feature-packed update that pushes unikernel-native infrastructure even closer to everyday container workflows.\n","tags":["news"],"title":"urunc v0.6.0 Released: Linux Guests, Volume Mounts \u0026 CNCF Sandbox","type":"post"},{"authors":[""],"categories":null,"content":"Simplified Session Management, Extended RPC Support, and Robust Backend Integration\nKey Features and Improvements v0.7.0 Highlights Released: June 18, 2025\nEnable runtime plugin loading and coexistence of remote and local plugins Allow selection of inference model at runtime Add unified config and enable explicit bootstrap/cleanup Refactor vAccel resources Enable robust plugin versioning and compatibility checks Cleanup and reorganize headers and plugin functions v0.7.1 Highlights Released: June 24, 2025\nBug Fixes \u0026amp; Stability, Resolves memory issues and plugin initialization bugs. Extending the vAccel Ecosystem With its refactored resource architecture and plugin lifecycle improvements, vAccel can now better support heterogeneous accelerators, serverless runtimes, and secure offloading at the edge. These improvements build on ongoing research into cloud-native compute offloading.\nAvailability Source code and binaries are available under the Apache 2.0 License:\nv0.7.0 Release Notes v0.7.1 Release Notes Visit the GitHub repository: github.com/nubificus/vaccel\nAbout vAccel vAccel is an open-source virtualization acceleration framework by Nubificus LTD, enabling efficient offloading of compute workloads to accelerators or remote endpoints across the edge–cloud continuum. It features modular plugins for backends, transports, and runtimes, and is designed to work seamlessly with containerized and serverless environments.\nHands-on! For more information, tutorials, and links, visit: https://docs.vaccel.org\nContact:\nvaccel@nubificus.co.uk GitHub: github.com/nubificus/vaccel ","date":1750809600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1750809600,"objectID":"cd0da3891509ee1a567c0aa842408cc8","permalink":"/post/2025-06-25-vaccel-v07-released/","publishdate":"2025-06-25T00:00:00Z","relpermalink":"/post/2025-06-25-vaccel-v07-released/","section":"post","summary":"Simplified Session Management, Extended RPC Support, and Robust Backend Integration\n","tags":["news"],"title":"vAccel v0.7 Released!","type":"post"},{"authors":null,"categories":null,"content":"That’s what we thought when setting up a BERT-based hate speech classifier. This was part of a broader experiment using vAccel, our hardware acceleration abstraction for AI inference across the Cloud-Edge-IoT continuum.\nWe had offloading working locally (on the same physical host \u0026amp; OS), and started experimenting with our transport plugins to make sure everything works smoothly so that we can deploy that as part of a distributed kubernetes setup. First thing to try was localhost, and we expected communication to be lightning fast. Instead, we got… surprises.\nThe original experiment How BERT works The BERT model (Bidirectional Encoder Representations from Transformers) is a transformer-based architecture that maps input text to contextual embeddings. In this example, we’re using a distilled BERT checkpoint, traced via TorchScript (cnn_trace.pt), to classify short tweets into three categories:\noffensive-language hate-speech neither Each line of input (a tweet) goes through the following stages:\nTokenization The tweet is split into word/subword tokens using a predefined vocabulary and tokenizer (e.g. WordPiece). Each token is mapped to an integer ID.\nEmbedding + Encoding These token IDs are passed through BERT’s embedding layer and several transformer encoder blocks, generating context-aware representations of each token.\nClassification Head For classification, we only use the embedding of the special [CLS] token (added at the start). This vector is fed into a small feed forward layer that outputs logits for each class.\nPrediction The class with the highest logit is selected as the prediction.\nThe model is serialized with TorchScript so that it can be loaded and run from C++ or via runtime frameworks like vAccel. This avoids Python overhead and allows seamless execution across backends (CPU, CUDA, remote offload, etc.).\nSo if we run this on a subset of an example dataset and see the following:\n$ ./build-stock-cpu/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt Processing 100 lines from: build-local/tweets_100.txt == [Vocab Loaded] == [snipped] Line 5: Duration: 141.214 ms Prediction: offensive-language Line 6: Duration: 115.528 ms Prediction: offensive-language Line 7: Duration: 117.649 ms Prediction: offensive-language [snipped] Line 10: Duration: 69.3163 ms Prediction: neither [snipped] Line 99: Duration: 117.06 ms Prediction: offensive-language Line 100: Duration: 110.692 ms Prediction: hate-speech Average (after 4rd iteration): 92.07 ms CPU execution on such models seems to take quite some time. If you enable GPU execution we get something really better:\n$ ./build-stock/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt Processing 100 lines from: build-local/tweets_100.txt == [Vocab Loaded] == == [Using GPU] == [snipped] Line 5: Duration: 7.98571 ms Prediction: offensive-language Line 6: Duration: 7.81541 ms Prediction: offensive-language Line 7: Duration: 7.76802 ms Prediction: offensive-language [snipped] Line 10: Duration: 8.22414 ms Prediction: neither [snipped] Line 99: Duration: 7.76586 ms Prediction: offensive-language Line 100: Duration: 7.8277 ms Prediction: hate-speech Average (after 4rd iteration): 7.88 ms How vAccel facilitates the execution vAccel enables seamless interchange between hardware and transport plugins at runtime. So given a port of this classifier to consume the vAccel API, all we need to do is configure vAccel to use the CPU or the GPU plugin at runtime.\n$ export VACCEL_LOG_LEVEL=3 $ export VACCEL_PLUGINS=$HOME/vaccel-plugin-torch/build-cpu/src/libvaccel-torch.so $ ./build-local/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt 2025.06.22-13:44:38.82 - \u0026lt;info\u0026gt; vAccel 0.7.0-7-e67e52b6 2025.06.22-13:44:38.82 - \u0026lt;info\u0026gt; Registered plugin torch 0.2.0-2-f80dd939-dirty Processing 100 lines from: build-local/tweets_100.txt == [Vocab Loaded] == 2025.06.22-13:44:38.86 - \u0026lt;warn\u0026gt; Path does not seem to have a `\u0026lt;prefix\u0026gt;://` 2025.06.22-13:44:38.86 - \u0026lt;warn\u0026gt; Assuming build-local/cnn_trace.pt is a local path Created new model resource 1 Initialized vAccel session 1 [snipped] Line 5: Duration: 142.267 ms Prediction: offensive-language Line 6: Duration: 116.333 ms Prediction: offensive-language Line 7: Duration: 117.776 ms Prediction: offensive-language [snipped] Line 10: Duration: 69.8447 ms Prediction: neither [snipped] Line 99: Duration: 118.195 ms Prediction: offensive-language Line 100: Duration: 111.499 ms Prediction: hate-speech Average (after 4rd iteration): 92.41 ms and the equivalent GPU execution by just tweaking an environment variable:\n$ export VACCEL_LOG_LEVEL=3 $ export VACCEL_PLUGINS=$HOME/vaccel-plugin-torch/build-gpu/src/libvaccel-torch.so $ ./build-local/classifier -m build-local/cnn_trace.pt -v bert_cased_vocab.txt -f build-local/tweets_100.txt 2025.06.22-13:50:09.38 - \u0026lt;info\u0026gt; vAccel 0.7.0-7-e67e52b6 2025.06.22-13:50:09.39 - \u0026lt;info\u0026gt; Registered plugin torch …","date":1750575771,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1750575771,"objectID":"3b24f70840adb81b7e8c41f9db4ced9d","permalink":"/blog/ttrpc-tcp/","publishdate":"2025-06-22T08:02:51+01:00","relpermalink":"/blog/ttrpc-tcp/","section":"blog","summary":"That’s what we thought when setting up a BERT-based hate speech classifier. This was part of a broader experiment using vAccel, our hardware acceleration abstraction for AI inference across the Cloud-Edge-IoT continuum.\n","tags":["vaccel","Nagle","TCP","TTRPC","rust"],"title":"“It’s just localhost. How slow could it be?”","type":"blog"},{"authors":[""],"categories":null,"content":" The urunc team is excited to announce the release of urunc v0.5.0, a major step forward in our mission to make unikernel-based deployments seamless, secure, and cloud-native. This release expands urunc’s capabilities across containerized and orchestrated environments, with powerful new features, enhanced monitor control, and expanded compatibility.\nWhat’s New in v0.5.0 Namespace \u0026amp; Compatibility Enhancements Full namespace support (except user namespaces) unlocks broader container and runtime integrations. MirageOS support brings OCaml-based unikernels into the fold, expanding Urunc’s reach across the unikernel ecosystem.\nKubernetes-First Deployments urunc_deploy introduced for one-command installation and configuration in existing Kubernetes clusters. Non-root monitor execution now supported, enhancing security and compliance for enterprise-grade environments.\nSmarter Monitor Interface A revamped monitor interface introduces specialized CLI hooks:\nMonitorBlockCli() for block device options MonitorNetCli() for network-specific flags MonitorCli() for general monitor options Monitors now spawn directly from the container’s root filesystem.\nCloud-Native Integrations Fixes for Knative readiness probes and improvements to devmapper/rootfs handling ensure tighter integration with serverless platforms and container runtimes.\nCI/CD, Docs \u0026amp; Developer Experience Go updated to v1.24.1, introduced End-to-end testing powered by Incus, replacing legacy VM spawning. CI enhancements include warning handling during container ops and a cleanup workflow for stale issues/PRs.\nDocumentation improvements include: Updated Kubernetes YAML examples, added new tutorials for EKS and Knative, and expanded examples on unikernel packaging\nRead the Full Changelog View on GitHub\nHands-on! urunc in k8s Knative \u0026amp; urunc urunc on EKS urunc continues to push the boundaries of lightweight virtualization, bringing the benefits of unikernels to modern cloud-native environments. With v0.5.0, we’re building the foundation for secure, performant, and flexible infrastructure at scale.\nFor more information, tutorials, and community links, visit: https://urunc.io\nContact:\ninfo@urunc.io GitHub: github.com/nubificus/urunc ","date":1744243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1744243200,"objectID":"c6f6b06a53860a34b7ca0bcf94b02337","permalink":"/post/2025-04-10-urunc-v050-released-more-unikernel-power-now-k8s-native/","publishdate":"2025-04-10T00:00:00Z","relpermalink":"/post/2025-04-10-urunc-v050-released-more-unikernel-power-now-k8s-native/","section":"post","summary":" The urunc team is excited to announce the release of urunc v0.5.0, a major step forward in our mission to make unikernel-based deployments seamless, secure, and cloud-native. This release expands urunc’s capabilities across containerized and orchestrated environments, with powerful new features, enhanced monitor control, and expanded compatibility.\n","tags":["news"],"title":"urunc v0.5.0 Released: More Unikernel Power, Now K8s-Native!","type":"post"},{"authors":["Charalampos Mainas"],"categories":null,"content":"Further Reading urunc - A Container Runtime for Unikernels WebAssembly and urunc Integration ","date":1738684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1738684800,"objectID":"45b291bf9efc4aacf88e9169b2c9e5c1","permalink":"/event/cloudnativesummit2025/","publishdate":"2025-02-04T00:00:00Z","relpermalink":"/event/cloudnativesummit2025/","section":"event","summary":"Isolating Workloads in Multi-Tenant Kubernetes Clusters","tags":["kubernetes","security","container-isolation","urunc"],"title":"Cloud-native Summit Munich 2025: Isolating Workloads in Multi-Tenant Kubernetes Clusters","type":"event"},{"authors":["Charalampos Mainas","Anastassios Nanos"],"categories":null,"content":"Further Reading WASM and urunc Integration Code \u0026amp; Resources urunc - Container Runtime Repository bunny - Building Tool Repository ","date":1738486800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1738486800,"objectID":"7a104c324d65fa8689a414f2e82df002","permalink":"/event/fosdem2025-1/","publishdate":"2025-02-02T00:00:00Z","relpermalink":"/event/fosdem2025-1/","section":"event","summary":"WASM Meets Unikernels - Secure and Efficient Cloud-Native Deployments","tags":["webassembly","wasm","unikernels","cloud-native","urunc","security"],"title":"FOSDEM 2025: WASM Meets Unikernels - Secure and Efficient Cloud-Native Deployments","type":"event"},{"authors":["Charalampos Mainas","Anastassios Nanos"],"categories":null,"content":"Code \u0026amp; Resources urunc - Container Runtime Repository bunny - Kernel Building Tool ","date":1738426800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1738426800,"objectID":"39324fb50356c4caec35c017abb35c52","permalink":"/event/fosdem2025-2/","publishdate":"2025-02-01T00:00:00Z","relpermalink":"/event/fosdem2025-2/","section":"event","summary":"Less Overhead, Strong Isolation - Running Containers in Minimal Specialized Linux VMs","tags":["containers","unikernels","isolation","security","urunc","microVMs"],"title":"FOSDEM 2025: Less Overhead, Strong Isolation - Running Containers in Minimal Specialized Linux VMs","type":"event"},{"authors":[""],"categories":null,"content":" Our team has been honored with the 3rd Best Talk Award at the ML4ECS workshop, held during the prestigious HiPEAC conference. Our presentation, titled “Scalable and Lightweight Cloud-Native Application Sandboxing”, showcased innovative approaches to enhancing scalability and efficiency in cloud-native environments.\nThis recognition highlights the team’s groundbreaking work in developing advanced solutions for secure and lightweight application isolation, addressing critical challenges in edge and cloud computing.\nThis award comes shortly after the most stable release of urunc so far, along with significant efforts from our team to streamline Firecracker support for the rust runtime of kata-containers.\nExplore urunc 0.4.0 today! Discover the full release details and download urunc 0.4.0 from GitHub.\nHiPEAC (High-Performance Embedded Architecture and Compilation) is Europe’s leading network for researchers and industry professionals in computer architecture, programming models, and systems software. Its annual conference brings together experts from academia and industry to showcase cutting-edge research and technological advancements in computing systems, making it a key platform for innovation and collaboration in the field. Urunc is an open-source unikernel container runtime, designed to unlock the potential of unikernels in cloud-native environments. By combining efficiency, security, and ease of use, Urunc sets a new standard for lightweight container orchestration. ","date":1737504e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737504e3,"objectID":"cc3f54e18e9b87797386b890fe156013","permalink":"/post/2025-01-22-application-sandboxing-talk-wins-best-talk-award-in-hipeacs-workshop-ml4ecs/","publishdate":"2025-01-22T00:00:00Z","relpermalink":"/post/2025-01-22-application-sandboxing-talk-wins-best-talk-award-in-hipeacs-workshop-ml4ecs/","section":"post","summary":" Our team has been honored with the 3rd Best Talk Award at the ML4ECS workshop, held during the prestigious HiPEAC conference. Our presentation, titled “Scalable and Lightweight Cloud-Native Application Sandboxing”, showcased innovative approaches to enhancing scalability and efficiency in cloud-native environments.\n","tags":["news"],"title":"Application sandboxing talk wins Best Talk award in HiPEAC's workshop ML4ECS","type":"post"},{"authors":["Avgi Vitanidi","Anastassios Nanos"],"categories":null,"content":"","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735689600,"objectID":"4c031ce3afcd3e4a9065731050439e86","permalink":"/publication/vitanidi-2025/","publishdate":"2025-10-15T23:59:22.822956Z","relpermalink":"/publication/vitanidi-2025/","section":"publication","summary":"","tags":null,"title":"Harnessing Artificial Intelligence for Early Identification of Autism Spectrum Disorder","type":"publication"},{"authors":["Aristotelis Kretsis","Panagiotis Kokkinos","Polyzois Soumplis","Fotis Kouzinos","Yoray Zack","Marcell Fehér","Márton Sipos","Daniel E Lucani","Kamil Tokmakov","Javad Fadaie Ghotbi"," others"],"categories":null,"content":"","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735689600,"objectID":"1decbe7599035db791665e53d79b28cb","permalink":"/publication/kretsis-2025-integrating/","publishdate":"2025-10-15T23:59:22.798628Z","relpermalink":"/publication/kretsis-2025-integrating/","section":"publication","summary":"","tags":null,"title":"Integrating Heterogeneous Hardware and Software Platforms towards the Edge-Cloud-HPC Continuum","type":"publication"},{"authors":["Anastassios Nanos","Charalampos Mainas","Konstantinos Papazafeiropoulos","Apostolos Giannousas","Ilias Lagomatis","Aristotelis Kretsis"],"categories":null,"content":"","date":1735689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1735689600,"objectID":"562980cc88e1483655e9fc056ecba910","permalink":"/publication/nanos-2025-mliot/","publishdate":"2025-10-15T23:59:22.804847Z","relpermalink":"/publication/nanos-2025-mliot/","section":"publication","summary":"","tags":null,"title":"MLIoT: Transparent and Secure ML Offloading in the Cloud-Edge-IoT Continuum","type":"publication"},{"authors":[""],"categories":null,"content":" Nubificus proudly announces the release of urunc 0.4.0, the next-generation unikernel container runtime. This release introduces new features, internal improvements, and expanded documentation, reflecting our commitment to enabling lightweight, secure, and high-performance unikernel adoption in modern containerized environments.\nWhat’s new in urunc 0.4.0? New Features Docker Support: Seamless integration with Docker for broader compatibility and usability. Seccomp Support in VMMs: Enhanced security through syscall filtering. Block Image Support: Ability to use block images directly within urunc’s container image. Configurable Memory: Dynamically allocate memory based on container specifications. Documentation Revamp urunc’s documentation has undergone a significant overhaul, now providing detailed guidance for users \u0026amp; developers. Explore the new documentation.\nEnhancements to Internals Network Cleanup: Automated deletion of TC rules and TAP devices when terminating unikernels. Advanced Unikernel Interface: New functions for checking supported features, such as: Init(): Initializes the unikernel structure based on arguments. SupportsBlock(): Verifies block device support. SupportsFS(): Checks compatibility with specific filesystem types. Devmapper Enhancements: Refactored snapshot handling with a new USE_DEVMAPPER_AS_BLOCK environment variable. Static Networking: Enabled NAT and IP forwarding for improved connectivity. Annotations Urunc now supports custom annotations to streamline unikernel configurations:\ncom.urunc.unikernel.block: Specifies the block image path within the container. com.urunc.unikernel.blkMntPoint: Defines the mount point for the block image. com.urunc.unikernel.unikernelVersion: Indicates the version of the unikernel. CI/CD and Building Improvements New actions for unit testing and a restructured CI job workflow. Transition to Github ARC runners for enhanced efficiency. Build system refactor to improve usability and flexibility. Miscellaneous Updates Support for Unikraft boot on arm64 Firecracker. Refactor of container handling and improvements in path and annotation management. Bug fixes and internal refinements to ensure stability and performance. Why This Matters urunc 0.4.0 represents a step forward in making unikernels a practical and efficient choice for modern workloads. By introducing features like Docker support and block image compatibility, it bridges the gap between traditional containers and unikernels, offering a hybrid solution tailored to the needs of today’s developers.\nQuote from the Team “This release is the result of tireless work and collaboration across our team. With urunc 0.4.0, we are giving developers the opportunity to fully leverage the advantages of unikernels without sacrificing usability or compatibility with existing ecosystems”.\nExplore urunc 0.4.0 today! Discover the full release details and download urunc 0.4.0 from GitHub.\nStay tuned for our next planned release featuring shared FS support, WASM unikernel support, MirageOS and many more!\nAbout Urunc Urunc is an open-source unikernel container runtime, designed to unlock the potential of unikernels in cloud-native environments. By combining efficiency, security, and ease of use, Urunc sets a new standard for lightweight container orchestration. ","date":1733702400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733702400,"objectID":"8430e61d7dd5bc46d206ef4b4cf0204e","permalink":"/post/2024-12-09-urunc-040-released/","publishdate":"2024-12-09T00:00:00Z","relpermalink":"/post/2024-12-09-urunc-040-released/","section":"post","summary":" Nubificus proudly announces the release of urunc 0.4.0, the next-generation unikernel container runtime. This release introduces new features, internal improvements, and expanded documentation, reflecting our commitment to enabling lightweight, secure, and high-performance unikernel adoption in modern containerized environments.\n","tags":["news"],"title":"urunc 0.4.0 Released!","type":"post"},{"authors":null,"categories":null,"content":"In the world of DevOps and CI/CD pipelines, GitHub Actions stands out as a powerful tool for automation. However, as with all tools, the devil lies in the details; or in this case, the dependency chain. This post dives into the recent horror story we faced when we realized that our armv7l-based self-hosted runners cannot connect to Github in Ubuntu 24.04.\nThe Root of the Problem: Y2038 and .NET Compatibility Ubuntu 24.04 introduced a critical fix to the Y2038 problem for armv7l. By shifting to 64-bit time values, it rendered many older libraries and binaries incompatible. One such casualty was the GitHub Actions runner.\nGitHub’s runner for armv7l is built using .NET 8.0, which remains incompatible with the Y2038 fix. This manifests as SSL connection failures with errors like:\nThe remote certificate is invalid because of errors in the certificate chain: NotTimeValid The problem stems from outdated libraries in .NET 8.0 that don’t handle the updated time format correctly. While .NET 9.0 has introduced Y2038 compatibility, backports to .NET 8.0 are not planned, as confirmed in the dotnet/core discussions.\nDIY Solution: Rebuilding the Runner The immediate workaround involves rebuilding the GitHub Actions runner with .NET 9.0. However, this is easier said than done. The official documentation lacks guidance for such rebuilds, and even attempts with .NET 9.0 SDKs often result in the same issues persisting.\nFortunately, the community stepped in; satmandu provided a pre-built binary release compatible with Ubuntu 24.04, which can be found here. Testing confirmed that this binary works where manually rebuilt runners failed.\nThe Broader Implications This issue highlights the challenges of operating in a diverse ecosystem. Moving to Y2038-compatible time values was a necessary but disruptive step, breaking backward compatibility in critical infrastructure. The reliance on older .NET versions by GitHub Actions created a brittle link in the chain. Luckily, the open-source community played a crucial role in bridging the gap with patches and pre-built binaries.\nIf you’re managing self-hosted runners, test against the latest operating systems and library versions regularly. Be aware of the technology stack underpinning critical systems. As demonstrated here, the interaction between .NET and Ubuntu was at the heart of the issue. Follow GitHub and .NET release notes closely. Breaking changes like those in .NET 9.0 can have far-reaching effects.\nThe armv7l runner saga is a cautionary tale for the DevOps community. While the path forward involves embracing .NET 9.0 and its support for Ubuntu 24.04, the transition highlights the complexities of maintaining compatibility across evolving platforms.\nFor now, if you’re running Ubuntu 24.04 on armv7l, grab the patched binary or dive into the rebuilding process - just be prepared for some manual effort.\nIf you’ve faced similar challenges or have alternative solutions, feel free to share and drop us a line!\n","date":1733236126,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733236126,"objectID":"7507a37f191210271dab13b5997e9bc1","permalink":"/blog/github-runner-armv7l/","publishdate":"2024-12-03T14:28:46Z","relpermalink":"/blog/github-runner-armv7l/","section":"blog","summary":"In the world of DevOps and CI/CD pipelines, GitHub Actions stands out as a powerful tool for automation. However, as with all tools, the devil lies in the details; or in this case, the dependency chain. This post dives into the recent horror story we faced when we realized that our armv7l-based self-hosted runners cannot connect to Github in Ubuntu 24.04.\n","tags":["github","actions","armv7l"],"title":"Horror Story: The Fiasco of armv7l GHA Self-hosted Runners for Ubuntu 24.04","type":"blog"},{"authors":null,"categories":null,"content":"The world of IoT is expanding rapidly, bringing with it a growing demand for scalable, secure, and efficient device management solutions. Traditional approaches often struggle with the complexities of managing heterogeneous fleets of devices while ensuring secure operations and reliable software delivery. Our Cloud-Native IoT project addresses these challenges by leveraging k8s to provide a unified, highly automated, and secure ecosystem tailored for IoT environments.\nAt the core of this architecture is Akri, which acts as the critical bridge between k8s and the diverse array of IoT devices in a fleet. Akri enables seamless discovery and management of IoT devices, treating them as integral components of the k8s ecosystem. This integration eliminates many of the manual steps traditionally required in IoT deployments, allowing developers to deploy and manage applications across devices with the same ease as managing containerized workloads in the cloud.\nUnderstanding Akri’s Architecture Akri’s architecture is designed to extend k8s’ capabilities to IoT devices, creating a unified interface for managing workloads across cloud, edge, and IoT layers. It achieves this through three core components: Akri Controllers, Akri Brokers, and k8s Custom Resources.\nThe Akri Controller operates within the k8s control plane, continuously monitoring for available IoT devices by interacting with various device plugins. These plugins are responsible for identifying devices such as cameras, sensors, or other hardware endpoints, which might be connected via USB, network protocols, or proprietary interfaces. When a device is detected, the Akri Controller registers it as a k8s resource by creating a corresponding custom resource (CR). These CRs make IoT devices discoverable and manageable using k8s-native tools, enabling seamless integration with containerized workloads.\nFigure 1: Akri’s Architecture Once devices are discovered, Akri Brokers come into play. Brokers are lightweight, containerized components deployed as k8s pods to manage the interaction between k8s workloads and the IoT devices they rely on. For instance, if an application requires access to a specific camera or sensor, the Akri Broker ensures the application receives the appropriate connection and data from the device. Multiple brokers can run simultaneously, each tailored to specific device types, ensuring scalability and flexibility across diverse environments.\nAkri’s architecture enables dynamic device discovery and usage. Whether a device is added or removed, Akri automatically updates the cluster’s state, reflecting the current device inventory. This dynamic nature makes it particularly well-suited for environments where IoT devices may frequently change, such as smart factories or mobile deployments. By abstracting the complexities of device management, Akri allows developers to focus on building and deploying IoT applications rather than wrestling with the nuances of device connectivity. For more information on our engagement with Akri please have a look at our previous experiments.\nFigure 2: Akri’s steady state IoT device handling Beyond Application Management: Secure \u0026amp; Efficient OTA Updates While Akri provides a robust foundation for integrating IoT devices with k8s, our architecture goes further to address critical challenges like simplified software delivery, trusted execution and secure on-boarding.\nFlashJob Operator \u0026amp; CRD First, we build a simplified operator and CRD to handle application deployment on IoT devices. We follow the same principles as Akri (and our previous attempts), but we decouple the firmware flashing operation from the inventory setup and discovery of devices: essentially, we let Akri do the heavy lifting of populating the k8s-related structures with device info, and use this information to initiate a firmware flash operation.\nThe operator subscribes to Akri instances (the device entries in the k8s database) and, when needed, is able to retrieve information about each device. For instance, the operator is able to query the database for the device unique ID, its network endpoint, firmware version, application type etc.\nFigure 3: FlashJob spawn and IoT device update When a request reaches the operator for a firmware flash on a set of devices, the operator fetches the relevant information from the database and spawns a FlashJob Pod with this information that initiates the OTA mechanism for the specific devices. The IoT devices are updated, and, in turn, re-discovered by the relevant discovery handler of Akri. For instance, if the update refers to device re-purposing (running a different application than previously), the relevant discovery handler will detect new devices, whereas the former discovery handler will update that the old device is not reachable, and it will be removed from the specific inventory entry.\nTo facilitate software delivery, the FlashJob Pod fetches the firmware blob that will be flashed onto the device from a …","date":1732354126,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732354126,"objectID":"9d52c884c96365410261421bc20b4aae","permalink":"/blog/cloud-native-iot_v2/","publishdate":"2024-11-23T09:28:46Z","relpermalink":"/blog/cloud-native-iot_v2/","section":"blog","summary":"The world of IoT is expanding rapidly, bringing with it a growing demand for scalable, secure, and efficient device management solutions. Traditional approaches often struggle with the complexities of managing heterogeneous fleets of devices while ensuring secure operations and reliable software delivery. Our Cloud-Native IoT project addresses these challenges by leveraging k8s to provide a unified, highly automated, and secure ecosystem tailored for IoT environments.\n","tags":["Cloud-native","k8s","IoT","Akri"],"title":"Cloud-Native IoT: Redefining Device Management with k8s","type":"blog"},{"authors":null,"categories":null,"content":" WebAssembly (WASM) is rapidly emerging as a transformative technology in the cloud-native ecosystem. Its binary instruction format, designed for execution on a stack-based Virtual Machine (VM), enables WASM modules to run seamlessly on any platform, unlocking unprecedented portability. Moreover, WASM promises near-native execution performance and enhanced security due to its “sandboxed” execution model.\nA closer examination of WASM’s sandboxing reveals certain limitations. In WASM, modules execute within a stack-based VM that isolates them from direct interaction with the external environment. Communication with the outside world is mediated through imports and exports, with no direct access to system calls or underlying resources. Instead, the WebAssembly System Interface (WASI) employs a capability-based system to grant controlled access to external resources. Simply put, the WASM sandbox is essentially a software construct that enforces controlled interactions between the module and its environment.\nWhile this sandboxing model offers strong software-based isolation, it still raises questions about the robustness of security, particularly given the findings of several research papers on WASM’s security vulnerabilities. To achieve stronger isolation, a more robust mechanism, such as hardware-based virtualization, is necessary. By executing each WASM module within its own VM, in the event of a WASM runtime escape, an attacker would also need to breach the VM — a significantly more challenging exercise.\nOn the other hand, VMs have the reputation of being heavyweight and unsuitable for lightweight workloads, such as running individual WASM modules. The overhead in terms of memory and CPU makes traditional virtualization impractical for this purpose.\nUnikernels, a technology introduced in 2013 that has quietly matured over the years, could provide a viable alternative to minimize this overhead. Unikernels are highly specialized, lightweight operating system kernels designed to run a single application efficiently, eliminating the overhead of general-purpose OSes. They feature a minimal footprint, extremely fast boot times, and robust isolation. However, unikernels have historically faced criticism for being difficult to use, with application porting requiring significant effort.\nThis is where WASM complements unikernels beautifully. WASM’s portable binary representation is designed to execute on any compatible environment, including unikernels. This means WASM modules can run on unikernels with minimal adaptation, unlocking a powerful combination: the portability and flexibility of WASM, paired with the lightweight, fast-booting, and hardware-isolated properties of unikernel-based VMs.\nBy combining these technologies, we achieve a compelling solution: WASM modules running within small, fast, and truly isolated unikernel-powered VMs. This approach delivers the best of both worlds: strong isolation through virtualization and efficient resource utilization.\nFortunately, we are not alone in this vision. Several projects have already begun exploring this intersection of WASM, unikernels, and virtualization, demonstrating the potential of this powerful synergy. Specifically:\nIn particular:\nMewz is a unikernel framework designed to only run WASM applications. Unikraft, an active unikernel project, provides support for WAMR. OSv, one of the most well known unikernel framework supports the well known wasmer runtime. Hermit-Wasm used RustyHermit, a unikernel written in Rust to execute WebAssembly applications. So, we can already execute WASM applications in unikernels. But how do we manage these unikernels efficiently? Enter urunc, a container runtime tailored specifically for unikernels. As explained in a previous post, urunc functions as the runc equivalent for unikernels. With urunc, we can deploy, execute, and manage unikernels as seamlessly as managing typical containerized workloads.\nBuilding on this foundation, we explored combining WASM, unikernels, and urunc to create a cohesive system stack that allows us to deploy, execute, and manage WASM applications running on unikernels as easily as traditional containers. To achieve this, we extended urunc to support additional unikernel frameworks, including Mewz and OSv, alongside its existing support for Unikraft. This diversity of unikernel frameworks gives us flexibility in running WASM applications on top of various unikernel environments, each offering unique advantages.\nLet’s take a closer look at each one of them.\nMewz Mewz is a unikernel framework designed to support ultra-lightweight, high-performance workloads. In contrast to other WASM runtimes that execute on top of general purpose operating systems, Mewz is designed as a specialized kernel where WASM applications can execute. In addition, every WASM application executes on a separate Mewz instance, maintaining the single-purpose notion of unikernels. This makes Mewz particularly well-suited for scenarios …","date":1732202532,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1732202532,"objectID":"0c5ebe8a1dabb95f3550344171c174ca","permalink":"/blog/wasm-urunc/","publishdate":"2024-11-21T15:22:12Z","relpermalink":"/blog/wasm-urunc/","section":"blog","summary":" WebAssembly (WASM) is rapidly emerging as a transformative technology in the cloud-native ecosystem. Its binary instruction format, designed for execution on a stack-based Virtual Machine (VM), enables WASM modules to run seamlessly on any platform, unlocking unprecedented portability. Moreover, WASM promises near-native execution performance and enhanced security due to its “sandboxed” execution model.\n","tags":["unikernels","container runtimes","containers","urunc","wasm","mewz","k8s","kubernetes"],"title":"Sandboxing WASM with Unikernels for Secure Cloud-Native Deployments","type":"blog"},{"authors":null,"categories":null,"content":"If you’re into open-source and tech meetups, FOSSCOMM is the event to be at. This year, it was held in Thessaloniki, organized by the open-source community of the University of Macedonia (UoM), and they absolutely crushed it with the setup and vibe!\nMain Entrance – UoM building The atmosphere was awesome—super inclusive and welcoming, with folks from all over Greece and beyond coming together to celebrate open-source software. Whether you were a newcomer or a seasoned developer, everyone felt part of the community, sharing knowledge, projects, and ideas.\nWe were lucky to give two talks at this year’s FOSSCOMM! The first one was about k8s, CRDs, and how we’re using them to tailor k8s clusters to fit our projects’ needs. If you’re curious, here’s the slide deck for that session. We also demoed some of our work—check out the video here. Additionally, we built a step-by-step tutorial for this at Killercoda.\nPanos Mavrikos talking about k8s operators \u0026amp; CRDs The second talk tackled sandboxing workloads in cloud-edge environments, exploring flexibility and performance, the role of hardware accelerators, and how these all impact cloud-edge use cases. It was an exciting session, sparking lots of ideas around security and efficiency. Here’s the slide deck and three demo videos for this talk: Demo 1, Demo 2, and Demo 3.\nAnastassios Nanos talking about hardware acceleration in sandboxed environments The program was absolutely packed this year, with tons of insightful sessions for anyone into open-source, cloud, DevOps, education, IoT, and more! Here are just a couple of standout talks from the packed weekend that really captured the spirit of innovation and open knowledge sharing:\n“Lessons from the Unix History” by Professor Diomidis Spinellis from AUEB: A must-see for any history and tech lovers, this talk walked us through the evolution of Unix, exploring its massive influence on modern operating systems and open-source culture. If you’ve ever wondered how Unix’s legacy continues to impact today’s software, this was the place to be.\n“Running a Company on Free Software: The First 20 Years” by Apollon Oikonomopoulos, CIO of Skroutz: Apollon shared an inspiring journey of building and managing the core infrastructure of a successful business using free software for over two decades. This talk was especially interesting for anyone wondering about the practicalities of relying on open-source tools in a business setting.\n“How to Build a Sustainable Open Source Company” by Frank Karlitschek, CEO of NextCloud: Frank brought tons of insight into what it takes to not just create open-source software but to build a thriving, sustainable business around it. This session had great takeaways on balancing open-source ideals with business realities.\n“Quo Vadis, Free Software? Quo Vadis, Society?” by Luis Falcon, Founder of GNU Solidario: A thought-provoking talk on the societal role of free software, and the vision of a world where open-source tech powers humanitarian causes. This session looked at how free software can impact issues like healthcare and education and was a great reminder of the deeper purpose driving open-source communities.\n“EdgeAI: A HW-SW Codesign Approach” by George Keramidas from Aristotle University of Thessaloniki: This one was a treat for those interested in the cutting-edge (literally!) of AI at the hardware level. George explored the integration of hardware and software for AI at the edge, showing how open-source innovation is shaping this critical area.\nBut it wasn’t just about the talks. FOSSCOMM this year was a hub of energy, ideas, and connections! The booth setups were fantastic, ranging from beginner-friendly intros to cutting-edge tech demos. And the people—wow. We met a huge range of enthusiasts, from students trying open-source for the first time to veterans in the field sharing their latest work.\nBig shoutout to the entire organizing team for a smooth, engaging event. Everything ran like clockwork, and you could really tell how much care went into making it a positive, collaborative space. If you’re thinking of attending next year, don’t hesitate—FOSSCOMM always delivers, and we can’t wait to see what’s in store for next time!\nLooking forward to FOSSCOMM 2025!\n","date":1731312638,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1731312638,"objectID":"eea6ee53815b0ddd2c460b4144ef961e","permalink":"/blog/fosscomm-2024/","publishdate":"2024-11-11T08:10:38Z","relpermalink":"/blog/fosscomm-2024/","section":"blog","summary":"If you’re into open-source and tech meetups, FOSSCOMM is the event to be at. This year, it was held in Thessaloniki, organized by the open-source community of the University of Macedonia (UoM), and they absolutely crushed it with the setup and vibe!\n","tags":["FOSSCOMM","Event","Developers","Containers","Virtualization","vaccel","k8s","CRD","kata-containers"],"title":"Fosscomm 2024","type":"blog"},{"authors":null,"categories":null,"content":"Kubernetes is the de-facto platform to automate the management, deployment, and scaling of containerized applications. One of its strongest features is its ability to scale, allowing users to customize the system according to their needs. A key mechanism for this extensibility is Custom Resource Definitions (CRD).\nWhat are CRDs? CRDs allow Kubernetes users to create and manage their own custom resources. A Custom Resource (CR) is essentially a Kubernetes resource that does not exist by default but is defined via a CRD.\nIn other words, CRDs allow the creation of new resource types that Kubernetes doesn’t know about a priori. These resources behave like Kubernetes’ built-in resources (such as Pods, Services, Deployments), but their structure and functionality are determined by the CRD you create.\nHow do CRDs Work? When we define a CRD, Kubernetes extends its API to handle the new resource type. Once a CRD is registered with the Kubernetes API Server, we can create instances of the custom resource just like we would with native resources. The API server will store these objects in Kubernetes’ distributed key-value store, etcd, and manage their lifecycle.\nFigure 1: Generic K8s Operator For example, after defining a MyApp CRD, we can create MyApp resources in our cluster, just like we would create Pods or Deployments. There are tools available to facilitate the definition, setup, and management of CRDs and their accompanying components. Such a tool is KubeBuilder.\nWhat is KubeBuilder? Kubebuilder is a powerful framework that makes it easy to build and manage Kubernetes operators and APIs. Developed by the Kubernetes team at Google, it is widely recognized as a popular tool among developers working with Kubernetes.\nBasic features Building Operators: Kubebuilder provides a simple and efficient process for building, deploying, and testing Kubernetes operators. Building APIs: With Kubebuilder, users can build custom Kubernetes APIs and resources, allowing them to extend Kubernetes with their own functions and resources. Based on Go: Kubebuilder is written in Go and uses Kubernetes’ Go SDK, providing seamless integration with the Go ecosystem. Now let’s dive into the process of creating our first custom Kubernetes controller using KubeBuilder.\nCreating Custom Kubernetes Controller with KubeBuilder To get hands-on with KubeBuilder, we’ll create a custom resource called MyApp. This resource represents a simple application comprising multiple pods. We’ll also build a Kubernetes controller to manage the lifecycle of these MyApp instances, ensuring the desired state of your application is maintained within the cluster.\nPrerequisites Go version v1.20.0+ Docker version 17.03+. Access to a Kubernetes v1.11.3+ cluster. Installation Let’s install KubeBuilder:\ncurl -L -o kubebuilder \u0026#34;https://go.kubebuilder.io/dl/latest/$(go env GOOS)/$(go env GOARCH)\u0026#34; chmod +x kubebuilder \u0026amp;\u0026amp; sudo mv kubebuilder /usr/local/bin/ Create a project First, let’s create and navigate into a directory for our project. Then, we’ll initialize it using KubeBuilder:\nexport GOPATH=$PWD echo $GOPATH mkdir $GOPATH/operator cd $GOPATH/operator go mod init nbfc.io kubebuilder init --domain=nbfc.io Create a new API # kubebuilder create api --group application --version v1alpha1 --kind MyApp --image=ubuntu:latest --image-container-command=\u0026#34;sleep,infinity\u0026#34; --image-container-port=\u0026#34;22\u0026#34; --run-as-user=\u0026#34;1001\u0026#34; --plugins=\u0026#34;deploy-image/v1-alpha\u0026#34; --make=false This command creates a new Kubernetes API with these parameters:\n–group application: Defines the API group name. –version v1alpha1: The API version. –kind MyApp: The name of custom resource. –image=ubuntu:latest: The default container image to use –image-container-port=“22”: Exposes port 22 on the container –run-as-user=“1001”: Sets the user ID for running the container –plugins=“deploy-image/v1-alpha”: Uses the deploy-image plugin We use these parameters to create a custom resource that includes all necessary deployment settings — such as container image, port, and run-as-user — making it more manageable and production-ready within our Kubernetes environment. By using the deploy-image plugin, we achieve optimal customization for our CR without needing further deployment modifications.\nInstall the CRDs into the cluster make install For quick feedback and code-level debugging, let’s run our controller:\nmake run Create an Application CRD with a custom controller In this section we build an application description as a CRD, along with its accompanying controller that performs simple operations upon spawning. The application consists of a couple of pods and the controller creates services according to the exposed container ports. The rationale is to showcase how easy it is to define logic that accompanies the spawning of pods.\nTo create custom pods using the controller, we need to modify the following files:\nmyapp_controller.go myapp_controller_test.go application_v1alpha1_myapp.yaml myapp_types.go . |-- Dockerfile |-- …","date":1730962971,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730962971,"objectID":"e59570927d976715910cf9e40f95b3a3","permalink":"/blog/crd/","publishdate":"2024-11-07T08:02:51+01:00","relpermalink":"/blog/crd/","section":"blog","summary":"Kubernetes is the de-facto platform to automate the management, deployment, and scaling of containerized applications. One of its strongest features is its ability to scale, allowing users to customize the system according to their needs. A key mechanism for this extensibility is Custom Resource Definitions (CRD).\n","tags":["Kubernetes","CRD"],"title":"Kubebuilder: Extending the k8s API to match applications' needs","type":"blog"},{"authors":null,"categories":null,"content":"As the Internet of Things (IoT) continues to expand, managing a large number of heterogeneous devices with diverse requirements and characteristics has become essential. The Cloud computing ecosystem has provided a vast amount of tools to efficiently manage workload deployments. However, there is a huge gap between the requirements of Cloud infrastructure as opposed to Edge or IoT infrastructure. One crucial difference is the diversity and resource constraints of IoT devices. As billions of devices become part of our environment, the need for secure, robust device management and seamless integration into higher-level orchestration frameworks becomes increasingly critical.\nKubernetes (K8s) has emerged as the standard for container orchestration but dealing with resource-constrained edge devices poses significant challenges. This blog post explores the challenges and proposed solutions for managing IoT device firmware in the context of cloud-native orchestration.\nChallenges in IoT Device Management Managing IoT devices presents unique challenges due to the diversity of IoT ecosystems, the scale of deployments, and the necessity for secure and efficient operations. These challenges are categorized into three main areas:\nFirmware Management: Ensuring that all IoT devices are running the latest firmware versions securely and efficiently. Orchestration: Integrating IoT devices into existing orchestration frameworks without overburdening their limited resources. Security and On-boarding: Securely on-boarding new devices and managing the security of existing devices throughout their lifecycle . The Cloud-native Ecosystem The cloud-native concept has revolutionized the design, development, deployment, and management of applications by leveraging cloud computing principles such as scalability, resilience, automation, and agility.\nContainerization and micro-services are essential to this approach, providing flexibility, efficient resource utilization, and consistency across diverse environments.\nHowever, deploying containers on IoT devices is not entirely feasible, due to their limited processing power, memory, and storage. To address these constraints, the community has proposed alternative solutions, including cloud-native management frameworks running as micro-services on edge devices close to the IoT infrastructure. These edge devices act as proxies or gateways, facilitating communication with the IoT devices.\nIoT Device Management Frameworks Two popular cloud-native management frameworks for Edge/IoT devices are KubeEdge and Akri. These frameworks integrate with the cloud-native ecosystem, offering various benefits and facing certain limitations:\nKubeEdge: Extends Kubernetes to the edge, providing infrastructure support for edge computing applications. It enables centralized management of edge nodes and devices, offering features like device management, data synchronization, and edge application deployment. Akri: Focuses on edge device discovery and management, simplifying the integration of IoT devices into Kubernetes clusters. Akri automatically detects and registers IoT devices, making them available to applications running in the cluster. None of the above frameworks, however, provide a pure cloud-native approach to firmware updating. Specifically, both use custom containers that can fetch the firmware from specific locations available either locally or publicly, and use custom tools to flash the firmware directly to the device. This means that none of these frameworks take advantage of the unique characteristics of the OCI spec to leverage software delivery benefits seamlessly.\nEnhancements to Akri Earlier this year, we shared our take on how to tackle these challenges in a research paper presented at the MECC workshop in EuroSys 2024. In this work, we introduce enhancements to the Akri framework to reduce resource utilization on edge gateways, moving towards a fully unified infrastructure management solution based on cloud-native concepts. These enhancements aim to simplify IoT device firmware management and improve the efficiency of IoT device firmware upgrades across the Cloud-Edge-IoT continuum.\nFigure 1: Stock Akri workflow Essentially, Akri is a fully-featured, modular framework to manage IoT device applications. When it comes to firmware flashing (e.g. OTA updates), Akri relies on the user to provide a custom container image that is responsible for fetching the firmware, communicating with the device and eventually flashing the firmware on the device. Our approach is to leverage Akri for the device identification and mapping, while at the same time add the functionality to define which devices we would like to upgrade / re-purpose. A proof of concept has been implemented and the code changes are minimal.\nIn essence, we add an extra type of job in the Akri Controller logic. Initially, the Akri Configuration must include additional values such as the FirmwareJobSpec, responsible for managing the …","date":1723127326,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1723127326,"objectID":"900df35bb169100490b899d9a0eeb127","permalink":"/blog/cloud-native-iot/","publishdate":"2024-08-08T14:28:46Z","relpermalink":"/blog/cloud-native-iot/","section":"blog","summary":"As the Internet of Things (IoT) continues to expand, managing a large number of heterogeneous devices with diverse requirements and characteristics has become essential. The Cloud computing ecosystem has provided a vast amount of tools to efficiently manage workload deployments. However, there is a huge gap between the requirements of Cloud infrastructure as opposed to Edge or IoT infrastructure. One crucial difference is the diversity and resource constraints of IoT devices. As billions of devices become part of our environment, the need for secure, robust device management and seamless integration into higher-level orchestration frameworks becomes increasingly critical.\n","tags":["Cloud-native","Kubernetes","IoT","Akri"],"title":"Cloud-native IoT deployments with Akri","type":"blog"},{"authors":[""],"categories":null,"content":" Anastassios Nanos delivered an insightful presentation at OpenInfra Days Hungary, in Budapest, earlier this June, highlighting the innovative capabilities of the Kata Containers project. The talk focused on the project’s advancements, particularly the introduction and maturity of the Rust runtimes, alongside the go version of the runtime, and demonstrated how the Kata team is leveraging these technologies to enhance the performance and security of serverless workloads. As a first time Architecture Committee member since April 2024, Dr Nanos presented the current status of the project, and how the team at Nubificus \u0026amp; Nubis is using kata-containers to sandbox serverless workloads, while exposing hardware acceleration capabilities by integrating vAccel to the container runtime.\nKey Highlights of the Presentation Kata Containers Project Overview: Dr Nanos began by providing an overview of the Kata Containers project, an open-source initiative that integrates lightweight virtual machines (VMs) to enhance the security and isolation of containerized applications. He explained how Kata Containers combine the speed and flexibility of containers with the robust security features of VMs, making them an ideal solution for modern cloud-native applications.\nGo and Rust Runtimes: A significant portion of the presentation was dedicated to the dual support for Go and Rust runtimes within Kata Containers. Dr Nanos detailed the journey from the initial Go runtime to the introduction of the Rust runtime, emphasizing the performance and safety benefits of Rust. He discussed the efforts to achieve feature parity between the two runtimes, enabling users to choose the runtime that best suits their needs without sacrificing functionality or performance.\nSandboxing Serverless Workloads: Dr Nanos illustrated how Kata Containers are being used to sandbox serverless workloads, a growing trend in cloud computing. He explained that by using Kata Containers, the team can provide strong isolation for serverless functions, ensuring that workloads are securely sandboxed and isolated from one another. This approach not only enhances security but also improves the reliability and stability of serverless environments.\nvAccel Integration: One of the standout points of the talk was the integration of vAccel with Kata Containers. Dr Nanos explained how vAccel allows the exposure of hardware acceleration functionalities to sandboxed containers, significantly boosting the performance of compute-intensive tasks. This integration enables developers to take full advantage of hardware acceleration within a secure and isolated environment, paving the way for more efficient and powerful applications.\nLooking Ahead: Dr Nanos concluded his presentation with a look ahead to the future of Kata Containers, including the anticipated release of Kata Containers 4.0. He highlighted ongoing efforts to further unify the Go and Rust runtimes and to expand the project’s capabilities to meet the evolving needs of the cloud-native ecosystem.\nThe presentation at OpenInfra days Hungary underscored the Kata Containers project’s commitment to innovation and security in containerized environments. With its robust feature set and continued development, Kata Containers is poised to play a crucial role in the future of cloud-native infrastructure.\nFor more information about the Kata Containers project, visit katacontainers.io. All demos presented are available online at github.\nMany thanks to the OpenInfra Foundation for hosting Dr Nanos, as well as to DESIRE6G and MLSysOps for supporting the development of most of the work presented.\n","date":1720569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720569600,"objectID":"d2468093181f416acc2b42221c3906fa","permalink":"/post/2024-07-10-kata-containers-sandboxing-container-workloads-for-security-and-efficiency/","publishdate":"2024-07-10T00:00:00Z","relpermalink":"/post/2024-07-10-kata-containers-sandboxing-container-workloads-for-security-and-efficiency/","section":"post","summary":" Anastassios Nanos delivered an insightful presentation at OpenInfra Days Hungary, in Budapest, earlier this June, highlighting the innovative capabilities of the Kata Containers project. The talk focused on the project’s advancements, particularly the introduction and maturity of the Rust runtimes, alongside the go version of the runtime, and demonstrated how the Kata team is leveraging these technologies to enhance the performance and security of serverless workloads. As a first time Architecture Committee member since April 2024, Dr Nanos presented the current status of the project, and how the team at Nubificus \u0026 Nubis is using kata-containers to sandbox serverless workloads, while exposing hardware acceleration capabilities by integrating vAccel to the container runtime.\n","tags":["news"],"title":"Kata-containers: sandboxing container workloads for security and efficiency","type":"post"},{"authors":[""],"categories":null,"content":" Nubificus LTD proudly presents the latest release of the vAccel framework. vAccel v0.6.0 is available, as well as a hot fix followup for x86_64, aarch64, and armv7l architectures, with the essential binaries for users to get started.\nA major addition to our previous release is Torch support. Users are now able to run inference on Torch models seamlessly, on local and remote targets (CPU/GPU etc.). A short screen cast is available to check torch with vAccel in action!\nvAccel v0.6 includes updated helper functions for easier argument definition (exec), as well as enhanced CI and testing support.\nA fun addition was native Golang bindings! Users can now natively interact with Go programs and enjoy hardware acceleration from simple web services! Golang bindings helped a lot with our Knative integration!\nvAccel v0.6 offers updated API remoting functionality over generic sockets, supporting AF_VSOCK and AF_INET, enabling local and remote execution over the network. AF_VSOCK support is also updated with a streaming optimization, to reduce the amount of memory allocated by the gRPC transport layer.\nThis iteration’s update also contains important bug fixes and performance optimizations. For the list of changes, see the RELEASE notes.\nThe individual components are packaged as binary artifacts or deb packages for users to install them directly. For a list of the binary components please visit vAccel’s documentation page. The core vAccel library is open-source, available on github.\nThe roadmap for v0.7 contains enhanced Torch support, OpenCV-native bindings, a C++-based transport layer, the move to the meson build system, and more!\nvAccel vAccel enables workloads to enjoy hardware acceleration while running on environments that do not have direct (physical) access to acceleration devices. With a slim design and precise abstractions, vAccel semantically exposes hardware acceleration features to users with little to no knowledge of software acceleration framework internals.\nvAccel integrates with container runtimes such as kata-containers. v0.6 brings updated support for kata-containers v3.X, both for the Go and rust runtimes, so deploying an application that requires hardware acceleration in a sandboxed container in k8s is now possible without complicated hardware setups!\nServerless computing workflows are now able to enjoy compute-offload mechanisms to provide AI/ML services as functions, triggered thousands or millions of times by events, on-demand, auto-scaling to multiple physical and virtual nodes, without the need to directly attach a hardware device to the instance. This functionality is enabled through vAccel’s virtualization backends, enabling the execution of hardware-accelerated functions on Virtual Machines. Support for numerous hypervisors is available, including AWS Firecracker, QEMU/KVM, Cloud Hypervisor, and Dragonball, the stock hypervisor of kata-containers! See the relevant documentation on how to run an example on a VM.\nEnd-users can get a sneak peek at what vAccel has to offer on the project’s website, on github, or by browsing through the available documentation. To get started, follow the Quick start guide.\n","date":1719705600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1719705600,"objectID":"ab347760edfd10727515717b9ad79fd6","permalink":"/post/2024-06-30-vaccel-v06-is-out/","publishdate":"2024-06-30T00:00:00Z","relpermalink":"/post/2024-06-30-vaccel-v06-is-out/","section":"post","summary":" Nubificus LTD proudly presents the latest release of the vAccel framework. vAccel v0.6.0 is available, as well as a hot fix followup for x86_64, aarch64, and armv7l architectures, with the essential binaries for users to get started.\n","tags":["news"],"title":"vAccel v0.6 is out!","type":"post"},{"authors":null,"categories":null,"content":" Have you ever found yourselves scratching your heads over perplexing connectivity issues? If you’re anything like us, you’ve probably encountered your fair share of head-scratching moments. In this short post, we would like to share one such experience we recently tackled while trying to connect our ESP32 devices to our office Wi-Fi Access Point (AP).\nThe challenge emerged when we needed to integrate ESP32 devices into our existing infrastructure, a set of x86_64 and aarch64 nodes, interconnected into a number of clusters where we experiment and deploy the systems software we build. The catch? The data room lacked Wi-Fi coverage, except for the AP of the upstream modems, leaving our IoT devices isolated from the clusters’ VLANs.\nTo address this, we had two options: either invest in a new AP, dedicated to the IoT devices in the data room or use one of the cluster nodes to act as an AP, bridging their ethernet connectivity with their Wi-Fi interfaces. Opting for the latter, we started working on setting up the AP on a RPi device.\nBy default, Raspbian uses Network Manager to handle all network connections. Using nmtui on stock Raspbian, we set up a bridge between eth0 and wlan0, configure the SSID, and set the WPA-PSK password for the AP. After playing around with nmtui, we figured that it might be easier to setup the AP using the command line tool, nmcli. A snippet like the following would do the trick:\nnmcli c add type wifi ifname wlan0 con-name WIFI_AP autoconnect yes ssid nbfc-iot nmcli con modify WIFI_AP 802-11-wireless.mode ap 802-11-wireless.band bg ipv4.method shared nmcli con modify WIFI_AP wifi-sec.key-mgmt wpa-psk nmcli con modify WIFI_AP wifi-sec.psk \u0026#34;PASSWORD\u0026#34; nmcli con up WIFI_AP Initial tests with a mobile phone and a laptop connecting to the AP were successful, but when it came to connecting the ESP32 devices, we hit a roadblock.\nDespite Wi-Fi scans showing the correct SSIDs, the ESP32 devices stubbornly refused to connect. We tried everything: from tinkering with frequency bands (5GHz/2.4GHz) to flashing one of the devices with micropython and experimenting with network and WLAN connectivity settings – all to no avail.\nWhile debugging the issue, we listed available Wi-Fi networks from a neighboring RPi node, and from the ESP32 device that we flashed with micropython. The output from the RPi device and the ESP32 device is shown below:\n$ nmcli device wifi list BSSID SSID MODE CHAN RATE SIGNAL BARS SECURITY D8:3A:DD:AF:D7:D3 nbfc-iot Infra 3 65 Mbit/s 100 ▂▄▆█ WPA1 5C:64:8E:41:AB:32 NBFC-AP Infra 100 270 Mbit/s 69 ▂▄▆_ WPA2 \u0026gt;\u0026gt;\u0026gt; import network \u0026gt;\u0026gt;\u0026gt; sta_if = network.WLAN(network.STA_IF) \u0026gt;\u0026gt;\u0026gt; sta_if.active(True) True \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; authmodes = (\u0026#39;Open\u0026#39;, \u0026#39;WEP\u0026#39;, \u0026#39;WPA-PSK\u0026#39;, \u0026#39;WPA/WPA2-PSK\u0026#39;) \u0026gt;\u0026gt;\u0026gt; for (ssid, bssid, channel, RSSI, authmode, hidden) in sta_if.scan(): ... print(\u0026#34;* {:s}\u0026#34;.format(ssid)) ... print(\u0026#34; - Auth: {}\u0026#34;.format(authmodes[authmode])) ... print(\u0026#34; - Channel: {}\u0026#34;.format(channel)) ... print(\u0026#34; - RSSI: {}\u0026#34;.format(RSSI)) ... print(\u0026#34; - BSSID: {:02x}:{:02x}:{:02x}:{:02x}:{:02x}:{:02x}\u0026#34;.format(*bssid)) ... print() * nbfc-iot - Auth: WPA-PSK - Channel: 3 - RSSI: -31 - BSSID: d8:3a:dd:af:d7:d3 * NBFC-AP - Auth: WPA/WPA2-PSK - Channel: 1 - RSSI: -69 - BSSID: 5c:64:8e:41:ab:32 By examining the output, we finally pinpointed the culprit: the ESP32 devices only support WPA2, whereas our RPi AP defaulted to plain WPA. The solution? Modify the Wi-Fi security protocol setting on the RPis to use WPA2. Finding a way to do this was much more complicated than it should. Luckily, we were able to find our way around nmcli and setup RSN (or WPA2) using the nmcli command:\nnmcli con modify WIFI_AP 802-11-wireless-security.proto rsn Where WIFI_AP is the names of the connection. A command that proved particularly useful was the following:\n$ nmcli con show WIFI_AP [snipped] 802-11-wireless.ssid: nbfc-iot 802-11-wireless.mode: ap 802-11-wireless.band: bg 802-11-wireless.channel: 3 802-11-wireless.bssid: -- 802-11-wireless.rate: 0 802-11-wireless.tx-power: 0 802-11-wireless.mac-address: -- 802-11-wireless.cloned-mac-address: -- 802-11-wireless.generate-mac-address-mask:-- 802-11-wireless.mac-address-blacklist: -- 802-11-wireless.mac-address-randomization:default 802-11-wireless.mtu: auto 802-11-wireless.seen-bssids: D8:3A:DD:AF:D7:D3 802-11-wireless.hidden: no 802-11-wireless.powersave: 0 (default) 802-11-wireless.wake-on-wlan: 0x1 (default) 802-11-wireless.ap-isolation: -1 (default) 802-11-wireless-security.key-mgmt: wpa-psk 802-11-wireless-security.wep-tx-keyidx: 0 802-11-wireless-security.auth-alg: -- giving us all the configuration parameters for nmcli and this particular connection.\nWith this seemingly small adjustment, the ESP32 devices connected to our office networks, and our cloud-native exploration for IoT devices could proceed without further hiccup.\nReflecting on this experience, we’re reminded of the importance of thorough testing and persistence when dealing with diverse hardware and complex network setups. Sometimes, …","date":1715157064,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715157064,"objectID":"5a4d9f89fc9b1355fed822f41ea0ff32","permalink":"/blog/esp32_wifi/","publishdate":"2024-05-08T09:31:04+01:00","relpermalink":"/blog/esp32_wifi/","section":"blog","summary":" Have you ever found yourselves scratching your heads over perplexing connectivity issues? If you’re anything like us, you’ve probably encountered your fair share of head-scratching moments. In this short post, we would like to share one such experience we recently tackled while trying to connect our ESP32 devices to our office Wi-Fi Access Point (AP).\n","tags":["ESP32","`WPA`","WiFi","IoT"],"title":"Navigating ESP32 wireless connectivity issues","type":"blog"},{"authors":["Charalampos Mainas","Anastassios Nanos"],"categories":null,"content":"Code \u0026amp; Resources urunc - Container Runtime Repository ","date":1711112400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711112400,"objectID":"c95b9ac2a77ed49d73f417aedc7627a5","permalink":"/event/kubeconeu2024/","publishdate":"2024-03-22T00:00:00Z","relpermalink":"/event/kubeconeu2024/","section":"event","summary":"Unikernels in K8s - Performance and Isolation for Serverless Computing with Knative","tags":["kubernetes","unikernels","knative","serverless","performance","isolation","urunc"],"title":"KubeCon + CloudNativeCon Europe 2024: Unikernels in K8s - Performance and Isolation for Serverless Computing with Knative","type":"event"},{"authors":[""],"categories":null,"content":" FOSDEM 2024 was a blast! Our team was there presenting our work around unikernels, and their interaction with the cloud-native world.\nThe first presentation took place in the Containers devroom, where we described the challenges of securing containers within VM or microVM sandboxes and the complexities of resource optimization on physical nodes. We presented urunc, a CRI-compatible container runtime spawning unikernels packaged in OCI images. The talk highlighted urunc’s internals, covering hypervisor support, network and storage handling, and integration with high-level orchestration frameworks like Kubernetes, while also addressing network setup implications when combining unikernels and generic containers in a k8s context.\nIn the Microkernel devroom, we continued our exploration of unikernels with a focus on modularity. The team presented bunny, a novel build system aimed at simplifying the Unikernel building process. bunny adopts a layered approach, making use of the modular aspect of Unikernels. Each component represents a distinct layer, eliminating the need for users to manually build dependencies. This approach allows for the creation of minimal and specialized Unikernel images tailored for individual applications.\nThese presentations highlight our commitment to simplify and optimize application delivery in diverse, heterogeneous infrastructure, providing valuable insights into security and system optimization in the evolving landscape of container technology.\nFind below the slides and video recordings of the presentations, along with links to the devrooms full schedule.\nA Modular Approach to Effortless and Dependency-Aware Unikernel Building, Microkernel and Component-based OS devroom session slides video devroom From Containers to Unikernels: Navigating Integration Challenges in Cloud-Native Environments, Containers devroom session slides video devroom FOSDEM\nFOSDEM stands for “Free and Open Source Software Developers’ European Meeting.” It is one of the largest gatherings of open-source developers and enthusiasts in the world. FOSDEM is an annual event that takes place in ULB Solbosch Campus, Brussels, Belgium, typically in late January or early February.\nThe conference provides a platform for developers, contributors, and users of free and open-source software to come together, share knowledge, collaborate on projects, and discuss the latest developments in the open-source community. FOSDEM is known for its diverse range of talks, developer rooms, and stands, covering a wide array of topics related to open-source software, including programming languages, operating systems, security, networking, and more.\nFOSDEM is a free event, organized by volunteers, and it attracts participants from around the globe. The conference aims to foster collaboration and promote the principles of free and open-source software within the technology community.\nAlthough statistics are not out yet, this year, getting into the talks was harder than ever. Rooms were filling up even 30’ before the scheduled talks. As an indication, almost 855 events are recorded and are being transcoded to be available for online viewing. Our estimation is that there were almost 10000 people in ULB.\n","date":1707264e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707264e3,"objectID":"e90df2aa8c22ba8904c51c160c5741a0","permalink":"/post/2024-02-07-fosdem-2024/","publishdate":"2024-02-07T00:00:00Z","relpermalink":"/post/2024-02-07-fosdem-2024/","section":"post","summary":" FOSDEM 2024 was a blast! Our team was there presenting our work around unikernels, and their interaction with the cloud-native world.\n","tags":["news"],"title":"FOSDEM 2024","type":"post"},{"authors":null,"categories":null,"content":"As per a well-known and established definition:\n“Continuous Integration (CI) is the practice of merging all developers’ working copies to a shared mainline several times a day.”\nFigure 1: What is continuous integration ? (source: exoscale) Continuous Integration practice comes along with the automated testing and verification of the integrated code changes, giving testing automations the status as one of the pivotal elements in software development. The automation of testing can range from a few simple scripts, to complex setups, employing an entire testing suite. However, challenges escalate significantly when deploying software across diverse architectures (such as amd64, ARM, etc.) and varied environments (including containers, Kubernetes, bare-metal, etc.). The emergence of Kubernetes and its extensive software ecosystem, coupled with the robust tooling provided by Github, has introduced an intriguing perspective on automating the building and testing phases of software development.\nAs a company specializing in systems software, we’ve had to innovate to ensure that our code not only avoids regressions but also remains compatible with multiple architectures, facilitating deployment in a range of environments. This talk provides insights into our findings, outlining the use of an open-source Kubernetes operator built to scale self-hosted Github runners. We share the intricacies of all customisations necessary to align this solution with our specific requirements.\nSpecifically, we go through a brief introduction of the operator, and dig in the customization of the runners, built to support various combinations of system-specific requirements: interact with VM middlewares (libvirt), enable hardware accelerator access (GPUs), setup a standalone k8s environment, and finally, automate the building of the runners themselves.\nTo wrap up, we showcase a workflow that we use to release vAccel, a software framework we build, that consists of various individual components, that eventually interact with each other.\nGithub Actions Github Actions is the Software Development Life Cycle (SDLC) automation platform introduced by Github. With Github Actions framework, one can create automation workflows as part of the actual software code already stored in Github.\nGithub workflows are represented in YAML syntax and it are executed on configured runners.\nFigure 2: Github Actions The basic elements of Github Actions framework are:\nGithub Actions element Description Event Triggers starting signals for a Github action to start: operation from a Github repo, manual schedule, external event Workflow workflow is a script consisting of jobs, steps, calling actions or other workflows forming a pipeline Job jobs is an execution block consisting of steps that run on a specific runner Step atomic script code blocks of jobs Action script module consisting of steps and invoked by workflow pipelines Environment building \u0026amp; deployment targets of github workflow pipelines Runner VMs, containers or pods where workflow code is executed Github Marketplace is another feature of Github Actions: a public place in Github where a vast number of Github Actions are published and available to use.\nAs a systems software company, our deliverables have different software forms: from simple libraries, to executable binaries and whole containerised images. To accommodate the various building, deployment and testing necessities, we have been implementing several Github workflow pipelines with several customised features:\nmulti-arch build \u0026amp; unit-test of vAccel components, in self-hosted custom built runners multi-arch build \u0026amp; test of vAccel, in self-hosted custom built runners SDLC supporting pipelines for automating repeating tasks To fulfill the various requirements of system-specific combinations, the following Github workflow components were used:\nbare metal k8s cluster evryfs k8s operator custom built runner images Github option for self-hosted runners The following sections of this post describe these custom made components in details.\nSelf-hosted Runners Generally speaking, self hosted github runners provide extensive control of hardware, operating system containers with hardware-specific components. We take advantage of these components’ features to build, execute and maintain a Continuous Integration/Continuous Deployment set of pipelines for the Nubificus multiple hardware architectures solution.\nLet us list some of these Self-hosted runners advantages:\nlaunching pods in architectures not currently supported by Github hosted runners images with hardware specific customisations management \u0026amp; maintenance by hosting organization implementation in both virtual or physical infrastructure maximize volumes \u0026amp; concurrent jobs cost efficiency persistence, not being ephemeral as Github hosted runners To get the maximum out of the Github self-hosted runners capabilities and at the same time enforce the restriction of a multi-arch pipeline ecosystem, we opted to …","date":1707121864,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707121864,"objectID":"59dd9c6e07f2ba4e44f798d21905163d","permalink":"/blog/gar_k8s/","publishdate":"2024-02-05T09:31:04+01:00","relpermalink":"/blog/gar_k8s/","section":"blog","summary":"As per a well-known and established definition:\n","tags":null,"title":"Build Multi-arch CI pipelines with Github Actions Self-hosted Runners","type":"blog"},{"authors":[],"categories":null,"content":"","date":1706950800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706950800,"objectID":"fdf924f85f98794452d7fc843823f6a0","permalink":"/event/fosdem2024/","publishdate":"2024-02-04T00:00:00Z","relpermalink":"/event/fosdem2024/","section":"event","summary":"FOSDEM is a free event for software developers to meet, share ideas and collaborate.","tags":[],"title":"FOSDEM 2024 - From Containers to Unikernels: Navigating Integration Challenges in Cloud-Native Environments","type":"event"},{"authors":null,"categories":null,"content":"Testing is an important part of software development, yet is often neglected in many projects. We felt our code base for vAccel had reached a stage where the foundations for a proper testing framework would be a good decision.\nIntroducing a testing framework early on would allow us to identify and fix bugs and errors in our code faster. It would also allow us to implement tests easily and faster. In this post, we will look at the initial setup we used for testing and the testing frameworks and methods we considered before finally ending up with our framework of choice, Catch2.\nInitially, we used Google Test, which is a testing framework written in C++ and is a very popular testing framework. In fact, it was the most popular according to this survey by JetBrains.\nSurprisingly (or unsurprisingly), a large majority in this survey don’t test at all - fortunately, we will not follow this approach. Google Test is a really good testing framework and is full of many features which can be useful in complex projects. However, for vAccel, it is a bit overkill and quite hard to write tests for as we will see later.\nThis forms part of the desire of wanting to switch to a different testing framework - if the framework allows tests to be written in a faster and efficient manner, it will encourage us to write tests in the first place - which is a good thing!\nInitial Google Test setup Let’s dive into the very rudimentary test we had, written using Google Test.\nclass PluginTests : public ::testing::Test { static int fini(void) { return VACCEL_OK; } static int init(void) { return VACCEL_OK; } protected: /* initialize structures with zeros */ struct vaccel_plugin plugin = {0}; struct vaccel_plugin_info pinfo = {0}; void SetUp() override { plugin.info = \u0026amp;this-\u0026gt;pinfo; plugin.info-\u0026gt;name = pname; list_init_entry(\u0026amp;plugin.entry); list_init_entry(\u0026amp;plugin.ops); plugin.info-\u0026gt;init = init; plugin.info-\u0026gt;fini = fini; plugins_bootstrap(); } void TearDown() override { plugins_shutdown(); } }; Here we provide definitions for SetUp() and TearDown() in the fixture PluginTests. We can then use this fixture in the following way:\nTEST_F(PluginTests, plugin_init_null) { ASSERT_EQ(register_plugin(NULL), VACCEL_EINVAL); } In this basic test, SetUp() is ran before asserting our function register_plugin(NULL) is equal to VACCEL_EINVAL. To wrap up our test, TearDown() is executed last and that’s our basic test done!\nThis seems rather straightforward, and it is! However, this is only one test and if we want to use other fixtures in our test files, it gets convoluted very quickly and the readability of the tests goes downhill very fast. Of course, this is fine and we could have carried on using Google Test as our framework and be done with it but since we are at an early stage it is relatively easy to change our approach and see if anything else fits better for vAccel. A key thing with GTest was that it is possible to include in the CMake process which is useful for our build process.\nUnity? One of the frameworks we looked at was Unity - which is a framework written in C. Unlike Google Test, to include Unity in our testing directory, we just #include \u0026#34;unity.h\u0026#34; in our testing file and that’s it!\nTests in unity are straightforward:\nAgain, we have a setUp() and tearDown():\nvoid setUp(void) { plugin.info = \u0026amp;pinfo; pinfo.name = pname; list_init_entry(\u0026amp;plugin.entry); list_init_entry(\u0026amp;plugin.ops); pinfo.init = init; pinfo.fini = fini; plugins_bootstrap(); } void tearDown(void) { plugins_shutdown(); } Our basic test would be as follows:\nvoid test_plugin_null(void) { TEST_ASSERT_EQUAL(register_plugin(NULL), VACCEL_EINVAL); } Like GTest, we have a setUp and TearDown function and for our assertion macro we use TEST_ASSERT_EQUAL. From a readability standpoint, this is easier to read than GTest. The main issue we found with Unity is that having different fixtures in the same file is difficult and in many cases, it’s just easier to create a new file instead. This is rather tedious and having several files for one source file we are testing is not ideal so we now instead cast our eyes at Catch2.\nFinally, Catch2 Catch2, just like Unity, uses a single header file. We just drag and drop the header in our testing directory - which is very convenient! Let’s start again with our basic test written in Catch2 this time.\nTEST_CASE(\u0026#34;plugin_register\u0026#34;) { int ret; struct vaccel_plugin plugin; struct vaccel_plugin_info pinfo; plugin.info = \u0026amp;pinfo; plugin.info-\u0026gt;name = pname; list_init_entry(\u0026amp;plugin.entry); list_init_entry(\u0026amp;plugin.ops); plugin.info-\u0026gt;init = init; plugin.info-\u0026gt;fini = fini; ret = plugins_bootstrap(); REQUIRE(ret == VACCEL_OK); ret = register_plugin(\u0026amp;plugin); REQUIRE(ret == VACCEL_OK); ret = plugins_shutdown(); REQUIRE(ret == VACCEL_OK); } As before, this is quite readable but Catch2 does not encourage the use of traditional fixtures, unlike the other two frameworks we have looked at. Catch2 uses an alternative syntax (as well as traditional fixtures) using BDD-style test …","date":1704890500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704890500,"objectID":"ba0874d7ed0f91cb506672d6c1f9a1f8","permalink":"/blog/testing_post/","publishdate":"2024-01-10T14:41:40+02:00","relpermalink":"/blog/testing_post/","section":"blog","summary":"Testing is an important part of software development, yet is often neglected in many projects. We felt our code base for vAccel had reached a stage where the foundations for a proper testing framework would be a good decision.\n","tags":["Testing","CI","GitHub Actions"],"title":"Updating our testing framework using Catch2","type":"blog"},{"authors":null,"categories":null,"content":"To facilitate the use of vAccel, we provide bindings for popular languages, apart from C. Essentially, the vAccel C API can be called from any language that interacts with C libraries. Building on this, we are thrilled to present support for Go.\nEssentially, the C/Go interaction is already pretty smooth, given the native CGO support available. We introduce v0.1 of the vAccel-go bindings, pending a feature-full update in the coming months. In this post, we go through the initial implementation details, as well as a hands-on tutorial on how to write your first vAccel program in Go!\nvAccel overview vAccel is a library for hardware acceleration. Actually, it is a set of software tools that semantically expose hardware acceleration functionality to isolated workloads running on VM sandboxes. For more insights or examples about vAccel, there is a splash page, along with more elaborate documentation.\nGolang overview The Go Programming Language is designed for scalability, making it suitable for cloud computing and large-scale servers. Go enhances development speed and efficiency, since it compiles quickly (compared to other languages), and provides a great Standard Library, along with built-in concurrency tools. Golang is also a cloud-native Programming Language. Information about Go installation can be found here, but there are also instructions on how to install Go in the vAccel-go bindings installation guide.\nvAccel Go package The vaccel package in Golang provides access to vAccel operations, which can be used by the developers on their own Go programs. The vaccel package uses the native C bindings in order to use the vAccel C API. The following diagram demonstrates the functionality of the vaccel package:\nFigure 1: High-level overview of the vAccel Go package Installation Guide vAccel Installation First of all, a vAccelRT installation is required before proceeding to the next sections.\nBuild from source In Ubuntu-based systems, you need to have the following packages to build vaccelrt:\ncmake build-essential You can install them using the following command:\nsudo apt-get install -y cmake build-essential Get the source code for vaccelrt:\ngit clone https://github.com/cloudkernels/vaccelrt --recursive Prepare the build directory:\ncd vaccelrt mkdir build cd build Building the core runtime library # This sets the installation path to /usr/local, and the current build # type to \u0026#39;Release\u0026#39;. The other option is the \u0026#39;Debug\u0026#39; build cmake ../ -DCMAKE_INSTALL_PREFIX=/usr/local -DCMAKE_BUILD_TYPE=Release -DBUILD_EXAMPLES=ON -DBUILD_PLUGIN_EXEC=ON -DBUILD_PLUGIN_NOOP=ON make sudo make install vAccel-Go Bindings Installation Go Installation Of course, prior to installing the bindings, we have to make sure that Golang 1.20 or newer is installed in our system. We can check this using the following command:\ngo version Otherwise, go 1.20 needs to be installed. You can find instructions on how to install Go here.\nBuild the Bindings from source Download the source code:\ngit clone https://github.com/nubificus/go-vaccel.git First, you can build the examples:\n# Set vaccel location export PKG_CONFIG_PATH=/usr/local/share/ cd go-vaccel make all Now you have successfully built some vaccel programs using Go. The executables are located in go-vaccel/bin. You can run the noop example:\nexport VACCEL_BACKENDS=/usr/local/lib/libvaccel-noop.so ./bin/noop Or the exec example, providing a path for the shared object and an integer:\nexport VACCEL_BACKENDS=/usr/local/lib/libvaccel-exec.so ./bin/exec /usr/local/lib/libmytestlib.so 100 # if everything go as expected, the # plugin will probably double the integer Tutorial The following example demonstrates the usage of the vaccel package to build vaccel-enabled Go programs. The tutorial will perform an image classification operation, using the no-op plugin. Keep in mind the following three conditions before building:\n1. Make sure to import the package in your programs:\nimport \u0026#34;github.com/nubificus/go-vaccel/vaccel\u0026#34; 2. Define vaccel location:\nexport PKG_CONFIG_PATH=/usr/local/share 3. And finally, always define the location of the vaccel-plugin you are willing to use:\n# In case of No-Op for testing: export VACCEL_BACKENDS=/usr/local/lib/libvaccel-noop.so Example Create the project directory\ncd ~ mkdir go-vaccel-test cd go-vaccel-test Initialize the Module\ngo mod init go-vaccel-test Download the bindings\ngo get github.com/nubificus/go-vaccel And create a Go file\ntouch main.go Add the following lines to perform Image Classification\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/nubificus/go-vaccel/vaccel\u0026#34; ) func main() { if len(os.Args) \u0026lt; 2 { fmt.Println(\u0026#34;Usage: ./main \u0026lt;filename\u0026gt;\u0026#34;) return } /* Get the filename from command line argument */ filePath := os.Args[1] /* Session */ var session vaccel.Session err := vaccel.SessionInit(\u0026amp;session, 0) if err != 0 { fmt.Println(\u0026#34;error initializing session\u0026#34;) os.Exit(err) } var outText string /* Read the image-bytes */ imageBytes, e := os.ReadFile(filePath) if e != nil { …","date":1704205726,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704205726,"objectID":"1239990ea439853362b402ad7ec8c099","permalink":"/blog/vaccel_go/","publishdate":"2024-01-02T14:28:46Z","relpermalink":"/blog/vaccel_go/","section":"blog","summary":"To facilitate the use of vAccel, we provide bindings for popular languages, apart from C. Essentially, the vAccel C API can be called from any language that interacts with C libraries. Building on this, we are thrilled to present support for Go.\n","tags":["vAccel","go"],"title":"vAccel-go: Golang bindings for vAccel","type":"blog"},{"authors":["Aristotelis Kretsis","Panagiotis Kokkinos","Emmanouel Varvarigos","Dimitris Syrivelis","Paraskevas Bakopoulos","Marton Sipos","Marcel Fehér","Daniel Lucani","Jose Manuel Bernabe","Antonio Skarmeta"," others"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"3967b9661c8b516eba1fc880442f29f2","permalink":"/publication/kretsis-2024-empyrean/","publishdate":"2025-10-15T23:59:22.792151Z","relpermalink":"/publication/kretsis-2024-empyrean/","section":"publication","summary":"","tags":null,"title":"EMPYREAN: Trustworthy, Cognitive and AI-driven Collaborative Associations of IoT Devices and Edge Resources for Data Processing","type":"publication"},{"authors":["Anastassios Nanos","Ioannis Plakas","Georgios Ntoutsos","Charalampos Mainas"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"efc0ba556cb932e9e0ca10ada895c1a7","permalink":"/publication/nanos-2024-enabling/","publishdate":"2025-10-15T23:59:22.786179Z","relpermalink":"/publication/nanos-2024-enabling/","section":"publication","summary":"","tags":null,"title":"Enabling Cloud-native IoT Device Management","type":"publication"},{"authors":["Anastassios Nanos"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"1c992c2aeaaccdf1eb2eac9278152d91","permalink":"/publication/nanos-2024-lightweight/","publishdate":"2025-10-15T23:59:22.816896Z","relpermalink":"/publication/nanos-2024-lightweight/","section":"publication","summary":"","tags":null,"title":"Lightweight Sandboxing for Cloud-native 6G Services and Applications","type":"publication"},{"authors":["Charalampos Mainas","Ioannis Plakas","Georgios Ntoutsos","Anastassios Nanos"],"categories":null,"content":"","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"90d870b703aea3a840097b82e5940092","permalink":"/publication/mainas-2024-sandboxing/","publishdate":"2025-10-15T23:59:22.774045Z","relpermalink":"/publication/mainas-2024-sandboxing/","section":"publication","summary":"","tags":null,"title":"Sandboxing functions for efficient and secure multi-tenant serverless deployments","type":"publication"},{"authors":[""],"categories":null,"content":"The SERRANO H2020 research project, a collaborative effort led by ICCS’ High Speed Communication Networks Lab, successfully concludes, marking a significant breakthrough in various aspects of cloud- and edge-based computing technologies. The project aimed to introduce a novel ecosystem spanning application deployment software components, to systems software enabling the use of specialized hardware resources.\nOne of the key achievements of the SERRANO project is the work carried out by Nubificus LTD in enabling interoperable hardware acceleration using vAccel. Nubificus LTD played a pivotal role in developing and implementing solutions that allow seamless interoperability among diverse hardware resources, contributing to enhanced performance and efficiency in Cloud and Edge computing.\nFurthermore, Nubificus LTD has been instrumental in facilitating the deployment of applications in Cloud and Edge environments through the integration of secure and efficient container runtimes. Through the development of urunc, an open-source, lightweight container runtime that spawns unikernels as generic containers Nubificus LTD paved the path for seamless applications deployment at the Edge, maintaining security standards and efficiency across various computing environments.\nSERRANO’s ambitious goal of transforming distributed edge, cloud, and HPC resources into a single borderless infrastructure has been realized through the efforts of all partners. The project has not only closed existing technology gaps but has also set the stage for advanced infrastructures capable of meeting the stringent requirements of future applications and services.\nSERRANO\nICT-40-2020 SERRANO Project (H2020 GA No 101017168): Transparent Application Deployment in a Secure, Accelerated and and Cognitive Cloud Continuum. SERRANO investigates the transparent deployment of applications in a secure and accelerated infrastructure of edge, cloud and HPC resources, based on FPGAs, GPUs, Virtual Platforms and Smart NICs, while facilitating their automated and cognitive orchestration. SERRANO website: https://ict-serrano.eu Twitter: https://twitter.com/ProjectSerrano LinkedIn: https://www.linkedin.com/company/serrano-project Researchgate: https://www.researchgate.net/project/SERRANO-Transparent-Application-Deployment-in-a-Secure-Accelerated-and-Cognitive-Cloud-Continuum ","date":1703808e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703808e3,"objectID":"71e01f7ae2de2f8c9cacc40c413db8c8","permalink":"/post/2023-12-29-serrano-project-concludes-with-nubificus-ltd-laying-the-ground-for-interoperable-hardware-acceleration-and-secure-application-deployment/","publishdate":"2023-12-29T00:00:00Z","relpermalink":"/post/2023-12-29-serrano-project-concludes-with-nubificus-ltd-laying-the-ground-for-interoperable-hardware-acceleration-and-secure-application-deployment/","section":"post","summary":"The SERRANO H2020 research project, a collaborative effort led by ICCS’ High Speed Communication Networks Lab, successfully concludes, marking a significant breakthrough in various aspects of cloud- and edge-based computing technologies. The project aimed to introduce a novel ecosystem spanning application deployment software components, to systems software enabling the use of specialized hardware resources.\n","tags":["news"],"title":"SERRANO Project Concludes with Nubificus LTD laying the ground for Interoperable Hardware Acceleration and Secure Application Deployment","type":"post"},{"authors":[""],"categories":null,"content":"NUBIS contribution: Involved in the systems software development, to enable transparent and interoperable execution of various types of workloads as standalone enclaves (containers, microVMs, VMs and unikernels), as well as in the security hardening of devices and application attestation.\n","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701388800,"objectID":"60ad46f030db88a1ad0feca1b1c21a1e","permalink":"/projects/serrano/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/projects/serrano/","section":"projects","summary":"Intent-driven paradigm for federated edge/cloud/HPC with develop-once-deploy-everywhere approach","tags":["completed"],"title":"SERRANO","type":"projects"},{"authors":null,"categories":null,"content":" In our previous posts, we walked through the process of configuring various low-level container runtimes in Knative using the RuntimeClass feature of K8s. We detailed the setup for isolation mechanisms like gVisor, with a special focus on Kata and its associated hypervisors, including AWS Firecracker and QEMU. Additionally, we delved into the capabilities of unikernels, showcasing the power of urunc in the serverless realm.\nNow, you might be wondering: What’s the real advantage beyond ensuring the security isolation of workloads? Why choose one mechanism over another? And why even dive into this conversation?\nThis post aims to provide insights into these questions, shedding light on the considerations and factors at play in the dynamic landscape of low-level container runtimes for Knative workloads.\nOverview Despite achieving security isolation through sandboxed container runtimes like Kata-containers or gVisor, it is crucial to acknowledge that running containerized workloads in a VM, isolated from the host kernel, can introduce significant overhead to the final execution. In serverless architectures, where the cost of a function is directly tied to deployment time, this factor becomes a key consideration.\nMoreover, optimizing the use of hardware resources by activating them only when necessary contributes to a ‘greener’ cloud solution, reducing Software Carbon Intensity. Conventional practices, like booting an entire OS and unnecessary libraries, can be counterproductive in terms of software stack complexity and performance. To this end, we try to combine isolated execution of user-workloads with optimal resource utilization and performance efficiency in a serverless context.\nTo validate the above hypothesis we present an initial, high-level performance analysis of Knative using:\ngeneric container runtimes (runc) sandboxed container runtimes (kata-containers, gVisor) urunc, our own unikernel container runtime We base our measurements on our own modified version of kperf.\nOur primary focus revolved around examining function-spawning latency, response time, and scalability. To achieve this, we developed scripts utilizing kperf to specifically measure these aspects. Below, we showcase two experiments conducted using kperf.\nThe initial set of measurements pertain to “cold-start” latencies, representing the duration required for a function to spawn and respond to a request. The second set, illustrates average latencies when concurrently spawning multiple services. By presenting these findings, our aim is to provide users with critical metrics that aid in selecting the most efficient low-level container runtime, optimizing their serverless workload performance.\nNOTE: kperf development was stale for a while and Sep 15th the maintainers archived the repo, focusing on a different approach for benchmarking Knative. We plan to gather metrics related to container runtimes with this tool as well.\nArchitecture overview Figure 1: Knative Serving stock components and workflow. Figure 1 depicts a typical Knative setup on a k8s cluster. Boxes in light blue show the Knative components, while the ingress controller is assumed to be provided by the infrastructure. Since Knative function pods are essentially containers, cloud vendors refrain from serving multiple tenants on shared hardware due the limitations this technology imposes on isolating user workloads. Consequently, these vendors opt to offer dedicated bare-metal or virtualized infrastructure specifically for serverless tenants.\nA solution to the issue above, could be to sandbox the user workload (the Serverless Function) in an enclave that protects the host (and the rest of the tenants) from potentially malicious code. Figure 2 shows an alternative setup to the default, where the function pods are being spawned using sandboxed container runtimes such as kata-containers (left) and gVisor (right).\nFigure 2: Knative Serving with sandboxed container runtimes. This mechanism has merits and shortcomings:\ncons:\nthe sandbox spawn is on the critical path (slower cold boot times) the code executing in the user-container runs virtualized (no straightforward access to hardware accelerators) pros:\nthe code executing in the user-container runs virtualized: it is much harder for malicious user code to escape to the host and/or access sensitive data from the system or other co-located tenants. User workloads Another point of discussion is the type of workloads in a typical serverless setup. What do users run, or better, how complicated is the code that users run in a common serverless application?\nUnikernels have come a long way since their inception several years ago, where they showed great potential (e.g. millisecond boot times, ultra low OS memory consumption) but were difficult to use, hard to debug or limited in functionality. Today, with the advent of approaches to run unmodified Linux binaries as unikernels, or automate the building process they seem ready for prime time. …","date":1700825326,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700825326,"objectID":"58e42d9a6ac7084a0d4bc46e856c14d3","permalink":"/blog/knative-runtime-eval/","publishdate":"2023-11-24T11:28:46Z","relpermalink":"/blog/knative-runtime-eval/","section":"blog","summary":" In our previous posts, we walked through the process of configuring various low-level container runtimes in Knative using the RuntimeClass feature of K8s. We detailed the setup for isolation mechanisms like gVisor, with a special focus on Kata and its associated hypervisors, including AWS Firecracker and QEMU. Additionally, we delved into the capabilities of unikernels, showcasing the power of urunc in the serverless realm.\n","tags":["Knative","Container","Runtimes","Kubernetes","urunc","unikernels"],"title":"Optimizing Performance with Unikernels: Exploring Container Runtimes for Serverless Workloads with Knative Benchmarking","type":"blog"},{"authors":null,"categories":null,"content":" This post is about urunc, a tool that we build to treat unikernels as containers and properly introduce unikernels to the cloud-native world! Essentially, urunc is a container runtime able to spawn unikernels that reside in container images. Before digging into the gory details, let us walk through some required concepts: unikernels, containers, and container runtimes.\nWhat are unikernels Unikernels is a technology that was introduced in 2013 and has been quietly evolving for some years now. They can be seen as highly specialized, lightweight operating systems. Unlike traditional general purpose operating systems, unikernels are tailored for the singular purpose of running a specific application with efficiency, eliminating unnecessary overhead and minimizing footprint. A unikernel contains all the essential components needed for running a particular application, including the application code and the necessary portions of the operating system code. Additionally, everything that is not required for running the app is stripped out of the unikernel. That results in a self-contained, portable and minimal unit of software that can be run anywhere with virtually no overhead and significantly decreased attack surface.\nUnikernels use cases Due to their inherent design simplicity, low footprint and near-instant spawn times, unikernels seem suitable for a number of interesting use-cases:\nServerless functions: With ultra-fast boot times and efficient execution unikernels are a great fit for the dynamic environment of serverless functions.\nSaaS/microservices: In SaaS and microservices environments, unikernels provide a tailored solution by isolating individual applications, minimizing interference, and enhancing security in multi-tenant scenarios.\nEdge deployments for resource-constrained devices: Due to their minimal footprint, unikernels can shine in edge computing by efficiently utilizing limited resources, ensuring optimal performance for deployment in edge devices.\nComparing unikernels and containers How do unikernels compare to containers, the current standard of software delivery and execution? In our experience, there are some benefits and some drawbacks when it comes to adopting unikernels. We present the basic criteria and some comments on each technology below:\nLightweight: While containers are known for their lightweight nature, unikernels take this to the next level, offering an even more streamlined and specialized environment. By eliminating non-essential components, unikernels minimize their footprint to an extent beyond what traditional containers achieve.\nPortable: Both containers and unikernels maintain a similar level of portability, allowing applications to run consistently across various environments.\nEfficient Resource Consumption: Efficiency in resource consumption is a shared strength between containers and unikernels. Both excel in optimizing resource usage, but unikernels, with their minimalistic design, stand out in resource-constrained environments, ensuring optimal performance with minimal overhead.\nScalable: The scalability factor remains comparable between containers and unikernels. Both can be scaled horizontally to meet increased demand, providing a responsive and adaptable infrastructure for dynamic workloads.\nIsolated: Containers provide a certain level of isolation using mechanisms like cgroups and namespaces. However, unikernels benefit from hardware isolation, offering the same security as traditional VMs.\nEasy to Use: Containers boast a wide and well-defined ecosystem of tools designed to package, distribute, execute and orchestrate application. Unikernels, while powerful, are not as intuitive or as easy to use for those accustomed to the simplicity of container technologies. They require a more specialized understanding of application architecture and deployment. Furthermore, there are some missing tools required to elevate unikernels to the status of a first-class citizen in the cloud-native landscape.\nWhat is missing? The technology behind unikernels is a good candidate for cloud deployments, especially in the case of Microservices and FaaS. However, an important component is missing; the tool that will spawn unikernels and manage their lifecycle. At the same time, industry standards, such as the Open Container Initiative create specific requirements for such a tool. urunc aspires to fill this gap, enabling the use of unikernels in cloud-native environments as simply as containers.\nUnikernels are not a drop-in replacement for containers. First off, there’s no simple and user-friendly way to build unikernels for specific applications (more on this coming soon). Then, there is no way to package and distribute unikernels like OCI images. Additionally, there’s no tooling to run unikernels in a way that is compatible with OCI standards.\nThe closest tools we have for deploying unikernels are the ones made for VMs: typical sandboxed container runtimes such as kata-containers, or VM …","date":1699975332,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699975332,"objectID":"029d0c39d902b9bb4954148d93994c49","permalink":"/blog/urunc/","publishdate":"2023-11-14T15:22:12Z","relpermalink":"/blog/urunc/","section":"blog","summary":" This post is about urunc, a tool that we build to treat unikernels as containers and properly introduce unikernels to the cloud-native world! Essentially, urunc is a container runtime able to spawn unikernels that reside in container images. Before digging into the gory details, let us walk through some required concepts: unikernels, containers, and container runtimes.\n","tags":["unikernels","container runtimes","containers","urunc","k8s","kubernetes"],"title":"urunc: Introducing a unikernel container runtime","type":"blog"},{"authors":null,"categories":null,"content":"The Serverless computing paradigm has revolutionized the way applications are deployed. In a serverless architecture, developers can focus solely on writing code without the need to provision or manage servers.\nKnative is such a Serverless-Framework, providing a set of essential building blocks (on top of Kubernetes) for developers to simplify the deployment and management of cloud-services and workloads. The user simply containerizes the function (FaaS) to be deployed, managed in yaml-format as a Kubernetes-object predefined by Knative’s CRDs (Knative.Service).\nAs every K8s deployment, execution of the workload takes place in the node selected by the Scheduler (core component responsible for making decisions about where to run containers or pods within a cluster of nodes).\nAt a lower level, execution of the workload inside the node is handled by a container-manager (Docker, containerd) and the “low-level” runtimes (runc, gVisor, Kata-containers) that are pre-configured. But on the other hand, why use a ’non-traditional’ low-level runtime? What is the purpose of such an act?\nWell, to answer this we need to fully understand certain aspects of containers:\n“Containers are packages of software that contain all of the necessary elements to run in any environment. In this way, containers virtualize the operating system and run anywhere from a private data center to the public cloud” …\nDevelopers running and deploying their code in servers often led to “it works on my machine” type of problems in the past (e.g. misconfigured dependencies). Utilization of containerization technologies provides a consistent and isolated environment for applications, ensuring that software runs reliably across platforms.\nBut wait … isn’t this why we used VMs in the first place?\nOne might make the case that this is reminiscent of the original rationale for employing virtual machines (VMs) prior to the advent of containerization technologies. Despite the fact that containers and virtual machines offer analogous resource isolation and allocation advantages (as explained here), their primary distinguishing factor, from an architectural perspective, lies in the location of the abstraction layer, which can be seen in the graph below.\nFigure 1: Containers vs VMs Architecture Containers are “an abstraction at the app layer”. Several containers can operate on a single system, utilizing the same OS kernel and running as separate processes within the user space, effectively isolated from one another. On the other hand, the architecture diagram of VMs implies “an abstraction of physical hardware”. The hypervisor enables the simultaneous operation of multiple virtual machines (VMs) on a single system. Each VM encompasses a complete instance of an operating system along with the requisite applications, binaries, and libraries.\nWhile Containers offer the potential for workload isolation through the utilization of cgroups and namespaces, the underlying host’s operating system remains susceptible to risks. On the other hand VMs have their own kernel and do not directly interact with the host system, thus providing an additional layer of security between the Host and the workload.\nWhat if we could get the best of both worlds … the security of VMs plus the “lightweight” nature of containers ?\nFigure 2: Micro-VM’s Architecture “Micro-VMs provide enhanced security and workload isolation over traditional VMs, while enabling the speed and resource efficiency of containers”.\nUtilizing and configuring ’low-level’ runtimes like Kata-containers and gVisor empowers the generation of Micro-VMs from the container manager, along with the various tools using it, including Kubernetes and Knative.\nIn this article we explore how to enable the deployment of workloads sandboxed in Micro-VMs, across various runtimes via Knative.\nPrerequisites : Kubernetes Cluster \u0026gt;= v1.25 NOTE: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage.\nKnative containerd More info on how we set-up our K8s cluster with Knative can be found here\nKnative-Configuration To configure the placement of Knative Services on designated nodes and specify the runtime for execution, you can achieve this by editing the configuration as follows:\nkubectl -n knative-serving edit cm config-features kubernetes.podspec-affinity: enabled kubernetes.podspec-runtimeclassname: enabled NOTE: Add the lines under the data section, not the _example\nRunc deployment We use a simple helloworld server as Knative-Service to deploy the containerized function:\napiVersion: serving.knative.dev/v1 kind: Service metadata: name: helloworld namespace: ktest spec: template: metadata: null spec: containerConcurrency: 10 containers: - env: - name: TARGET value: Go Sample v1 image: …","date":1696861726,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696861726,"objectID":"f06c69cc49c3586ebf87075efca961ed","permalink":"/blog/knative-diverse-deployments/","publishdate":"2023-10-09T14:28:46Z","relpermalink":"/blog/knative-diverse-deployments/","section":"blog","summary":"The Serverless computing paradigm has revolutionized the way applications are deployed. In a serverless architecture, developers can focus solely on writing code without the need to provision or manage servers.\n","tags":["Knative","Container","Runtimes","Kubernetes"],"title":"Knative:  A Deep Dive into K-Deployments across diverse Runtimes (part II)","type":"blog"},{"authors":[""],"categories":null,"content":"NUBIS contribution: Involved in the MEC systems software, as well as the application deployment. Introduced Secure boot, application attestation and secure, cloud-native deployments for MEC-related applications.\n","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696118400,"objectID":"cf2dddee2b1006248b4a96374015326b","permalink":"/projects/5g-complete/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/projects/5g-complete/","section":"projects","summary":"Unified FiWi 5G architecture converging compute, storage, and RAN over eCPRI","tags":["completed"],"title":"5G-COMPLETE","type":"projects"},{"authors":[""],"categories":null,"content":" Sep 18th-21st we were in Bilbao, Spain for the Open Source Summit Europe 2023! We had the opportunity to talk about open-source, meet friends, and make new ones!\nIt was a pleasure for us to present our work on bringing unikernels closer to the cloud-native world! Video Slides\nIn this session, we went through the options that users have to deploy applications in Cloud \u0026amp; Edge environments. We talked about containers, sandboxed containers, and unikernels, introduced our own container runtime tailored to unikernels, and presented the integration effort we have undertaken to allow unikernels to execute in k8s \u0026amp; Serverless Computing frameworks.\nurunc is an open-source, lightweight container runtime that spawns unikernels as generic containers. To facilitate the packaging of unikernels as OCI artifacts, we built bima, a tool that automates the process of injecting unikernel binaries into container image layers and adds metadata that are later used by urunc.\nThis work is supported by EU H2020 research and innovation programmes, under Grant Agreements 101017168 (SERRANO) and 871900 (5G-COMPLETE).\n","date":1695945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695945600,"objectID":"2fa99e91313b2ff03b00a3396cadb8af","permalink":"/post/2023-09-29-nubificus-ltd-at-open-source-summit-europe-2023/","publishdate":"2023-09-29T00:00:00Z","relpermalink":"/post/2023-09-29-nubificus-ltd-at-open-source-summit-europe-2023/","section":"post","summary":" Sep 18th-21st we were in Bilbao, Spain for the Open Source Summit Europe 2023! We had the opportunity to talk about open-source, meet friends, and make new ones!\n","tags":["news"],"title":"Nubificus LTD at Open Source Summit Europe 2023","type":"post"},{"authors":null,"categories":[],"content":"Serverless computing is a revolutionary approach to cloud computing that allows developers to focus solely on writing and deploying code without having to manage server infrastructure.In a traditional server-based model, developers need to consider and manage the servers that will run their applications.However, with serverless computing, developers can deploy their code as self-contained functions or services that are automatically run and scaled.This enables them to build and deploy applications quickly, without worrying about the underlying infrastructure.Several frameworks have emerged to facilitate serverless computing, including AWS Lambda, Google Cloud Functions, and Microsoft Azure Functions. One notable framework is Knative, which builds upon the concepts of serverless computing and provides additional features and functionalities.In the following blog, we will delve deeper into Knative.\nKnative is an open source platform built on top of Kubernetes to facilitate the deployment, scaling, and management of serverless workloads.It was initially released in the open-source world by Google in 2014 and has since been contributed to by over 50 companies including IBM, Red Hat and SAP.\nKnative aims to simplify Kubernetes adoption for serverless computing.It provides best practices to streamline deploying container-based serverless applications on Kubernetes clusters.This enables developers to focus on writing code without worrying about infrastructure concerns like auto-scaling, routing, monitoring etc.\nKnative serves as a Kubernetes extension by providing tools and abstractions that make it easier to deploy and manage containerized workloads natively on Kubernetes.Over the years, usage of containers in software development has increased rapidly.Knative enhances the developer experience on Kubernetes by packaging code into containers and managing their execution.\nKnative has three main components that provide the core functionality for deploying and running serverless workloads on Kubernetes:\n1. Build : The Knative Build component automates the process of converting source code into containers.The build component can convert publicly accessible source codes into containers.In this mode, Knative is not only flexible, but can be configured to meet specific requirements.It also supports various build strategies and can automatically rebuild and update images when the source code changes.\n2. Serving : This component focuses on the development and scaling of stateless applications known as “services”. Knative Serving utilizes a set of Kubernets called Custom Resource Definitions (CRDs).These resources are used to define and control how the serverless workload behaves in the cluster.\nFigure 1: The primary Knative Serving resources are Services, Routes, Configurations, and Revisions The main resources of Knative Serving are Services, Routes, Configurations, and Revisions:\n• Services: Autonomously manages the entire lifecycle of the workload. Controls the creation of other objects to ensure that the application has a Route,a Configuration,and a Revision .\n• Routes: Corresponds to an endpoint of the network or to several revisions.\n• Configurations: Maintains the desired state for the application.Modifying the configuration creates a new revision.\n• Revisions: It is a snapshot (at a specific moment) of the code and Configurations for each modification made to the workload.Revisions are immutable objects and can be persisted as long as they are useful.\n3. Eventing : Eventing describes the functionality of Knative that allows it to define the behavior of a container based on specific events. That is, different events can trigger specific container-based operations.\nNow let’s see how to install knative in a k8s cluster!\nInstalling Knative Serving Prerequisites\nWe need to install kubectl. A Kubernetes cluster is required for the installation.(we tested this on Kubernetes v1.24.) For the Kubernetes cluster, ensure it has access to the internet. Let’s start !\nFirst install brew to your system.\nRun the update commands sudo apt update sudo apt-get install build-essential Install Git sudo apt install git -y Run Homebrew installation script /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; Add Homebrew to your PATH (echo; echo \u0026#39;eval \u0026#34;$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\u0026#34;\u0026#39;) \u0026gt;\u0026gt; /home/$USER/.bashrc eval \u0026#34;$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\u0026#34; Check system for potential problems brew doctor Verifying image signatures\nInstall cosign and jq. For cosign:\nbrew install cosign and check the version with command: cosign version\nOutput:\nubuntuVM:~$ cosign version ______ ______ _______. __ _______ .__ __. / | / __ \\ / || | / _____|| \\ | | | ,----\u0026#39;| | | | | (----`| | | | __ | \\| | | | | | | | \\ \\ | | | | |_ | | . ` | | `----.| `--\u0026#39; | .----) | | | | |__| | | |\\ | \\______| \\______/ |_______/ |__| \\______| |__| \\__| cosign: A tool for Container Signing, Verification and …","date":1695804126,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695804126,"objectID":"28e050ea25e5e55c183b7a14dd60bd6f","permalink":"/blog/knative-installation/","publishdate":"2023-09-27T08:42:06Z","relpermalink":"/blog/knative-installation/","section":"blog","summary":"Serverless computing is a revolutionary approach to cloud computing that allows developers to focus solely on writing and deploying code without having to manage server infrastructure.In a traditional server-based model, developers need to consider and manage the servers that will run their applications.However, with serverless computing, developers can deploy their code as self-contained functions or services that are automatically run and scaled.This enables them to build and deploy applications quickly, without worrying about the underlying infrastructure.Several frameworks have emerged to facilitate serverless computing, including AWS Lambda, Google Cloud Functions, and Microsoft Azure Functions. One notable framework is Knative, which builds upon the concepts of serverless computing and provides additional features and functionalities.In the following blog, we will delve deeper into Knative.\n","tags":["Knative","kubernetes","Cluster"],"title":"Knative: Serverless Computing in K8s (part I)","type":"blog"},{"authors":null,"categories":null,"content":"Following up on a successful VM boot on a Jetson AGX Orin, we continue exploring the capabilities of this edge device, focusing on the cloud-native aspect of application deployment.\nAs a team, we’ve built vAccel, a hardware acceleration framework that decouples the operation from its hardware implementation. One of the awesome things vAccel offers is the ability to execute hardware-accelerated applications in a VM that has no direct access to a hardware accelerator. Given the Jetson Orin board has an Ampere GPU, with 1792 cores and 56 Tensor Cores, it sounds like a perfect edge device to try isolated hardware-accelerated workload execution through VM sandboxes.\nAdditionally, coupled with our downstream kata-containers port, we can invoke a VM sandboxed container through CRI that can execute compute-intensive tasks faster by using the GPU without having direct access to it!\nOverview In this post we go through the high-level architecture of kata-containers and vAccel and provide insights into the integration we did.\nThen, we go through the steps to run a stock kata container using various supported VMMs (QEMU, AWS Firecracker, Cloud hypervisor, and Dragonball) on the Jetson AGX Orin board.\nFinally, we provide the steps to enable vAccel on a VM sandboxed container using our custom kata-containers runtime for the Go and Rust variants.\nHow kata containers work Kata containers is a sandboxed container runtime, combining the benefits of VMs in terms of workload isolation, with those of containers, in terms of portability. Installing and configuring kata-containers is straightforward and we have covered that in some of our previous posts.\nIn this post, we will not go through the details of building – we will just install kata-containers from the stock releases and add the binaries to enable vAccel execution.\nHow vAccel works vAccel is a software framework to expose hardware acceleration functionality to workloads that do not have direct access to an acceleration device. vAccel features a modular design where runtime plugins implement API operations. Figure 1 shows the software stack for the core vAccel library, along with a number of plugins and their accompanying external libraries.\nFigure 1: The vAccel software stack Apart from “hardware” plugins (that implement the API operations in some sort of acceleration library, eg CUDA, OpenCL etc.), vAccel features virtual plugins that are able to forward requests to a remote Host. This functionality makes it an open-ended API remoting framework. Based on the transport plugin (virtio or vsock) multiple setups can be enabled, making it ideal to use on VMs or on resource-constrained devices for remote execution. You can find more info about vAccel in the vAccel docs website.\nThe socket mode of vAccel for remote operations of vAccel works as follows:\nthe Host component (direct access to an accelerator), listens for requests using a predefined protocol (over gRPC) and issues vAccel API calls to the core vAccelRT library. the Remote/Guest component (no direct access to an accelerator), forwards requests to the Host component via a gRPC channel and receives results from the execution. At the moment, the Host component is a simple gRPC agent (vaccel-agent) and the Remote/Guest component is a vAccel plugin (libvaccel-vsock.so). A logical diagram of the execution flow for a VM workload taking advantage of the vAccel framework to run accelerated operations is shown in Figure 2.\nFigure 2: Execution flow for a vAccel application running in a VM kata-vAccel Integrating the vAccel framework to a sandboxed container runtime removes the need for complicated passthrough setups. No kernel prerequisites are needed (apart from the VSOCK options which is by-default enabled in most sandboxed container runtimes).\nFigure 3: kata-vAccel modes Currently, there are two modes for kata-vAccel (Figure 3): (a) the first one, exec/external mode, supporting both runtime variants of kata-containers (Go and Rust), spawns the vaccelrt-agent as an external binary, listening to the vsock socket available for the kata-agent component of the container runtime; (b) the second mode, integrated is only supported for the rust runtime variant of kata-containers and embeds the functionality of the vaccelrt-agent into the runtime, allowing better control and sandboxing of the components running on the host system.\nAn overview of the execution flow for a vAccel-enabled sandboxed container running on kata-containers is shown in Figure 4.\nFigure 4: Execution flow for a vAccel-enabled sandboxed container kata-containers binary installation The steps to install the stock kata-containers runtime are shown below:\nGet the release tarball:\nwget https://github.com/kata-containers/kata-containers/releases/download/3.2.0-rc0/kata-static-3.2.0-rc0-arm64.tar.xz Unpack it in a temporary directory:\nmkdir /tmp/kata-release xzcat kata-static-3.2.0-rc0-arm64.tar.xz | tar -xvf - -C /tmp/kata-release You should be presented with the …","date":1695569464,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695569464,"objectID":"ad8728765ec4ac577fba7c18d657fade","permalink":"/blog/orin-vm-vaccel/","publishdate":"2023-09-24T16:31:04+01:00","relpermalink":"/blog/orin-vm-vaccel/","section":"blog","summary":"Following up on a successful VM boot on a Jetson AGX Orin, we continue exploring the capabilities of this edge device, focusing on the cloud-native aspect of application deployment.\n","tags":["NVIDIA","Jetson AGX Orin","vAccel","Kata Containers","Runtimes"],"title":"Isolated, hardware-accelerated functions on Jetson AGX Orin","type":"blog"},{"authors":["Anastassios Nanos"],"categories":null,"content":"Code \u0026amp; Resources urunc - Container Runtime Repository bima - Unikernel Containerization Tool ","date":1695213600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695213600,"objectID":"c5ad42bd3a19ab6a60bd303af4416238","permalink":"/event/oss2023/","publishdate":"2023-09-20T00:00:00Z","relpermalink":"/event/oss2023/","section":"event","summary":"URUNC - A Unikernel Container Runtime","tags":["unikernels","container-runtime","serverless","urunc","security"],"title":"OSS 2023: URUNC - A Unikernel Container Runtime","type":"event"},{"authors":[""],"categories":null,"content":" We are excited to host a Unikraft Hackathon in Athens, GR! It is a great opportunity for students, professionals and systems software enthusiasts to meet, exchange ideas, and code together in this two-day event!\nNubificus LTD, along with the Unikraft community, the High Speed Communication Networks Lab (HSCN) and the Computing Systems Lab (CSLab) of the National Technical University of Athens (ICCS/NTUA) come together to organize the Unikraft Athens Hackathon to be held on Thursday and Friday, March 30-31, 2023.\nAs part of the Unikraft community Răzvan Deaconescu and Ștefan Jumărea will be present on-site, with other community members providing support online, on Discord.\nAt the end of the first day, starting at 16:15, we will host a short session with invited talks:\n“The Value of Unikernels in the Emerging Disaggregated HPC and AI Clusters”, Dimitris Syrivelis\n“Unikernels for Serverless”, Anastassios Nanos\n“Running MPI applications on Toro unikernel”, Matias Vara Larsen\nThe hackathon will take place as an in-person event at the Multimedia Amphitheater at the Zografou campus of NTUA. The full address is: Multimedia Amphitheater, Central Library Building, Heroon Polytechniou 9, 15780 Zografou, Greece.\nSupport information and discussions will take place on Discord on the #hack-athens23 channel.\nUnikraft Unikraft is a fast, secure and open-source Unikernel Development Kit, optimizing application execution by tailoring the operating system, libraries and configuration to the particular needs of the application. It vastly reduces virtual machine and container image sizes to a few KBs, provides blazing performance, and drastically cuts down the software stack’s attack surface.\n","date":1679270400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1679270400,"objectID":"5014cec540ef33a662234a5d95f052a4","permalink":"/post/2023-03-20-unikraft-hackathon/","publishdate":"2023-03-20T00:00:00Z","relpermalink":"/post/2023-03-20-unikraft-hackathon/","section":"post","summary":" We are excited to host a Unikraft Hackathon in Athens, GR! It is a great opportunity for students, professionals and systems software enthusiasts to meet, exchange ideas, and code together in this two-day event!\n","tags":["news"],"title":"Unikraft Hackathon","type":"post"},{"authors":null,"categories":null,"content":"In 2022, NVIDIA released the Jetson Orin modules, specifically designed for extreme computation at the Edge. The NVIDIA Jetson AGX Orin modules deliver up to 275 TOPS of AI performance with power configurable between 15W and 60W.\nFigure 1: The NVIDIA Jetson AGX Orin devkit \u0026amp; module These powerful machines, apart from bleeding edge GPUs, feature 12 ARMv8 cores and come with 16GB, 32GB or 64GB of memory. The number of cores, combined with these amounts of RAM appear ideal for some use-cases where multi-tenancy is essential; making use of the virtualization extensions in these cores, we enforce stronger isolation among the workloads running at the Edge. Moreover, with the use of kata-containers, we maintain the cloud-native aspect of the application deployment at the Edge.\nFollowing up on our work with kata-containers and vAccel, we got a couple of Orin nodes and started experimenting. Unfortunately, things are a bit different than with the original Jetson AGX Xavier boards we were used to. The SoC is slightly different, with a fully-featured, multi-core GICv3 implementation.\nThat’s great news! Right? well… not really, as the necessary control structures for GICv3 are not created during boot, caused by an incomplete interrupts declaration in the device tree.\nVadim \u0026amp; Alexey identified the issue and provided the necessary patches to the device tree source files and .. tada! we can boot a non-emulated VM, using GICv3 on Jetson Orin!\nWalk through the issue So, initially, we started to see if the stock kernel has KVM enabled:\n# dmesg |grep -i kvm [ 0.362967] kvm [1]: IPA Size Limit: 48 bits [ 0.363130] kvm [1]: VHE mode initialized successfully Looks like there is support. So we moved on to try running an AWS Firecracker VM. Got our vmlinux, rootfs.img and config.json built for our tests and tried to boot the VM:\nwget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/vm-example/aarch64/rust-vmm/vmlinux wget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/vm-example/aarch64/rootfs.img wget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/vm-example/aarch64/fc/config_vsock.json wget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/vm-example/aarch64/fc/firecracker # ./firecracker --api-sock fc.sock --config-file config_vsock.json 2023-02-13T18:54:29.061153660 [anonymous-instance:main:ERROR:src/firecracker/src/main.rs:480] Building VMM configured from cmdline json failed: Internal(Vm(VmCreateGIC(CreateGIC(Error(19))))) We were a bit troubled as the exact same setup was working fine on a Jetson AGX Xavier. We also tried QEMU, with the same results:\n# qemu-system-aarch64 -cpu max -machine virt,gic-version=3,kernel-irqchip=on -m 1024 -nographic -monitor none -kernel /boot/Image -enable-kvm qemu-system-aarch64: gic-version=3 is not supported with kernel-irqchip=off but if we disable KVM on qemu, we can see the kernel booting:\n# qemu-system-aarch64 -cpu max -machine virt,gic-version=3,kernel-irqchip=on -m 1024 -nographic -monitor none -kernel /boot/Image [ 0.000000] Booting Linux on physical CPU 0x0000000000 [0x000f0510] [ 0.000000] Linux version 5.10.65-tegra (buildbrain@mobile-u64-5266-d7000) (aarch64-buildroot-linux-gnu-gcc.br_real (Buildroot 2020.08) 9.3.0, GNU ld (GNU Binutils) 2.33.1) #1 SMP PREEMPT Tue Mar 15 00:53:43 PDT 2022 [ 0.000000] OF: fdt: memory scan node memory@40000000, reg size 16, [ 0.000000] OF: fdt: - 40000000 , 40000000 [ 0.000000] Machine model: linux,dummy-virt [ 0.000000] efi: UEFI not found. [ 0.000000] Zone ranges: [ 0.000000] DMA [mem 0x0000000040000000-0x000000007fffffff] [ 0.000000] DMA32 empty [ 0.000000] Normal empty [ 0.000000] Movable zone start for each node [snipped] So after digging into this issue, googling and trying various workarounds, we came across the points raised above about the GICv3 initialization.\nSolution Following the instructions on how to build a kernel for the Jetson AGX Orin series, we get the sources and patch the device tree sources using the following snippet:\n--- Linux_for_Tegra/source/public/hardware/nvidia/soc/t23x/kernel-dts/tegra234-soc/tegra234-soc-minimal.dtsi.orig\t2022-08-11 03:14:51.000000000 +0000 +++ Linux_for_Tegra/source/public/hardware/nvidia/soc/t23x/kernel-dts/tegra234-soc/tegra234-soc-minimal.dtsi\t2023-02-12 09:07:10.259761186 +0000 @@ -43,6 +43,10 @@ reg = \u0026lt;0x0 0x0f400000 0x0 0x00010000 /* GICD */ 0x0 0x0f440000 0x0 0x00200000\u0026gt;; /* GICR CPU 0-15 */ ranges; +\tinterrupts = \u0026lt;GIC_PPI 9 + (GIC_CPU_MASK_SIMPLE(8) | IRQ_TYPE_LEVEL_HIGH)\u0026gt;; + interrupt-parent = \u0026lt;\u0026amp;intc\u0026gt;; + status = \u0026#34;disabled\u0026#34;; gic_v2m: v2m@f410000 { We build the kernel using the following command:\nLinux_for_Tegra/source/public# ./nvbuild.sh -o kernel_out_updated Essentially, just the device tree is needed:\nkernel_out_updated/arch/arm64/boot/dts/nvidia/tegra234-p3701-0000-p3737-0000.dtb We copy this file to the board at the following directory:\n/boot/dtb and tweak the boot loader config to load the updated device-tree file, instead of the default one:\n--- /boot/extlinux/extlinux.conf.orig\t2023-02-13 …","date":1676279644,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676279644,"objectID":"c53d02e711e5a549a8938c7a2bdc5a17","permalink":"/blog/orin-vm/","publishdate":"2023-02-13T10:14:04+01:00","relpermalink":"/blog/orin-vm/","section":"blog","summary":"In 2022, NVIDIA released the Jetson Orin modules, specifically designed for extreme computation at the Edge. The NVIDIA Jetson AGX Orin modules deliver up to 275 TOPS of AI performance with power configurable between 15W and 60W.\n","tags":["NVIDIA","Jetson AGX Orin","vAccel","GICv3","Kata Containers"],"title":"Boot a VM on an NVIDIA Jetson AGX Orin","type":"blog"},{"authors":["Anastassios Nanos","Charalampos Mainas"],"categories":null,"content":"Code \u0026amp; Resources vAccel Documentation vAccel Core Library Source vAccel v0.5.0 Release ","date":1675605600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675605600,"objectID":"ae767bafc67f729078e63ec05eed9710","permalink":"/event/fosdem2023/","publishdate":"2023-02-05T00:00:00Z","relpermalink":"/event/fosdem2023/","section":"event","summary":"Hardware Acceleration for Unikernels - A Status Update of vAccel","tags":["unikernels","hardware-acceleration","vaccel","security"],"title":"FOSDEM 2023: Hardware Acceleration for Unikernels - A Status Update of vAccel","type":"event"},{"authors":[""],"categories":null,"content":" Nubificus LTD proudly presents the latest release of the vAccel framework. vAccel v0.5.0 is available as a tar bundle for x86_64 and aarch64 architectures, with the essential binaries for users to get started.\nAdditionally, the individual components are packaged as binary artifacts or deb packages for users to install them directly. For a list of the binary components please visit vAccel’s documentation page. The core vAccel library is open-source, available on github.\nvAccel vAccel enables workloads to enjoy hardware acceleration while running on environments that do not have direct (physical) access to acceleration devices. With a slim design and precise abstractions, vAccel semantically exposes hardware acceleration features to users with little to no knowledge of software acceleration framework internals.\nvAccel integrates with container runtimes such as kata-containers. v0.5.0 brings updated support for kata-containers v3.0, so deploying an application that requires hardware acceleration in a sandboxed container in k8s is now possible without complicated hardware setups!\nA subset of vAccel’s API is also integrated with Unikernel frameworks such as Unikraft. See the relevant documentation for more information on how to get started!\nServerless computing workflows are now able to enjoy compute-offload mechanisms to provide AI/ML services as functions, triggered thousands or millions of times by events, on-demand, auto-scaling to multiple physical and virtual nodes, without the need to directly attach a hardware device to the instance. This functionality is enabled through vAccel’s virtualization backends, enabling the execution of hardware-accelerated functions on Virtual Machines. Support for numerous hypervisors is available, including AWS Firecracker, QEMU/KVM, Cloud Hypervisor, etc. See the relevant documentation on how to run an example on a VM.\nvAccel v0.5.0 offers enhanced API remoting functionality over generic sockets, supporting AF_VSOCK and AF_INET, enabling local (intra-node) and remote execution over the network. Optimized AF_VSOCK support is also offered using the virtio-vsock backend on QEMU/KVM, AWS Firecracker, Cloud hypervisor, Dragonball and any other hypervisor that supports virtio-vsock.\nThis iteration’s update also contains important bug fixes and performance optimizations.\nThe roadmap for v0.6.0 contains PyTorch support (currently under development).\nEnd-users can get a sneak peek at what vAccel has to offer on the project’s website, on github, or by browsing through the available documentation. To get started, follow the Quickstart guide.\n","date":1675036800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675036800,"objectID":"2c5fc590cc7847a4341b6a04fe982315","permalink":"/post/2023-01-30-vaccel-v050-is-out/","publishdate":"2023-01-30T00:00:00Z","relpermalink":"/post/2023-01-30-vaccel-v050-is-out/","section":"post","summary":" Nubificus LTD proudly presents the latest release of the vAccel framework. vAccel v0.5.0 is available as a tar bundle for x86_64 and aarch64 architectures, with the essential binaries for users to get started.\n","tags":["news"],"title":"vAccel v0.5.0 is out!","type":"post"},{"authors":null,"categories":null,"content":"Running applications that need hardware acceleration in public clouds remains a challenge, both for end-users and for service providers. The reasons for this are mostly related to the complicated hardware abstractions that acceleration devices expose, as well as to the complicated software stacks that drive these devices.\nIn an effort to hide the complexity of the software stack under the hood, and provide end-users with the capability of accelerating their applications, we have introduced vAccel, a hardware acceleration abstraction to semantically expose functions that can be accelerated to workloads running in VMs or even remote hosts.\nIn this post, we will present how we can use vAccel to remotely execute basic FPGA operations on a PYNQ-Z1 board. First, we go through a brief description of the hardware and software components of this example, as well as the steps to reproduce the experiment. We install the vAccel software stack to the board running a generic linux distribution and run a local example. Then, we run the same example remotely, using a client machine connected to the same network as our development board.\nOverview As mentioned above, we are using a PYNQ-Z1 development board. The PYNQ-Z1 board is the hardware platform for the PYNQ open-source framework. It features a Zynq-7000 (XC7Z020-1CLG400C) All Programmable System-On-Chip (APSoC), integrating a feature-rich dual-core Cortex-A9 based processing system (PS) and Xilinx programmable logic (PL) in a single device. Figure 1 shows an image of the PYNQ-Z1 development board by Digilent.\nApart from Petalinux, you can install a generic linux distribution. We recently walked through the process of installing debian on a PYNQ-Z1.\nInstall vAccel We can use the binary release or build from source. For the sake of completeness we present both options.\nInstall from binaries Get the deb package for the core vAccelRT library and install it:\nwget https://s3.nbfc.io/nbfc-assets/github/vaccelrt/master/aarch32/Release-deb/vaccel-0.5.0-Linux.deb sudo dpkg -i vaccel-0.5.0-Linux.deb We should be presented with a couple of libraries in /usr/local/lib as well as some example binaries on /usr/local/bin.\nSkip the next section and go directly to Test the installation.\nBuild from source Clone the repo and prepare to build:\ngit clone https://github.com/cloudkernels/vaccelrt --recursive cd vaccelrt mkdir -p build \u0026amp;\u0026amp; cd build cmake ../ -DBUILD_PLUGIN_NOOP=ON -DBUILD_EXAMPLES=ON To build and install use the following simple command:\nmake install Test the installation To make sure we’ve got everything setup correctly, we can run a couple of examples. First we could use the noop plugin to do image classification on an image.\n# Set the path to the vAccel libraries export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib # Set the plugin to noop export VACCEL_BACKENDS=/usr/local/lib/libvaccel-noop.so # enable Debug export VACCEL_DEBUG_LEVEL=4 # Run the classify example /usr/local/bin/classify /usr/local/share/images/example.jpg 1 We should be presented with debug output and a dummy classification tag:\nuser@debian-fpga:~$ /usr/local/bin/classify /usr/local/share/images/example.jpg 1 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Initializing vAccel 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Created top-level rundir: /run/user/1001/vaccel.ogcPti 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Registered plugin noop 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Registered function noop from plugin noop 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Registered function sgemm from plugin noop 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Registered function image classification from plugin noop 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Registered function image detection from plugin noop 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Registered function image segmentation from plugin noop 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Registered function image pose estimation from plugin noop 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Registered function image depth estimation from plugin noop 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Registered function exec from plugin noop 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Registered function TensorFlow session load from plugin noop 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Registered function TensorFlow session run from plugin noop 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Registered function TensorFlow session delete from plugin noop 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Registered function MinMax from plugin noop 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Registered function Array copy from plugin noop 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Registered function Vector Add from plugin noop 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Registered function Parallel acceleration from plugin noop 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Registered function Matrix multiplication from plugin noop 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; Loaded plugin noop from /usr/local/lib/libvaccel-noop.so 2023.01.21-02:46:23.09 - \u0026lt;debug\u0026gt; session:1 New session Initialized session with id: 1 Image size: 79281B 2023.01.21-02:46:23.11 - \u0026lt;debug\u0026gt; session:1 Looking for …","date":1674206044,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674206044,"objectID":"da08059abb77ba0dfb0a6a22401145db","permalink":"/blog/pynq-z1-vaccel/","publishdate":"2023-01-20T10:14:04+01:00","relpermalink":"/blog/pynq-z1-vaccel/","section":"blog","summary":"Running applications that need hardware acceleration in public clouds remains a challenge, both for end-users and for service providers. The reasons for this are mostly related to the complicated hardware abstractions that acceleration devices expose, as well as to the complicated software stacks that drive these devices.\n","tags":["PYNQ-Z1","FPGA","vAccel","Zynq-7000"],"title":"Remote FPGA acceleration: vAccel on PYNQ-Z1","type":"blog"},{"authors":null,"categories":null,"content":"The PYNQ-Z1 board is the hardware platform for the PYNQ open-source framework. It features a Zynq-7000 (XC7Z020-1CLG400C) All Programmable System-On-Chip (APSoC), integrating a feature-rich dual-core Cortex-A9 based processing system (PS) and Xilinx programmable logic (PL) in a single device. Figure 1 shows an image of the PYNQ-Z1 development board by Digilent.\nFigure 1: The PYNQ-Z1 board Digging into the hardware architecture of this board is out of scope of this text, so if you are interested in more details, have a look at the Zynq Technical Reference manual.\nWe will go through a brief description of the software components needed to bring up the board We describe the process to install a linux distribution and verify that the board can execute a simple example.\nSoftware components Bringing up the PYNQ-Z1 board is fairly straight-forward:\nOption 1: use the vendor-provided SD card image for PYNQ-Z1 v3.0.1 from the official website. Option 2: use a custom linux distribution; build u-boot, the linux kernel/modules and a generic rootfs of the preferred linux distro, using the tools you are most familiar with. We will go with Option 2, following the guide provided by Ichiro Kawazome.\nInstall a linux distribution on the PYNQ-Z1 board First, we will create a target directory that will host the contents of the SD card image: the early boot files, the linux kernel and drivers, and the user-space utilities.\nmkdir -p target mkdir -p target/boot Step 1: Build u-boot. Step 2: Build the Linux kernel Step 3: Build the rootfs (Debian 11) Step 4: Build Drivers and Services Step 1: Build u-boot Get the patches/scripts:\ngit clone --depth=1 -b v2016.03-1 https://github.com/ikwzm/FPGA-SoC-U-Boot-PYNQ-Z1.git We need to build u-boot-spl.sfp and u-boot.img. The steps are outlined below:\nDownload U-boot Source git clone git://git.denx.de/u-boot.git u-boot-2016.03-zynq-pynq-z1 Pick the supported version:\ncd u-boot-2016.03-zynq-pynq-z1 git checkout -b u-boot-2016.03-zynq-pynq-z1 refs/tags/v2016.03 Apply a number of patches, specific to PYNQ-Z1:\npatch -p0 \u0026lt; ../files/u-boot-2016.03-zynq-pynq-z1.diff git add --update git add arch/arm/dts/zynq-pynqz1.dts git add board/xilinx/zynq/pynqz1_hw_platform/* git add configs/zynq_pynqz1_defconfig git add include/configs/zynq_pynqz1.h git commit -m \u0026#34;patch for zynq-pynq-z1\u0026#34; git tag -a v2016.03-zynq-pynq-z1-1 -m \u0026#34;Release v2016.03-1 for PYNQ-Z1\u0026#34; Patch for the breaking changes of OpenSSL v1.1.x:\nwget https://blog.cloudkernels.net/pynq/0001-fix-build-with-OpenSSL-1.1.x.patch git am 0001-fix-build-with-OpenSSL-1.1.x.patch Setup for Build:\ncd u-boot-2016.03-zynq-pynq-z1 export ARCH=arm export CROSS_COMPILE=arm-linux-gnueabihf- make zynq_pynqz1_defconfig Build u-boot \u0026amp; extract binaries:\nmake # Copy u-boot.img, u-boot.elf and u-boot-spl.sfp to root directory cp spl/boot.bin ../boot.bin cp u-boot.img ../u-boot.img cp u-boot ../u-boot.elf cd .. Copy boot.bin and u-boot.img to target/boot/:\nshell$ cp boot.bin ../target/zynq-pynqz1/boot/ shell$ cp u-boot.img ../target/zynq-pynqz1/boot/ Step 2: Build the Linux Kernel Let’s go back to our working director (cd ../) and get the repo sources (we will need some PYNQ-Z1 specific patches):\nwget https://github.com/ikwzm/FPGA-SoC-Linux/archive/refs/tags/v2.1.1.tar.gz tar -zxvf v2.1.1.tar.gz cd FPGA-SoC-Linux-2.1.1 Get the kernel sources (v5.10.109):\ngit clone --depth 1 -b v5.10.109 git://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git linux-5.10.109-armv7-fpga cd linux-5.10.109-armv7-fpga git checkout -b linux-5.10.109-armv7-fpga refs/tags/v5.10.109 Patch for armv7:\npatch -p1 \u0026lt; ../files/linux-5.10.109-armv7-fpga.diff git add --update git add arch/arm/configs/armv7_fpga_defconfig git add arch/arm/boot/dts/zynq-pynqz1.dts git commit -m \u0026#34;patch for armv7-fpga\u0026#34; Patch for usb chipidea driver:\npatch -p1 \u0026lt; ../files/linux-5.10.109-armv7-fpga-patch-usb-chipidea.diff git add --update git commit -m \u0026#34;patch for usb chipidea driver for issue #3\u0026#34; Patch for build debian package script:\npatch -p1 \u0026lt; ../files/linux-5.10.109-armv7-fpga-patch-builddeb.diff git add --update git commit -m \u0026#34;patch for scripts/package/builddeb to add tools/include and postinst script to header package\u0026#34; Create tag and .version:\ngit tag -a v5.10.109-armv7-fpga -m \u0026#34;release v5.10.109-armv7-fpga\u0026#34; echo 1 \u0026gt; .version Setup for Build:\ncd linux-5.10.109-armv7-fpga export ARCH=arm export CROSS_COMPILE=arm-linux-gnueabihf- make armv7_fpga_defconfig Build Linux Kernel and device tree:\nexport DTC_FLAGS=--symbols make deb-pkg make zynq-zybo.dtb make zynq-zybo-z7.dtb make zynq-pynqz1.dtb make socfpga_cyclone5_de0_nano_soc.dtb Copy zImage to vmlinuz-5.10.109-armv7-fpga:\ncp arch/arm/boot/zImage ../vmlinuz-5.10.109-armv7-fpga Copy devicetree to target/boot/:\ncp arch/arm/boot/dts/zynq-pynqz1.dtb ../target/zynq-pynqz1/boot/devicetree-5.10.109-zynq-pynqz1.dtb ./scripts/dtc/dtc -I dtb -O dts -o ../target//boot/devicetree-5.10.109-zynq-pynqz1.dts arch/arm/boot/dts/zynq-pynqz1.dtb Step 3: Build the rootfs (Debian 11) …","date":1672924444,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672924444,"objectID":"c94e7f477ca730158e52d6744d72edb2","permalink":"/blog/pynq-z1/","publishdate":"2023-01-05T14:14:04+01:00","relpermalink":"/blog/pynq-z1/","section":"blog","summary":"The PYNQ-Z1 board is the hardware platform for the PYNQ open-source framework. It features a Zynq-7000 (XC7Z020-1CLG400C) All Programmable System-On-Chip (APSoC), integrating a feature-rich dual-core Cortex-A9 based processing system (PS) and Xilinx programmable logic (PL) in a single device. Figure 1 shows an image of the PYNQ-Z1 development board by Digilent.\n","tags":["PYNQ-Z1","FPGA","Linux","Zynq-7000"],"title":"PYNQ-Z1 bringup","type":"blog"},{"authors":["Alexandros Patras","Foivos Pournaropoulos","Nikolaos Bellas","Christos D Antonopoulos","Spyros Lalis","Maria Goutha","Anastassios Nanos"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"06e8de4ac94296ca18f72844038d8ca5","permalink":"/publication/patras-2023-minimal/","publishdate":"2025-10-15T23:59:22.767899Z","relpermalink":"/publication/patras-2023-minimal/","section":"publication","summary":"","tags":null,"title":"A Minimal Testbed for Experimenting with Flexible Resource and Application Management in Heterogeneous Edge-Cloud Systems.","type":"publication"},{"authors":["Efthymios Chondrogiannis","Efstathios Karanastasis","Vassiliki Andronikou","Adrian Spătaru","Anastassios Nanos","Aristotelis Kretsis","Panagiotis Kokkinos"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"d2b2e717721e1a67be560cd4e2b1ab85","permalink":"/publication/chondrogiannis-2023-intent/","publishdate":"2025-10-15T23:59:22.761731Z","relpermalink":"/publication/chondrogiannis-2023-intent/","section":"publication","summary":"","tags":null,"title":"Intent-Based AI-Enhanced Service Orchestration for Application Deployment and Execution in the Cloud Continuum","type":"publication"},{"authors":["Adrian Spătaru","Anastassios Nanos","Aristotelis Kretsis","Panagiotis Kokkinos"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"d18a74050ce34401a51ce46cb648da2f","permalink":"/publication/spuataru-2023-orchestration/","publishdate":"2025-10-15T23:59:22.810891Z","relpermalink":"/publication/spuataru-2023-orchestration/","section":"publication","summary":"","tags":null,"title":"Orchestration for Application Deployment","type":"publication"},{"authors":["Polyzois Soumplis","Georgios Kontos","Aristotelis Kretsis","Panagiotis Kokkinos","Anastassios Nanos","Emmanouel Varvarigos"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"398c5045b1355a53b2521c6aa3c1bf11","permalink":"/publication/soumplis-2023-security/","publishdate":"2025-10-15T23:59:22.780075Z","relpermalink":"/publication/soumplis-2023-security/","section":"publication","summary":"","tags":null,"title":"Security-Aware Resource Allocation in the Edge-Cloud Continuum","type":"publication"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"/contact/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"Contact","type":"landing"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"b0d61e5cbb7472bf320bf0ef2aaeb977","permalink":"/tour/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/tour/","section":"","summary":"","tags":null,"title":"Tour","type":"landing"},{"authors":["Anastassios Nanos"],"categories":null,"content":"Code \u0026amp; Resources vAccel Documentation vAccel Core Library Source vAccel Demo ","date":1662022800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662022800,"objectID":"ae5c800fde6bda910d57a5823c7f7708","permalink":"/event/duac2022/","publishdate":"2022-09-01T00:00:00Z","relpermalink":"/event/duac2022/","section":"event","summary":"vAccel - Interoperable Application Hardware Acceleration","tags":["hardware-acceleration","vaccel","serverless","virtualization"],"title":"DUAC 2022: vAccel - Interoperable Application Hardware Acceleration","type":"event"},{"authors":[""],"categories":null,"content":" Nubificus LTD has been identified by the European Commission’s Innovation Radar as a high potential innovator for their work on Serverless computing. Specifically, EC’s IR identified the Serverless Framework that Nubificus LTD develops as having a high potential for innovation. The lightweight serverless framework for the Edge is being developed as part of the 5G-COMPLETE Horizon 2020-funded project.\nThe framework to provide Fast, Secure \u0026amp; Efficient Serverless for the Edge was assessed by the IR (https://www.innoradar.eu) as technology that addresses the needs of existing markets and falls under IR’s exploring category. This category prizes organizations’ initiative to take steps to actively explore value creation opportunities, commercialization and the pursuit of concrete market-oriented ideas that advance technology development processes.\n5G-COMPLETE The 5G-COMPLETE project aims to revolutionize the 5G architecture, by efficiently combining compute and storage resource functionality over a unified ultra-high capacity converged digital/analog Fiber-Wireless (FiWi) Radio Access Network (RAN). By employing the recent advances in Ethernet fronthauling introduced by the eCPRI standard as a launching point, 5G-COMPLETE introduces and combines a series of key technologies under a unique architectural proposition. Nubificus particular focus is on the rapid and cost-efficient service deployment through lightweight virtualization mechanisms and unikernel technology.\nThe unique systems software components stemmed from the project are leveraged to explore disruptive technologies for Serverless computing and extracted the essential features to allow workloads to be deployed and executed securely, at the Edge, supporting hardware acceleration functionality.\nInnovation Radar The Innovation Radar is a European Commission initiative that identifies high potential innovations and innovators in EU-funded research and innovation projects. It bases its selection on information and data gathered by independent experts who review research and innovation projects funded by the European Commission.\nThe goal of the Innovation Radar platform is to show citizens the scientific and technological advances that take place thanks to the Commission’s funding. By providing greater access to such information, the platform hopes to encourage the development of a dynamic ecosystem of incubators, entrepreneurs, funding agencies and investors that can help get EU-funded innovations to the market faster.\nResources Innovation Radar: https://www.innoradar.eu 5G-COMPLETE website: https://5gcomplete.eu Twitter: https://twitter.com/5gcomplete LinkedIn: https://www.linkedin.com/company/5gcomplete ","date":1661126400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661126400,"objectID":"1ac00b7fc1cc5b44451b88666192f8f7","permalink":"/post/2022-08-22-the-2022-innovation-radar-praises-nubificus-work-on-serverless-computing/","publishdate":"2022-08-22T00:00:00Z","relpermalink":"/post/2022-08-22-the-2022-innovation-radar-praises-nubificus-work-on-serverless-computing/","section":"post","summary":" ","tags":["news"],"title":"The 2022 Innovation Radar praises Nubificus' work on Serverless Computing","type":"post"},{"authors":["Anastassios Nanos","Charalampos Mainas"],"categories":null,"content":"","date":1654678800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654678800,"objectID":"1c390ae20e717443eb0d8ff78847a067","permalink":"/event/openinfra2022/","publishdate":"2022-06-08T00:00:00Z","relpermalink":"/event/openinfra2022/","section":"event","summary":"Interoperable Hardware Acceleration for Serverless","tags":["hardware-acceleration","vaccel","serverless","kubernetes","tensorflow","kata-containers","kvm"],"title":"OpenInfra Summit 2022: Interoperable Hardware Acceleration for Serverless","type":"event"},{"authors":null,"categories":null,"content":"Picking up from where we left in our previous post, we will now install AWS Firecracker and configure Kata Containers to use it as their hypervisor.\nBuild Firecracker Kata Containers only support AWS Firecracker v0.23.1 (yet). To build Firecracker, we will clone the Github repo and checkout to the 0.23.1 version:\ngit clone https://github.com/firecracker-microvm/firecracker.git -b v0.23.1 --depth 1 \u0026amp;\u0026amp;\\ cd firecracker \u0026amp;\u0026amp;\\ git submodule update --init UPDATE: Tue Feb 21 16:43:47 UTC 2023: As of kata-containers#4735, the kata go runtime supports AWS Firecracker v1.1.0. So you can use the following command:\ngit clone https://github.com/firecracker-microvm/firecracker.git -b v1.1.0 --depth 1 \u0026amp;\u0026amp;\\ cd firecracker \u0026amp;\u0026amp;\\ git submodule update --init Now we can build the binaries:\nNote AWS Firecracker uses docker to build the image, so make sure your user can access the docker daemon, or just run with sudo.\nsudo ./tools/devtool -y build --release toolchain=\u0026#34;$(uname -m)-unknown-linux-musl\u0026#34; sudo cp build/cargo_target/${toolchain}/release/firecracker /opt/kata/bin/firecracker \u0026amp;\u0026amp;\\ sudo cp build/cargo_target/${toolchain}/release/jailer /opt/kata/bin/jailer devmapper snapshotter AWS Firecracker requires a block device as the backing store for a VM. To interact with containerd and kata we use the devmapper snapshotter. To check support for your containerd installation, you can run:\nctr plugins ls |grep devmapper if the output of the above command is:\nio.containerd.snapshotter.v1 devmapper linux/amd64 ok then you can skip this section and move on to Configure Kata Containers to use Firecracker\nIf the output of the above command is:\nio.containerd.snapshotter.v1 devmapper linux/amd64 error then we need to setup devmapper snapshotter. Based on a very useful guide from docker, we can set it up using the following scripts:\n#!/bin/bash set -ex DATA_DIR=/var/lib/containerd/io.containerd.snapshotter.v1.devmapper POOL_NAME=containerd-pool mkdir -p ${DATA_DIR} # Create data file sudo touch \u0026#34;${DATA_DIR}/data\u0026#34; sudo truncate -s 100G \u0026#34;${DATA_DIR}/data\u0026#34; # Create metadata file sudo touch \u0026#34;${DATA_DIR}/meta\u0026#34; sudo truncate -s 10G \u0026#34;${DATA_DIR}/meta\u0026#34; # Allocate loop devices DATA_DEV=$(sudo losetup --find --show \u0026#34;${DATA_DIR}/data\u0026#34;) META_DEV=$(sudo losetup --find --show \u0026#34;${DATA_DIR}/meta\u0026#34;) # Define thin-pool parameters. # See https://www.kernel.org/doc/Documentation/device-mapper/thin-provisioning.txt for details. SECTOR_SIZE=512 DATA_SIZE=\u0026#34;$(sudo blockdev --getsize64 -q ${DATA_DEV})\u0026#34; LENGTH_IN_SECTORS=$(bc \u0026lt;\u0026lt;\u0026lt; \u0026#34;${DATA_SIZE}/${SECTOR_SIZE}\u0026#34;) DATA_BLOCK_SIZE=128 LOW_WATER_MARK=32768 # Create a thin-pool device sudo dmsetup create \u0026#34;${POOL_NAME}\u0026#34; \\ --table \u0026#34;0 ${LENGTH_IN_SECTORS} thin-pool ${META_DEV} ${DATA_DEV} ${DATA_BLOCK_SIZE} ${LOW_WATER_MARK}\u0026#34; cat \u0026lt;\u0026lt; EOF # # Add this to your config.toml configuration file and restart containerd daemon # [plugins] [plugins.devmapper] pool_name = \u0026#34;${POOL_NAME}\u0026#34; root_path = \u0026#34;${DATA_DIR}\u0026#34; base_image_size = \u0026#34;10GB\u0026#34; discard_blocks = true EOF Make it executable and run it:\nsudo chmod +x ~/scripts/devmapper/create.sh \u0026amp;\u0026amp; \\ cd ~/scripts/devmapper/ \u0026amp;\u0026amp; \\ sudo ./create.sh Now, we can add the devmapper configuration provided from the script to /etc/containerd/config.toml and restart containerd.\nsudo systemctl restart containerd We can use dmsetup to verify that the thin-pool was created successfully. We should also check that devmapper is registered and running:\nsudo dmsetup ls # devpool (253:0) sudo ctr plugins ls | grep devmapper # io.containerd.snapshotter.v1 devmapper linux/amd64 ok This script needs to be run only once, while setting up the devmapper snapshotter for containerd. Afterwards, make sure that on each reboot, the thin-pool is initialized from the same data dir. Otherwise, all the fetched containers (or the ones that you’ve created) will be re-initialized. A simple script that re-creates the thin-pool from the same data dir is shown below:\n#!/bin/bash set -ex DATA_DIR=/var/lib/containerd/io.containerd.snapshotter.v1.devmapper POOL_NAME=containerd-pool # Allocate loop devices DATA_DEV=$(sudo losetup --find --show \u0026#34;${DATA_DIR}/data\u0026#34;) META_DEV=$(sudo losetup --find --show \u0026#34;${DATA_DIR}/meta\u0026#34;) # Define thin-pool parameters. # See https://www.kernel.org/doc/Documentation/device-mapper/thin-provisioning.txt for details. SECTOR_SIZE=512 DATA_SIZE=\u0026#34;$(sudo blockdev --getsize64 -q ${DATA_DEV})\u0026#34; LENGTH_IN_SECTORS=$(bc \u0026lt;\u0026lt;\u0026lt; \u0026#34;${DATA_SIZE}/${SECTOR_SIZE}\u0026#34;) DATA_BLOCK_SIZE=128 LOW_WATER_MARK=32768 # Create a thin-pool device sudo dmsetup create \u0026#34;${POOL_NAME}\u0026#34; \\ --table \u0026#34;0 ${LENGTH_IN_SECTORS} thin-pool ${META_DEV} ${DATA_DEV} ${DATA_BLOCK_SIZE} ${LOW_WATER_MARK}\u0026#34; We can create a systemd service to run the above script on each reboot:\nsudo nano /lib/systemd/system/devmapper_reload.service The service file:\n[Unit] Description=Devmapper reload script [Service] ExecStart=/path/to/script/reload.sh [Install] WantedBy=multi-user.target Enable the newly created service:\nsudo systemctl daemon-reload sudo systemctl enable …","date":1648496009,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648496009,"objectID":"aa2c9507eaaaf7138e988535ddbed2e7","permalink":"/blog/kata-build-configure-fc/","publishdate":"2022-03-28T19:33:29Z","relpermalink":"/blog/kata-build-configure-fc/","section":"blog","summary":"Picking up from where we left in our previous post, we will now install AWS Firecracker and configure Kata Containers to use it as their hypervisor.\n","tags":["Kata Containers","Firecracker","AWS Firecracker","Containers","Hypervisor"],"title":"Kata Containers: Build and configure Firecracker","type":"blog"},{"authors":null,"categories":null,"content":"Picking up from where we left in our previous post, we will now install QEMU and configure Kata Containers to use QEMU as their hypervisor.\nBuild QEMU First, we need to build qemu-system for the CPU architecture of our host machine. Kata Containers provide scripts to manage the build of QEMU both for x86 and arm64 hosts. We will be using them to make sure that our QEMU installation is suitable for usage with Kata Containers.\nBuild QEMU for x86 We need to get the correct version of QEMU from the versions file:\nexport GOPATH=$(go env GOPATH) \u0026amp;\u0026amp; export GO111MODULE=off source ${GOPATH}/src/github.com/kata-containers/kata-containers/tools/packaging/scripts/lib.sh qemu_version=$(get_from_kata_deps \u0026#34;assets.hypervisor.qemu.version\u0026#34;) Next, we need to get the QEMU source from the matching branch:\ngo get -d github.com/qemu/qemu cd ${GOPATH}/src/github.com/qemu/qemu git checkout ${qemu_version} your_qemu_directory=${GOPATH}/src/github.com/qemu/qemu We need to see which version of QEMU we will be building.\necho ${qemu_version} # v6.2.0 Now we will use the scripts provided to apply the necessary patches to QEMU. Make sure to change the commands below to match the version of QEMU that you are targeting.\npackaging_dir=\u0026#34;${GOPATH}/src/github.com/kata-containers/kata-containers/tools/packaging\u0026#34; $packaging_dir/scripts/apply_patches.sh $packaging_dir/qemu/patches/6.2.x/ Once the patches are successfully applied, we need to install some apt packages that are required to build qemu-system:\nsudo apt install ninja-build pkg-config \\ libglib2.0-dev \\ libpixman-1-dev \\ libseccomp-dev \\ libcap-ng-dev \\ librados-dev \\ libpmem-dev \\ librbd-dev \\ libgcrypt-dev Now, we can finally build QEMU:\ncd $your_qemu_directory $packaging_dir/scripts/configure-hypervisor.sh kata-qemu \u0026gt; kata.cfg eval ./configure \u0026#34;$(cat kata.cfg)\u0026#34; make -j $(nproc) sudo -E make install If you followed the instructions in our previous post and you have set the PREFIX variable to /opt/kata the qemu-system-x86_64 binary will be stored under the /opt/kata/bin/ directory, tools/virtiofsd/virtiofsd under the /opt/kata/libexec/kata-qemu directory and all the other files will be stored under the /opt/kata/share/kata-qemu directory.\nWe can also install virtiofsd and qemu-system-x86_64 under the default location using the install_qemu.sh script.\ngo get -d github.com/kata-containers/tests script -fec \u0026#39;sudo -E ${GOPATH}/src/github.com/kata-containers/tests/.ci/install_qemu.sh\u0026#39; Build QEMU for aarch64 Install requirements:\nsudo apt-get install -y build-essential pkg-config \\ libglib2.0-dev libpixman-1-dev \\ libaio-dev libseccomp-dev \\ libcap-ng-dev librados-dev \\ librbd-dev libzstd-dev Set required environment variables\nexport PATH=$PATH:$(go env GOPATH)/bin \u0026amp;\u0026amp; \\ export GOPATH=$(go env GOPATH) \u0026amp;\u0026amp; \\ export GO111MODULE=off Build QEMU using the install_qemu.sh script:\nsudo rm -rf ${GOPATH}/src/github.com/qemu/qemu \u0026amp;\u0026amp; \\ go get -d github.com/kata-containers/tests \u0026amp;\u0026amp; \\ script -fec \u0026#39;sudo -E ${GOPATH}/src/github.com/kata-containers/tests/.ci/install_qemu.sh\u0026#39; Configure Kata Containers Next, we need to install the Kata Containers configuration file. We will use this file to configure Kata Containers to use the initrd image we built in our previous post. You could also use a rootfs image, if you prefer. We will also enable guest seccomp.\nsudo mkdir -p /opt/kata/configs sudo install -o root -g root -m 0640 /opt/kata/share/defaults/kata-containers/configuration.toml /opt/kata/configs # comment out the image entry sudo sed -i \u0026#39;s/^\\(image =.*\\)/# \\1/g\u0026#39; /opt/kata/configs/configuration.toml # enable seccomp sudo sed -i \u0026#39;/^disable_guest_seccomp/ s/true/false/\u0026#39; /opt/kata/configs/configuration.toml Make sure that /opt/kata/configs/configuration.toml has a initrd entry pointing to the initrd we created:\n17 | #image = \u0026#34;/opt/kata/share/kata-containers/kata-containers-image.img\u0026#34; 18 │ initrd = \u0026#34;/opt/kata/share/kata-containers/kata-containers-initrd.img\u0026#34; Configure containerd You need to edit the containerd config file (usually /etc/containerd/config.toml.\nFirst we need to remove cri from the disabled plugins list:\n#disabled_plugins = [\u0026#34;cri\u0026#34;] disabled_plugins = [] We also need to add the relevant handler for the kata runtime:\n[plugins] [plugins.cri] [plugins.cri.containerd] [plugins.cri.containerd.runtimes] [plugins.cri.containerd.runtimes.kata] runtime_type = \u0026#34;io.containerd.kata.v2\u0026#34; privileged_without_host_devices = true [plugins.cri.containerd.runtimes.kata.options] ConfigPath = \u0026#34;/opt/kata/configs/configuration.toml\u0026#34; Once the changes are saved, restart containerd: sudo systemctl restart containerd.\nTo verify our installation is successful, we can launch an Ubuntu test container:\nsudo ctr images pull docker.io/library/ubuntu:latest sudo ctr run --runtime io.containerd.run.kata.v2 -t --rm docker.io/library/ubuntu:latest ubuntu-kata-test uname -a # Linux e678ab3c6fc3 5.15.26 #2 SMP Sat Mar 19 21:02:56 UTC 2022 aarch64 aarch64 aarch64 GNU/Linux Note 1 for aarch64:\nWe noticed that, in some instances, trying to …","date":1648462686,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648462686,"objectID":"eb3c388dd53e6daa74a2a2fae3f23a4f","permalink":"/blog/kata-build-configure-qemu/","publishdate":"2022-03-28T10:18:06Z","relpermalink":"/blog/kata-build-configure-qemu/","section":"blog","summary":"Picking up from where we left in our previous post, we will now install QEMU and configure Kata Containers to use QEMU as their hypervisor.\n","tags":["Kata Containers","Containers","QEMU"," Aarch64","Hypervisor"],"title":"Kata Containers: Build and configure QEMU","type":"blog"},{"authors":null,"categories":null,"content":"Kata Containers enable containers to be seamlessly executed in Virtual Machines. Kata Containers are as light and fast as containers and integrate with the container management layers, while also delivering the security advantages of VMs. Kata Containers is the result of merging two existing open source projects: Intel Clear Containers and Hyper runV.\nKata Containers consist of several components. For amd64 machines, binaries are provided through the formal release process. However, for arm64, binary files are not available (just yet).\nIn this post, we will be going through the steps to build kata containers from source, both for amd64 and arm64 architectures. In follow-up posts, we go through the steps to build and configure QEMU and AWS Firecracker as VMMs for Kata Containers.\nInstall requirements To build Kata Containers we need to install Rust v1.58.1, Go v1.16.10, Docker and some apt/snap packages. The specific versions may change, so make sure to check the versions database.\nApt/Snap Packages: We need to install gcc, make and yq v3. containerd and runc are installed by the Docker install script, in the following steps.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y sudo apt install gcc make snapd -y sudo snap install yq --channel=v3/stable Rust (version 1.58.1): We will use rustup to install and set Rust 1.58.1 as our default toolchain:\ndown_dir=$(mktemp -d) pushd $down_dir wget -q https://images.rust-lang.org/rustup/dist/$(uname -p)-unknown-linux-gnu/rustup-init sudo chmod +x rustup-init ./rustup-init -q -y --default-toolchain 1.58.1 source $HOME/.cargo/env popd rm -rf $down_dir Go (version 1.16.10) We will download the appropriate Go binaries and add them to the PATH environment variable:\ndown_dir=$(mktemp -d) pushd $down_dir wget -q https://go.dev/dl/go1.16.10.linux-$(dpkg --print-architecture).tar.gz sudo mkdir -p /usr/local/go1.16 sudo tar -C /usr/local/go1.16 -xzf go1.16.10.linux-$(dpkg --print-architecture).tar.gz echo \u0026#39;export PATH=$PATH:/usr/local/go1.16/go/bin\u0026#39; \u0026gt;\u0026gt; $HOME/.profile source $HOME/.profile popd rm -rf $down_dir Docker We will install Docker using the provided convenience script:\nsudo apt-get remove docker docker-engine docker.io containerd runc -y \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 sudo rm -rf /var/lib/docker/ down_dir=$(mktemp -d) pushd $down_dir curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh # Optionally add user to docker group to run docker without sudo # sudo usermod -aG docker $USER popd rm -rf $down_dir Build Kata components Build kata-runtime First, we need to set the correct Go environment variables:\nexport PATH=$PATH:$(go env GOPATH)/bin \u0026amp;\u0026amp; \\ export GOPATH=$(go env GOPATH) \u0026amp;\u0026amp; \\ export GO111MODULE=off We will use go get to download kata-containers source code:\ngo get -d -u github.com/kata-containers/kata-containers We are now ready to build the kata-runtime:\npushd $GOPATH/src/github.com/kata-containers/kata-containers/src/runtime export GO111MODULE=on export PREFIX=/opt/kata make popd To install the binaries to a specific path (say /opt/kata) we need to specify the PREFIX environment variable prior to installing:\npushd $GOPATH/src/github.com/kata-containers/kata-containers/src/runtime export PREFIX=/opt/kata sudo -E PATH=$PATH -E PREFIX=$PREFIX make install popd Kata binaries are now installed in /opt/kata/bin and configs are installed in /opt/kata/share/defaults/kata-containers/.\nIt is recommended you add a symbolic link to /opt/kata/bin/kata-runtime and /opt/kata/bin/containerd-shim-kata-v2 in order for containerd to reach these binaries from the default system PATH.\nsudo ln -s /opt/kata/bin/kata-runtime /usr/local/bin sudo ln -s /opt/kata/bin/containerd-shim-kata-v2 /usr/local/bin Create a rootfs \u0026amp; initrd image We can use either a rootfs or initrd image to launch Kata Containers with Qemu. However, AWS Firecracker does not work with initrd images, so we will be using a rootfs image for Kata with Firecracker. If you do not want to use QEMU or QEMU with initrd, you can skip building the initrd image.\nCreate the rootfs base image:\nexport ROOTFS_DIR=${GOPATH}/src/github.com/kata-containers/kata-containers/tools/osbuilder/rootfs-builder/rootfs cd $GOPATH/src/github.com/kata-containers/kata-containers/tools/osbuilder/rootfs-builder # you may change the distro (in this case we used ubuntu). to get supported distros list, run ./rootfs.sh -l script -fec \u0026#39;sudo -E GOPATH=$GOPATH AGENT_INIT=yes USE_DOCKER=true ./rootfs.sh ubuntu\u0026#39; Note for arm64:\nWe noticed that in some instances the kata-agent compilation failed. A possible workaround was to remove the USE_DOCKER variable. This requires qemu-img command to be available on your system. You can install it with sudo apt install -y qemu-utils.\nexport ROOTFS_DIR=\u0026#34;${GOPATH}/src/github.com/kata-containers/kata-containers/tools/osbuilder/rootfs-builder/rootfs\u0026#34; sudo rm -rf ${ROOTFS_DIR} cd $GOPATH/src/github.com/kata-containers/kata-containers/tools/osbuilder/rootfs-builder script -fec \u0026#39;sudo -E GOPATH=$GOPATH AGENT_INIT=yes ./rootfs.sh …","date":1648117086,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648117086,"objectID":"0268b8596661b1e23013e2b21530126d","permalink":"/blog/kata-build-source/","publishdate":"2022-03-24T10:18:06Z","relpermalink":"/blog/kata-build-source/","section":"blog","summary":"Kata Containers enable containers to be seamlessly executed in Virtual Machines. Kata Containers are as light and fast as containers and integrate with the container management layers, while also delivering the security advantages of VMs. Kata Containers is the result of merging two existing open source projects: Intel Clear Containers and Hyper runV.\n","tags":["Kata Containers","Docker","QEMU","AWS Firecracker","Containers","arm64"],"title":"Build Kata Containers from source on x86 and arm64","type":"blog"},{"authors":null,"categories":null,"content":"This is the first of a number of posts regarding the orchestration, deployment and scaling of containerized applications in VM sandboxes using kubernetes, kata-containers and AWS Firecracker microVMs. We have gathered some notes during the installation and configuration of the necessary components and we thought they might be useful to the community, especially with regards to the major pain points in trying out recent open-source projects and technologies.\nAbout Orchestration, the Edge, and Kata Containers To manage and orchestrate containers in a cluster, the community is using kubernetes (k8s), a powerful, open-source system for automating the deployment, scaling and management of containerized applications. To accommodate the vast majority of options and use-cases, k8s supports a number of container runtimes: the piece of software that sets up all the necessary components in the system to run the containerized application. For more information on the container runtime support or k8s visit the official documentation.\nk8s at the edge A stripped down version of k8s is available through K3s, a kubernetes distribution designed for production workloads in unattended, resource-constrained, remote locations or inside IoT appliances. K3s is packaged as a single \u0026lt;40MB binary that reduces the dependencies and steps needed to install, run and auto-update a production Kubernetes cluster. K3s supports various architectures (amd64, ARMv8, ARMv8) with binaries and multiarch images available for all. K3s works great from something as small as a Raspberry Pi to an AWS a1.4xlarge 32GiB server. K3s is also great for trying out k8s on a local machine without messing up the OS.\ncontainer sandboxing Kata Containers enable containers to be seamlessly executed in Virtual Machines. Kata Containers are as light and fast as containers and integrate with the container management layers, while also delivering the security advantages of VMs. Kata Containers is the result of merging two existing open source projects: Intel Clear Containers and Hyper runV.\nKata Containers integrate with k8s easily; however, there are some (minor) pain points when adding more options to the mix. For instance, choosing AWS Firecracker as the VMM for the sandbox environment, brings a storage backend dependency: device mapper. In this post, we will be going through the steps needed to setup kata containers with Firecracker, focusing on the device mapper setup for k8s and k3s.\nKata containers and containerd First, lets start of by installing kata containers and add the relevant handler to containerd. Following the docs is pretty straightforward, especially this one. To make sure everything is working as expected, you could try out a couple of examples found here.\nIn short, the needed steps are:\ninstall kata binaries Download a release from: https://github.com/kata-containers/kata-containers/releases\n(using v2.1.1, released in June 2021):\n$ wget https://github.com/kata-containers/kata-containers/releases/download/2.1.1/kata-static-2.1.1-x86_64.tar.xz Unpack the binary\n$ xzcat kata-static-2.1.1-x86_64.tar.xz | sudo tar -xvf - -C / by default, kata is being installed in /opt/kata. Check the installed version by running:\n$ /opt/kata/bin/kata-runtime --version It should output something like the following:\n$ /opt/kata/bin/kata-runtime --version kata-runtime : 2.1.1 commit : 0e2be438bdd6d213ac4a3d7d300a5757c4137799 OCI specs: 1.0.1-dev It is recommended you add a symbolic link to /opt/kata/bin/kata-runtime and /opt/kata/bin/containerd-shim-kata-v2 in order for containerd to reach these binaries from the default system PATH.\n$ sudo ln -s /opt/kata/bin/kata-runtime /usr/local/bin $ sudo ln -s /opt/kata/bin/containerd-shim-kata-v2 /usr/local/bin install containerd To install containerd, you can grab a release from https://github.com/containerd/containerd/releases or use the package manager of your distro:\n$ sudo apt-get install containerd A fairly recent containerd version is recommended (e.g. we’ve only tested with containerd versions v1.3.9 and above).\nAdd the kata configuration to containerd’ config.toml (/etc/containerd/config.toml):\n[plugins] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd] default_runtime_name = \u0026#34;kata\u0026#34; [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes] [plugins.\u0026#34;io.containerd.grpc.v1.cri\u0026#34;.containerd.runtimes.kata] runtime_type = \u0026#34;io.containerd.kata.v2\u0026#34; If /etc/containerd/config.toml is not present, create it using the following command:\n$ sudo containerd config default \u0026gt; /etc/containerd/config.toml and add the above snippet to the relevant section.\ntest kata with containerd Now, after we restart containerd:\nsudo systemctl restart containerd we should be able to launch a Ubuntu test container using kata containers and containerd:\n$ sudo ctr image pull docker.io/library/ubuntu:latest docker.io/library/ubuntu:latest: resolved |++++++++++++++++++++++++++++++++++++++| …","date":1625822044,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625822044,"objectID":"9a1e26aa0001a926d8f04cd0cea33fff","permalink":"/blog/kata-fc-k3s-k8s/","publishdate":"2021-07-09T10:14:04+01:00","relpermalink":"/blog/kata-fc-k3s-k8s/","section":"blog","summary":"This is the first of a number of posts regarding the orchestration, deployment and scaling of containerized applications in VM sandboxes using kubernetes, kata-containers and AWS Firecracker microVMs. We have gathered some notes during the installation and configuration of the necessary components and we thought they might be useful to the community, especially with regards to the major pain points in trying out recent open-source projects and technologies.\n","tags":["Kata Containers","Kubernetes","AWS Firecracker","Containers","K3s","MicroVMs"],"title":"Running containers on Firecracker microVMs using kata on kubernetes","type":"blog"},{"authors":[""],"categories":null,"content":" Nubificus LTD participates in the ICT-40-2020 SERRANO Project (H2020 GA No 101017168): Transparent Application Deployment in a Secure, Accelerated and Cognitive Cloud Continuum, which kicked off in January 2021 and will last three years. SERRANO consortium consists of 11 partners from industry and academia. SERRANO investigates the transparent deployment of applications in a secure and accelerated infrastructure of edge, cloud and HPC resources, based on FPGAs, GPUs, Virtual Platforms and Smart NICs, while facilitating their automated and cognitive orchestration.\nSERRANO investigates the transparent deployment of applications in a secure and accelerated infrastructure of edge, cloud and HPC resources, based on FPGAs, GPUs, Virtual Platforms and Smart NICs, while facilitating their automated and cognitive orchestration.\nNubificus LTD will lead the development of lightweight mechanisms to enable workload isolation and trusted execution in multi-tenancy nodes, hardware acceleration abstractions for serverless workloads and the development of resource orchestration and lightweight virtualization mechanisms.\nSERRANO website: https://ict-serrano.eu Twitter: https://twitter.com/ProjectSerrano LinkedIn: https://www.linkedin.com/company/serrano-project Researchgate: https://www.researchgate.net/project/SERRANO-Transparent-Application-Deployment-in-a-Secure-Accelerated-and-Cognitive-Cloud-Continuum ","date":1620691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620691200,"objectID":"2e2d44c47ab4189ee53084a295efded5","permalink":"/post/2021-05-11-nubificus-ltd-enables-efficient-secure-execution-at-the-edge/","publishdate":"2021-05-11T00:00:00Z","relpermalink":"/post/2021-05-11-nubificus-ltd-enables-efficient-secure-execution-at-the-edge/","section":"post","summary":" Nubificus LTD participates in the ICT-40-2020 SERRANO Project (H2020 GA No 101017168): Transparent Application Deployment in a Secure, Accelerated and Cognitive Cloud Continuum, which kicked off in January 2021 and will last three years. SERRANO consortium consists of 11 partners from industry and academia. SERRANO investigates the transparent deployment of applications in a secure and accelerated infrastructure of edge, cloud and HPC resources, based on FPGAs, GPUs, Virtual Platforms and Smart NICs, while facilitating their automated and cognitive orchestration.\n","tags":["news"],"title":"Nubificus LTD enables efficient \u0026 secure execution at the edge","type":"post"},{"authors":["Aristotelis Kretsis","Panagiotis Kokkinos","Polyzois Soumplis","Juan Jose Vegas Olmos","Marcell Fehér","Márton Sipos","Daniel E Lucani","Dmitry Khabi","Dimosthenis Masouros","Kostas Siozios"," others"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"8452625f717a55929e9294f6fe4c03a5","permalink":"/publication/kretsis-2021-serrano/","publishdate":"2025-10-15T23:59:22.755332Z","relpermalink":"/publication/kretsis-2021-serrano/","section":"publication","summary":"","tags":null,"title":"SERRANO: transparent application deployment in a secure, accelerated and cognitive cloud continuum","type":"publication"},{"authors":null,"categories":null,"content":"In our previous post we spoke about the potential solutions for deploying serverless offerings with hardware acceleration support. With the increasing adoption of the serverless and FaaS paradigms, providers will need to offer some form of hardware acceleration semantics.\nFor some time now, Amazon has identified this as a “compelling use case” for their AWS Firecracker hypervisor which powers the Amazon Lambda service. What is more, they identify traditional techniques for GPU support in VMs such as GPU passthrough comes with limitations and significantly increases the attack surface of the hypervisor.\nAs an alternative to passing through the accelerator device inside the guest, paravirtual interfaces can expose hardware acceleration capabilities inside the VM with minimal overhead and offering a simple user interface for offloading code to the host for acceleration.\nIn fact, such interfaces already exist. virtio-crypto is an example, where the guest VM uses a simple crypto API while the actual computation is offloaded, through the paravirtual driver, to the host.\nWe believe that the same paradigm can be applied to any kind of computation that can benefit from acceleration. Whether that is crypto operations, Machine Learning or linear algebra operators, the workflow from the point of view of the developer these days is the same; You will use a framework such as cryptodev, Jetson Inference or the BLAS library, to write your applications and you will not deal with the low-level complexities of the actual accelerator. Moreover, that workflow should not be different whether your application runs on baremetal or inside a VM.\nIn the rest of this post we present vAccel, an acceleration framework that enables portable and hardware agnostic acceleration for cloud and edge applications.\nvAccel design In simple terms, vAccel is an acceleration API. It offers support for a set of operators that commonly use hardware acceleration to increase performance, such as machine learning and linear algebra operators.\nThe API is implemented by VaccelRT a thin and efficient runtime system that links against the user application and is responsible for dispatching operations to the relevant hardware accelerators. The interaction with the hardware itself is mediated by plugins which implement the API for the specific hardware accelerator.\nThis design is driven by our requirements for high degree of portability, an application that consumes the vAccel API can run without modification or even re-compilation to any platform for which there is suitable back-end plugin.\nIn fact, this opens up the way to enable the vAccel API inside a VM guest. The missing bits are a virtio driver that implements the vAccel API and a backend plugin that speaks with the virtio device. Once you have this components in place, you can run your existing vAccel application inside a VM, just by using the virtio-plugin at runtime.\nVaccelRT vAccel support in AWS Firecracker Once we implemented the frontend vAccel-virtio driver and virtio plugin for VaccelRT, we need a hypervisor to test this on. We already showed, in the previous post some initial vAccel results with QEMU as the target hypervisor. In this post, we will focus on AWS Firecracker.\nAWS Firecracker has been designed having in mind really small boot times and small attack surface, which makes it a compelling choice for cloud and edge deployments. Moreover, it powers up Lambda, Amazon’s serverless platform, which we see as a paradigm for which vAccel’s hardware abstraction level is a perfect fit.\nAWS Firecracker already implements virtio backend drivers for net, block and vsock. That was good news for us, we have all the required virtio machinery. All we had to do, was add a new device for vAccel and link the hypervisor with VaccelRT.\nThe last bit required us to create the necessary Rust bindings for calling C from AWS Firecracker which is written in Rust. This was actually a good exercise for us, since we plan to anyway provide bindings for the vAccel API in more high-level languages.\nWith all the components in place our stack looks like this:\nvaccel-e2e The user application is consumes the vAccel API and links against VaccelRT. Inside the VM the application uses the vAccel-virtio backend plugin. When a vAccel function is called, the plugin will offload the request to /dev/accel device which is the interface of the virtio driver. Next, the virtio driver will forward the request to the vAccel-enabled AWS Firecracker instance which will the host-residing VaccelRT. Finally, in the host side, VaccelRT will use one of the available plugins, to execute the operation on the hardware accelerator.\nBut how does this perform?\nWe first grabbed a copy of jetson-inference, a rich repo full of ML inference models and example applications based on TensorRT. We patched it to be able to run on an x86 GPU (we had an NVIDIA RTX 2060 SUPER handy), and we built the vAccelRT backend for an image classification operation. To …","date":1607087644,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607087644,"objectID":"f3941a44f94156d560cff458014acee9","permalink":"/blog/vaccel_v2/","publishdate":"2020-12-04T14:14:04+01:00","relpermalink":"/blog/vaccel_v2/","section":"blog","summary":"In our previous post we spoke about the potential solutions for deploying serverless offerings with hardware acceleration support. With the increasing adoption of the serverless and FaaS paradigms, providers will need to offer some form of hardware acceleration semantics.\n","tags":["vAccel","AWS Firecracker","VirtIO","Serverless"],"title":"Hardware acceleration in the Age of Functions (vol II)","type":"blog"},{"authors":[""],"categories":null,"content":"Today, Nubificus LTD introduced vAccel support on AWS Firecracker, opening the door for enabling hardware acceleration for serverless computing.\nWith a slim design and precise abstractions, vAccel semantically exposes hardware acceleration features to users with little to no knowledge of software acceleration framework internals.\nServerless computing workflows are now able to enjoy compute-offload mechanisms to provide AI/ML services as functions, triggered thousands or millions of times by events, on-demand, auto-scaling to multiple physical and virtual nodes.\nThe core of vAccel is the runtime system, essentially a library, that translates complicated compute frameworks to meaningful functions that users can directly call. These frameworks, commonly used on hardware accelerators, such as TensorRT, Tensorflow, Jetson-inference, or even lower-level abstractions such as CUDA, OpenCL, OpenACC, are now easily usable by the end-user via vAccel. Moreover, vAccel offers a virtualization backend, facilitating the execution of functions on Virtual Machines. Apart from support for AWS Firecracker, QEMU/KVM support is also available.\nEnd-users can get a sneak peek at what vAccel on AWS Firecracker has to offer on the project’s website, on github, or on-prem, by using the distributed binaries, or just a container image. To get started, follow one of the tutorials currently available at https://blog.cloudkernels.net\n","date":1607079600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607079600,"objectID":"c6d101552bd032b7bc171db32e1fef66","permalink":"/post/2020-12-04-110000+0000-vaccel-now-available-on-firecracker/","publishdate":"2020-12-04T11:00:00Z","relpermalink":"/post/2020-12-04-110000+0000-vaccel-now-available-on-firecracker/","section":"post","summary":"Today, Nubificus LTD introduced vAccel support on AWS Firecracker, opening the door for enabling hardware acceleration for serverless computing.\n","tags":["news"],"title":"vAccel now available on Firecracker","type":"post"},{"authors":null,"categories":null,"content":"The debate on how to deploy applications, monoliths or micro services, is in full swing. Part of this discussion relates to how the new paradigm incorporates support for accessing accelerators, e.g. GPUs, FPGAs. That kind of support has been made available to traditional programming models the last couple of decades and its tooling has evolved to be stable and standardized.\nOn the other hand, what does it mean for a serverless setup to access an accelerator? Should the function invoked to classify an image, for instance, link against the whole acceleration runtime and program the hardware device itself? It seems quite counter-intuitive to create such bloated functions.\nThings get more complicated when we consider the low-level layers of the service architecture. How does the system itself get access to the acceleration hardware? Docker allows exposing a GPU device inside a container for some time now, so serverless systems based on top of it can expose GPU devices to running functions. Virtual Machine-based setups rely on the monitor, e.g. QEMU or Firecracker, to expose acceleration devices to the guest.\nThere are several techniques used to expose a device from the host to a guest VM. Passthrough mode exposes the hardware accelerator as is inside the guest. This mode provides native performance using the accelerator from inside the VM, however it does cause issues with sharing the device across multiple VMs. API remoting, e.g. rCUDA, is another option, where requests are being forwarded to the accelerator device over the network. Finally, there is the option of paravirtual interfaces where the monitor exposes a generic device to the guest, with a very simple API. Applications in the guest send requests to the paravirtual device which are then passed to the hypervisor and dispatched by the latter to an accelerator device on the host.\nVirtIO drivers are an example of such paravirtualized frameworks. VirtIO exposes simple front-end device drivers to the guest, rather than emulating complex devices and offloads the complexity of interacting with the hardware to the back-end that lives in the Virtual Machine Monitor (VMM).\nvirtio-crypto One of the devices described in the VirtIO spec is the virtio-crypto device. The guest chooses the cryptographic operation to perform and passes a pointer to the data that will be manipulated. The actual operation is offloaded through the VMM to the host crypto acceleration device.\nA VM is able to use a crypto device by using a combination of cryptodev and virtio-crypto. Requests for encryption / decryption originating from the VM, get forwarded to the backend, get injected to the cryptodev device and end up being handled by the host Linux kernel. Figure 1 presents an overview of the virtio-crypto architecture.\nFigure 1: VirtIO-crypto architecture overview In the context of micro-services (FaaS/Serverless) cryptographic operations are quite common, presented to the user as language/library abstractions. Integrating an off-loading mechanism of these CPU-intensive operations seems like an interesting optimization. To showcase the potential of paravirtual accelerated devices, we implemented a virtio-crypto backend driver for AWS Firecracker. Since virtio-crypto’s frontend is already present in the Linux kernel, all we had to do is implement the corresponding back-end in the Firecracker code base. This effort was relatively straight-forward since Firecracker already provides a number of VirtIO devices, e.g. net and block, which means that all the machinery for communication with the guest was in place.\nFigure 2 shows the performance our virtio-crypto driver achieves (light bars) compared to running the computation in the guest kernel using the cryptodev-linux driver (dark bars), when running the AES-CBC cipher. Unfortunately, we have not been able to get our hands on a crypto acceleration device, so virtio-crypto is using the same cryptodev-linux device in the host (the CPU). This means that we do not actually accelerate the operation, but our experiment is quite useful to see the VirtIO overhead of offloading the operation to the host. As expected, the larger the block size of the blob we are encrypting, the better we are able to hide the cost of moving data from the userland of the guest to the kernel of the host.\nFigure 2: Host and Guest Throughput for AES-CBC-128 vs chunk size This is encouraging; once there is a hardware accelerator for computation, acceleration capabilities are automatically exposed inside a Firecracker VM in a secure way with reasonably low overhead. Which inevitably leads us to the thought, why only crypto? The virtio-crypto example showcases a simple interface through which we can achieve hardware acceleration, so why not generalize this to other types of acceleration?\nThis gave us the idea to define a simple, hardware-agnostic API to accelerate any operation, as long as the host supports it. We believe that an API at this granularity is the right …","date":1591043357,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591043357,"objectID":"7daa87280e6e200fa7e766c6a279afeb","permalink":"/blog/vaccel/","publishdate":"2020-06-01T20:29:17Z","relpermalink":"/blog/vaccel/","section":"blog","summary":"The debate on how to deploy applications, monoliths or micro services, is in full swing. Part of this discussion relates to how the new paradigm incorporates support for accessing accelerators, e.g. GPUs, FPGAs. That kind of support has been made available to traditional programming models the last couple of decades and its tooling has evolved to be stable and standardized.\n","tags":["vAccel","VirtIO","QEMU","Serverless"],"title":"Hardware acceleration in the Age of Functions","type":"blog"},{"authors":null,"categories":null,"content":"Earlier this month we visited FOSDEM, an absolutely open and free event for developers, open-source vendors and enthusiasts to meet, share their ideas and news, and discuss the latest in open source. Talks at FOSDEM are usually organized within several sections: Keynotes, Main tracks, Developer rooms and Lightning talks.\nSome people from our team had visited before, but for most of us first timers it was really exciting. Packed keynotes, busy dev rooms, people chatting outside, over coffee, beer, or snacks!\nThe conference was held in ULB Solbosch Campus, a very old, but also very modern student campus. The conference itself is free entry, without any registration necessary, so naturally, everyone interested can easily attend. Regarding food and drink at FOSDEM, there are few coffee shops inside the campus, and there’s always a few stands there serving coffee, beer, sausages, fries, and of course, Belgian waffles of all flavors, shapes and sizes.\nWe attended a couple of main track talks and keynotes which we found super interesting, but our heart belongs to the devrooms :D We were overwhelmed by the diversity of topics covered, especially with regards to containers, their ecosystem and related technologies: Container devroom, SDN or SDS devrooms, Microkernels or Virtualization. During the sessions, as well as during the breaks and evenings we had the opportunity to chat with interesting people, working on exciting projects.\nOur contribution to FOSDEM this year was a talk on lightweight virtualization, focusing on a simple hypervisor we’ve crafted, and our upstreaming effort for running Amazon’s Firecracker on a Raspberry Pi 4. People found it quite interesting, and we’re glad it stimulated insightful discussions for feedback, as well as next steps.\nThe event was quite big; attendees were estimated to be 5500 this year, whereas last year they were estimated to be around 6000. Network traffic statistics are available here and a detailed analysis here.\nAll in all it was a great experience, we loved talking to people about interesting projects, and we are super excited to showcase some of the things we’ve got piled up for next year! Brussels is a fun place to be, let alone the fine beer available from local and international breweries!\nLooking forward to FOSDEM 2021! ","date":1582747838,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582747838,"objectID":"5435411977d172cac36a2cbf46613823","permalink":"/blog/fosdem-2020/","publishdate":"2020-02-26T20:10:38Z","relpermalink":"/blog/fosdem-2020/","section":"blog","summary":"Earlier this month we visited FOSDEM, an absolutely open and free event for developers, open-source vendors and enthusiasts to meet, share their ideas and news, and discuss the latest in open source. Talks at FOSDEM are usually organized within several sections: Keynotes, Main tracks, Developer rooms and Lightning talks.\n","tags":["FOSDEM","Event","Developers","Containers","Firecracker","Raspberry Pi","Microkernels","Virtualization"],"title":"Fosdem 2020","type":"blog"},{"authors":null,"categories":null,"content":"Spawning applications in the cloud has been made super easy using container frameworks such as docker. For instance running a simple command like the following\ndocker run --rm -v /path/to/nginx-files:/etc/nginx nginx spawns an NGINX web server, provided you customize config files and the actual HTML files to be served.\nThis process, inherits NGINX’s stock docker hub rootfs, and spawns it as a docker container in a generic Linux container host.\nHowever, what happens when someone wants to create the absolute minimum rootfs for such a process?\nIn what follows we describe two straightforward approaches that came up handy when playing with firecraker and qemu rootfs images on low-power devices, where resources are scarce (memory footprint, storage footprint, bw etc.).\nOption 1: base the rootfs on a minimal linux container distro The firecracker guide provides a guide on how to build a rootfs based on Alpine Linux. We will re-use the same steps for building our rootfs, but to spice it up we will create a rootfs that upon boot runs immediately nginx.\nWe will use the following Dockerfile to build nginx:\nFROM alpine:latest MAINTAINER Babis Chalios \u0026lt;mail@bchalios.io\u0026gt; ENV NGINX_VERSION nginx-1.17.4 WORKDIR /build RUN apk --update add openssl-dev pcre-dev zlib-dev wget build-base \u0026amp;\u0026amp; \\ wget http://nginx.org/download/${NGINX_VERSION}.tar.gz \u0026amp;\u0026amp; \\ tar -zxvf ${NGINX_VERSION}.tar.gz \u0026amp;\u0026amp; \\ cd ${NGINX_VERSION} \u0026amp;\u0026amp; \\ ./configure \\ --with-http_ssl_module \\ --with-http_gzip_static_module \\ --prefix=/etc/nginx \\ --http-log-path=/var/log/nginx/access.log \\ --error-log-path=/var/log/nginx/error.log \\ --sbin-path=/usr/local/sbin/nginx \u0026amp;\u0026amp; \\ make \u0026amp;\u0026amp; \\ make install \u0026amp;\u0026amp; \\ apk del build-base \u0026amp;\u0026amp; \\ rm -rf /tmp/src \u0026amp;\u0026amp; \\ rm -rf /var/cache/apk/* WORKDIR / We build nginx:\n$ docker build -t build_static_nginx and use the container to build our rootfs:\n$ dd if=/dev/zero of=nginx_fc.ext4 bs=1M count=100 $ mkfs.ext4 nginx_fc.ext4 $ mkdir mnt $ sudo mount nginx_fc.ext4 mnt $ docker --rm -ti -v $(pwd)/mnt:/my-rootfs build_static_nginx and in the container:\n# for d in bin etc lib root sbin usr; do tar c \u0026#34;/$d\u0026#34; | tar x -C /my-rootfs; done # for dir in dev proc run sys var; do mkdir /my-rootfs/${dir}; done # exit As the final step, we will substitute the /sbin/init binary of our rootfs with the following script which launches the nginx binary:\n$ cat nginx_init #!/bin/sh mkdir -p /var/log/nginx # the init process should never exit /usr/local/sbin/nginx -g \u0026#34;daemon off;\u0026#34; $ sudo mv mnt/sbin/init mnt/sbin/init.old $ sudo cp nginx_init mnt/sbin/init $ sudo chmod a+x mnt/sbin/init $ sudo umount mnt Option 2: ditch the rootfs, replace /sbin/init with the application Well, this is more of a hack than an actual solution, but comes in handy when memory and storage space is critical.\nBased on an excellent tutorial by Rob Landley, we come up with a single kernel image that spawns an NGINX web server.\nStep 1: Build the application as a static binary There are a couple of tutorials on how to build NGINX statically. We followed this one. After successful compilation, we come up with the following files/dirs:\nobjs/nginx conf/ html/ Step 2: generate the list of files/dirs for initramfs To embed all files in the Linux kernel build, we use the initramfs option in the kernel config, where the kernel build process generates the cpio archive to be appended to the kernel binary, and used as initramfs.\nFirst step is to create a file that contains the list of all files to be included in the cpio archive:\n$ cat \u0026gt;\u0026gt; initramfs_list \u0026lt; EOF dir /dev 755 0 0 nod /dev/console 644 0 0 c 5 1 nod /dev/loop0 644 0 0 b 7 0 dir /bin 755 1000 1000 dir /proc 755 0 0 dir /sys 755 0 0 dir /mnt 755 0 0 file /init usr/nginx 755 0 0 dir /opt 755 0 0 dir /opt/nginx 755 0 0 dir /opt/nginx/logs 755 0 0 dir /opt/nginx/conf 755 0 0 dir /opt/nginx/html 755 0 0 file /opt/nginx/conf/fastcgi.conf usr/conf/fastcgi.conf 644 0 0 file /opt/nginx/conf/fastcgi_params usr/conf/fastcgi_params 644 0 0 file /opt/nginx/conf/koi-utf usr/conf/koi-utf 644 0 0 file /opt/nginx/conf/koi-win usr/conf/koi-win 644 0 0 file /opt/nginx/conf/mime.types usr/conf/mime.types 644 0 0 file /opt/nginx/conf/nginx.conf usr/conf/nginx.conf 644 0 0 file /opt/nginx/conf/scgi_params usr/conf/scgi_params 644 0 0 file /opt/nginx/conf/uwsgi_params usr/conf/uwsgi_params 644 0 0 file /opt/nginx/conf/win-utf usr/conf/win-utf 644 0 0 file /opt/nginx/html/index.html usr/html/index.html 644 0 0 EOF We then place all necessary files at the usr/ dir in the linux kernel build dir.\nStep 3: build the kernel $ git clone https://github.com/torvalds/linux.git $ cd linux $ git checkout v4.14 $ wget https://raw.githubusercontent.com/firecracker-microvm/firecracker/b1e48beaea7b917fef1e4846f1d75a2c1a136517/resources/microvm-kernel-aarch64-config -O .config $ make oldconfig In this step, we can select any kernel version we want, starting from version 4.14, e.g. git checkout v4.20.\nThe only thing left is to add the option CONFIG_INITRAMFS_SOURCE=\u0026#34;usr/initramfs_list\u0026#34; to …","date":1571648571,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571648571,"objectID":"3241ddbbe61385fda6f6b546e3097834","permalink":"/blog/fc-rootfs/","publishdate":"2019-10-21T10:02:51+01:00","relpermalink":"/blog/fc-rootfs/","section":"blog","summary":"Spawning applications in the cloud has been made super easy using container frameworks such as docker. For instance running a simple command like the following\n","tags":["Firecracker","MicroVMs","Docker","Rootfs","NGINX"],"title":"Build a single-app rootfs for Firecracker MicroVMs","type":"blog"},{"authors":null,"categories":null,"content":"Since we got our hands on the new Raspberry Pi 4, we started exploring how various virtualization technologies behave on the board. First thing we tried is how to run Nabla on it and how it compares to native KVM.\nNext thing we wanted to try is firecracker, the notorious micro-VMM that Amazon Lambda \u0026amp; Fargate run on. To our disappointment, firecracker was not yet running on RPi4. So we started looking into coding in the necessary changes :)\nAfter a bit of investigation, we figured out that the key missing piece is support for the GICv2 ARM interrupt controller. In fact, firecracker only supports GIC version 3 since it was open-sourced, but not version 2, which is the version appearing in the Raspberry Pi series, as well as in other popular boards, like the Hikey 970 we got our hands on, or Khadas VIM3. A bit more digging into the internals of firecracker and the Linux kernel, helped us work out the details and open a pull-request which adds support for the missing parts.\nChanges in Firecracker The Generic Interrupt Controller (GIC) is the IP block in the ARM processors that implements interrupt handling. The Linux kernel supports user- and kernel-space emulation for GICv2 as well as GICv3 and v4. However, Firecracker only handles the GICv3-related configuration of the virtual GIC (VGIC). Similarly, setting up the FDT for the guest microVM from Firecracker only handles GIC-v3 devices.\nIn terms of code organization, the GIC related code currently exposes a function for setting up GICv3 performing the corresponding ioctl KVM commands. The first part of our PR changes this by introducing a GIC Trait which defines the common interface for all GIC implementations. Even though it is still under discussion, what exactly will be in the Trait in its final form, it will be something along the following lines:\n/// Trait for GIC devices. pub trait GICDevice { /// Returns the GIC version of the device fn version(\u0026amp;self) -\u0026gt; u32; /// Returns the file descriptor of the GIC device fn device_fd(\u0026amp;self) -\u0026gt; \u0026amp;DeviceFd; /// Returns an array with GIC device properties fn device_properties(\u0026amp;self) -\u0026gt; \u0026amp;[u64]; } With this in place, we can define objects per GIC version, which implement this Trait, and still have the rest of the code deal with the GICDevice Trait, which is transparent to the GIC version.\nThe implementations for each version, implement the Trait and a new function which is used to create the new object:\npub fn new(vm: \u0026amp;VmFd, vcpu_count: u64) -\u0026gt; Result\u0026lt;Box\u0026lt;GICDevice\u0026gt;\u0026gt; The differentiation between the two versions lays in the VGIC register attributes that each device exposes. As a result, the work we need to do is essentially to mmap their addresses.\nGICv2 relevant addresses include the distributor and cpu groups:\n/* Setting up the distributor attribute. We are placing the GIC below 1GB so we need to substract the size of the distributor. */ set_device_attribute( \u0026amp;vgic_fd, kvm_bindings::KVM_DEV_ARM_VGIC_GRP_ADDR, u64::from(kvm_bindings::KVM_VGIC_V2_ADDR_TYPE_DIST), \u0026amp;get_dist_addr() as *const u64 as u64, 0, )?; /* Setting up the CPU attribute. */ set_device_attribute( \u0026amp;vgic_fd, kvm_bindings::KVM_DEV_ARM_VGIC_GRP_ADDR, u64::from(kvm_bindings::KVM_VGIC_V2_ADDR_TYPE_CPU), \u0026amp;get_cpu_addr() as *const u64 as u64, 0, )?; whereas the GICv3 includes the distributor and redistributor groups:\n/* Setting up the distributor attribute. We are placing the GIC below 1GB so we need to substract the size of the distributor. */ set_device_attribute( \u0026amp;vgic_fd, kvm_bindings::KVM_DEV_ARM_VGIC_GRP_ADDR, u64::from(kvm_bindings::KVM_VGIC_V3_ADDR_TYPE_DIST), \u0026amp;get_dist_addr() as *const u64 as u64, 0, )?; /* Setting up the redistributors\u0026#39; attribute. We are calculating here the start of the redistributors address. We have one per CPU. */ set_device_attribute( \u0026amp;vgic_fd, kvm_bindings::KVM_DEV_ARM_VGIC_GRP_ADDR, u64::from(kvm_bindings::KVM_VGIC_V3_ADDR_TYPE_REDIST), \u0026amp;get_redists_addr(u64::from(vcpu_count)) as *const u64 as u64, 0, )?; Finally, for both versions of GIC we finalize the device initialization by setting the number of supported interrupts and the control_init group.\n/// Finalize the setup of a GIC device pub fn finalize_device(fd: \u0026amp;DeviceFd) -\u0026gt; Result\u0026lt;()\u0026gt; { /* We need to tell the kernel how many irqs to support with this vgic. * See the `layout` module for details. */ let nr_irqs: u32 = super::layout::IRQ_MAX - super::layout::IRQ_BASE + 1; let nr_irqs_ptr = \u0026amp;nr_irqs as *const u32; set_device_attribute( fd, kvm_bindings::KVM_DEV_ARM_VGIC_GRP_NR_IRQS, 0, nr_irqs_ptr as u64, 0, )?; /* Finalize the GIC. * See https://code.woboq.org/linux/linux/virt/kvm/arm/vgic/vgic-kvm-device.c.html#211. */ set_device_attribute( fd, kvm_bindings::KVM_DEV_ARM_VGIC_GRP_CTRL, u64::from(kvm_bindings::KVM_DEV_ARM_VGIC_CTRL_INIT), 0, 0, )?; Ok(()) } Regarding the FDT creation, there are differences between v2 and v3 regarding the interrupt controller intc node.\nFirst, we need to define the GICv2 compatible property to be arm,gic-400, since it is the GIC-400 chip which …","date":1571644711,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571644711,"objectID":"3db308c99747ad92f9b214482636b60a","permalink":"/blog/firecracker-rpi4/","publishdate":"2019-10-21T09:58:31+02:00","relpermalink":"/blog/firecracker-rpi4/","section":"blog","summary":"Since we got our hands on the new Raspberry Pi 4, we started exploring how various virtualization technologies behave on the board. First thing we tried is how to run Nabla on it and how it compares to native KVM.\n","tags":["Raspberry Pi","Firecracker","GIC"],"title":"Porting Firecracker to a Raspberry Pi 4","type":"blog"},{"authors":null,"categories":null,"content":"Given the traction our previous post got, we thought we should jot down the steps to build a 64-bit bootable image for a RPi4. The distro we’re most familiar with is Debian, so we’ll go with a debian-like distro like Ubuntu. If you don’t feel like playing with kernel compilation and FS images, just grab the binary and dd it to an SD card!\nFirst step, download the 64-bit ubuntu server distro for the RPi3:\nhttp://cdimage.ubuntu.com/ubuntu/releases/bionic/release/ubuntu-18.04.2-preinstalled-server-arm64+raspi3.img.xz\nThen make sure you follow the instructions from these posts which help us build the kernel and update the boot firmware. The steps from these posts are summarized below:\nBuild the toolchain mkdir -p toolchains/aarch64 cd toolchains/aarch64 export TOOLCHAIN=`pwd` # Used later to reference the toolchain location cd \u0026#34;$TOOLCHAIN\u0026#34; wget https://ftp.gnu.org/gnu/binutils/binutils-2.32.tar.bz2 tar -xf binutils-2.32.tar.bz2 mkdir binutils-2.32-build cd binutils-2.32-build ../binutils-2.32/configure --prefix=\u0026#34;$TOOLCHAIN\u0026#34; --target=aarch64-linux-gnu --disable-nls make -j4 make install cd \u0026#34;$TOOLCHAIN\u0026#34; wget https://ftp.gnu.org/gnu/gcc/gcc-9.1.0/gcc-9.1.0.tar.gz tar -xf gcc-9.1.0.tar.gz mkdir gcc-9.1.0-build cd gcc-9.1.0-build ../gcc-9.1.0/configure --prefix=\u0026#34;$TOOLCHAIN\u0026#34; --target=aarch64-linux-gnu --with-newlib --without-headers --disable-nls --disable-shared --disable-threads --disable-libssp --disable-decimal-float --disable-libquadmath --disable-libvtv --disable-libgomp --disable-libatomic --enable-languages=c make all-gcc -j4 make install-gcc Build the Raspberry Pi kernel apt-get install bison flex git clone https://github.com/raspberrypi/linux.git rpi-linux cd rpi-linux git checkout origin/rpi-4.19.y # change the branch name for newer versions mkdir kernel-build PATH=$PATH:$TOOLCHAIN/bin make O=./kernel-build/ ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- bcm2711_defconfig PATH=$PATH:$TOOLCHAIN/bin make -j4 O=./kernel-build/ ARCH=arm64 CROSS_COMPILE=aarch64-linux-gnu- export KERNEL_VERSION=`cat ./kernel-build/include/generated/utsrelease.h | sed -e \u0026#39;s/.*\u0026#34;\\(.*\\)\u0026#34;.*/\\1/\u0026#39;` make -j4 O=./kernel-build/ DEPMOD=echo MODLIB=./kernel-install/lib/modules/${KERNEL_VERSION} INSTALL_FW_PATH=./kernel-install/lib/firmware modules_install git clone https://github.com/raspberrypi/tools.git rpi-tools cd rpi-tools/armstubs git checkout 7f4a937e1bacbc111a22552169bc890b4bb26a94 PATH=$PATH:$TOOLCHAIN/bin make armstub8-gic.bin echo \u0026#34;armstub=armstub8-gic.bin\u0026#34; \u0026gt;\u0026gt; config-extra.txt echo \u0026#34;enable_gic=1\u0026#34; \u0026gt;\u0026gt; config-extra.txt echo \u0026#34;arm_64bit=1\u0026#34; \u0026gt;\u0026gt; config-extra.txt echo \u0026#34;total_mem=1024\u0026#34; \u0026gt;\u0026gt; config-extra.txt We now have all the necessary files to create the boot partition and boot into the ubuntu-preinstalled image. Specifically:\nKernel: rpi-linux/kernel-build/arch/arm64/boot/Image\nBootstub: rpi-tools/armstubs/armstub8-gic.bin\nModules: rpi-linux/kernel-build/kernel-install/lib/modules/${KERNEL_VERSION}\nFirmware: https://github.com/RPi-Distro/firmware-nonfree\nSo, first thing to do after we have finished building the above is to de-compress and loop mount the ubuntu-preinstalled image we downloaded:\nxzcat ubuntu-18.04.2-preinstalled-server-arm64+raspi3.img.xz \u0026gt; ubuntu-18.04.2-preinstalled-server-arm64+raspi4.img kpartx -av ubuntu-18.04.2-preinstalled-server-arm64+raspi4.img You should end up with 2 device files:\n/dev/mapper/loop0p1 /dev/mapper/loop0p2 Mount them under /mnt like this:\nmount /dev/mapper/loop0p2 /mnt mount /dev/mapper/loop0p1 /mnt/boot/firmware Then copy in the kernel/stub, modules and firmware:\ncp rpi-linux/kernel-build/arch/arm64/boot/Image /mnt/boot/firmware/kernel8.img cp rpi-tools/armstubs/armstub8-gic.bin /mnt/boot/firmware/armstub8-gic.bin cp -avf rpi-linux/kernel-build/kernel-install/lib/modules/${KERNEL_VERSION} /mnt/lib/modules/ git clone https://github.com/RPi-Distro/firmware-nonfree firmware-nonfree cp -avf firmware-nonfree/* /mnt/lib/firmware Append config-extra.txt to config.txt:\ncat config-extra.txt \u0026gt;\u0026gt; /mnt/boot/firmware/config.txt and we’re done! Unmount / detach the loop device, dd it to an sdcard, plug it into a RPi4 and party!\numount /mnt/boot/firmware umount /mnt kpartx -dv ubuntu-18.04.2-preinstalled-server-arm64+raspi4.img losetup -d /dev/loop0 dd if=ubuntu-18.04.2-preinstalled-server-arm64+raspi4.img of=/dev/sdXX If you’re too lazy to do the above, feel free to grab our image built using the above steps:\nhttps://cloudkernels.net/ubuntu-18.04.2-preinstalled-server-arm64+raspi4+kvm.img.xz\n(sha1sum: 0b1d8b72ea5410fb7928925fd76dd0218b4f7a94)\nUPDATE\nLinks to the previous images were removed, so here’s the new ones: http://cdimage.ubuntu.com/ubuntu/releases/bionic/release/ubuntu-18.04.3-preinstalled-server-arm64+raspi3.img.xz https://cloudkernels.net/ubuntu-18.04.3-preinstalled-server-arm64+raspi4+kvm.img.xz\n","date":1563055767,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563055767,"objectID":"fa5150d6de60ba93136a965c6cb55d26","permalink":"/blog/rpi4-64bit-image/","publishdate":"2019-07-14T00:09:27+02:00","relpermalink":"/blog/rpi4-64bit-image/","section":"blog","summary":"Given the traction our previous post got, we thought we should jot down the steps to build a 64-bit bootable image for a RPi4. The distro we’re most familiar with is Debian, so we’ll go with a debian-like distro like Ubuntu. If you don’t feel like playing with kernel compilation and FS images, just grab the binary and dd it to an SD card!\n","tags":["Raspberry Pi","64-bit","Kernel"],"title":"Build a 64bit bootable image for a Raspberry Pi 4","type":"blog"},{"authors":null,"categories":null,"content":"Lightweight virtualization is a natural fit for low power devices and, so, seeing that the extremely popular Raspberry Pi line got an upgrade, we were very keen on trying the newly released Raspberry Pi 4 model B.\nGetting the board up and running with a 64bit kernel (and a 64bit userland) proved to be kind of a challenge, given that currently there is a number of limitations (SD card not fully working for \u0026gt; 1GB RAM, coherent memory allocations etc.). With the help of some very useful posts we were able to successfully boot a 64bit kernel with KVM support. Please note we also had to enable KVM \u0026amp; VIRTIO options in the kernel config to support QEMU/KVM \u0026amp; solo5-hvt instances.\nFor now, we can safely boot it with 1GB of RAM and play with our various configurations.\nWhat’s interesting about the RPI4, as far as virtualization is concerned, is that it now comes with ARM’s standard GIC (Generic Interrupt Controller). Thus, unlike previous RPI generations that were wired with custom interrupt handling, virtualization is supported natively without any patching needed for interrupt emulation (and the overhead that this can incur…).\nTo get a first taste of the performance of different virtualization options we ran a simple set of benchmarks for common use-case scenarios: a simple key-value store (REDIS), and a popular web server (NGINX). We used the stock tool for redis (redis-benchmark) and the apache-benchmark (ab), both running on the host OS.\nThe figures below show measurements from the following configurations: native (bare-metal execution on the linux host), nabla (or solo5-spt, runnc with SPT backend), solo5-hvt (runnc with HVT backend), and QEMU/KVM (generic linux guest, running Debian Buster with vhost enabled).\nFirst we ran the full redis-benchmark test on a RPi3 \u0026amp; a RPi4, for the various configurations above. Results for the linux guest are shown below:\nClearly, apart from the A72 vs A53 upgrade, the GIC addition boosted the guests’ performance significantly.\nTo dig into the virtualization internals we tried running a rumprun unikernel on top of solo5-hvt \u0026amp; solo5-spt (nabla). To facilitate deployment we used a modified version of runnc that includes a solo5-hvt backend and the ability to “mount” host fs files/directories in the container.\nThe performance is much better than in the Linux guest. To further investigate the overhead of running virtualized workloads on such SoCs, we examined the SET case. The figure below shows results from native, nabla, solo5-hvt and qemu/KVM cases.\nNabla containers exhibit the lowest overhead of all cases (~20%): this is expected as there is no virtualization involved, just seccomp filtering. The source of the overhead is the minimal I/O interface that solo5-spt offers. Solo5-hvt exhibits significant overhead (~40%), mainly due to the unoptimized I/O interface. In addition to that, trapping into user-space to handle I/O requests could possible be the main source of overhead. Qemu/KVM runs at 41% of the boards capabilities, albeit the use of the vhost mechanism which removes unnecessary traps to user-space for I/O handling.\nFinally, we plot the apache-benchmark results we got from an NGINX web server, running on a linux guest and a solo5-hvt unikernel.\nWe had fun playing with a RPi4 and its systems support! Clearly there is a need to study the systems stack and optimize the various missing pieces to be able to reduce or even eliminate the overheads for simple, lightweight workload execution.\nEvaluating virtualization options in the diverse low power SoC ecosystem is a non-trivial task; the popularity and the evolvement of such systems makes them very interesting to explore, especially when it comes to identifying the bottlenecks and providing optimizations that help the community design and implement energy-efficient, low-footprint, and high-performance solutions.\nIf you would like to try it out on your RPi 4, check out our custom-made RPi4 image (sha1sum: 1b96a6be5256182eaceb5894fb993c8ffce8c2a2), based on ubuntu 18.04.2, with KVM, docker \u0026amp; nabla container runtimes pre-installed!\nUPDATE\nActually, links to the older builds were removed, so here’s the updated links for the stock 18.04.3 image, and our KVM-enabled ubuntu 18.04.3 image.\nAlso, many people have reached out to us about the username and password for our custom image. The issue is that we assumed people would have a TTL2USB cable to play with, so the username is root, without any password. That (of course) doesn’t work if you try to SSH to the RPi.\n","date":1562762504,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562762504,"objectID":"8dd3edfb9aad825c7a67d41966493f36","permalink":"/blog/rpi4-64bit-virt/","publishdate":"2019-07-10T14:41:44+02:00","relpermalink":"/blog/rpi4-64bit-virt/","section":"blog","summary":"Lightweight virtualization is a natural fit for low power devices and, so, seeing that the extremely popular Raspberry Pi line got an upgrade, we were very keen on trying the newly released Raspberry Pi 4 model B.\n","tags":["Raspberry Pi","QEMU","Kernel","nabla","unikernel","Solo5"],"title":"Playing with a Raspberry Pi 4 64-bit","type":"blog"},{"authors":null,"categories":null,"content":"In our previous posts, we saw how to build the toolchain for a Nabla container, and also how we can use this toolchain to run applications as unikernels using Nabla.\nIn this post, we will be focusing on the steps we need to take into running something actually useful using Nabla. More specifically, we will go through all the steps for building Python3 into a Rumprun unikernel, suitable for running in a Nabla container, and cooking a filesystem that includes a Python script that we wish to run within.\nWe will be using the rumprun-packages git repository, which contains a collection of frameworks and applications that we can build on top of the Rumprun infrastructure. We have started doing some work on updating rumprun-packages, so that we can build and bake applications using the recent updates done by the Nabla people for Solo5 support in Rumprun and more specifically the spt and hvt Solo5 tenders. This is work in progress and we will be porting more packages from rumprun-packages to work on top of the upstream toolchain, both for x86 and aarch64.\nBuilding Python3.5 as a unikernel Once we have built the rumprun toolchain we can build and bake Python3.5 in a Rumprun unikernel following these steps:\ngit clone https://github.com/cloudkernels/rumprun-packages.git cd rumprun-packages # Setting up the rumprun-packages build environment cp config.mk.dist config.mk # If we are building for aarch64 we should also run: echo \u0026#34;RUMPRUN_TOOLCHAIN_TUPLE=aarch64-rumprun-netbsd\u0026#34; \u0026gt;\u0026gt; config.mk cd python3 # Build for the spt target make python.spt # Build for the hvt target make python.hvt Packing our Python script We still need to be able to pack our python script so that we can run it within the unikernel, i.e. the equivalent of doing python my_script.py?\nRemember, in the world of unikernels we do not have access to a terminal, our application is our Linux box / VM / container.\nWe have two problems to solve:\nMake our script available within the unikernel Prepare our environment with all the package dependencies our script needs, in order to execute. We will solve these issues by packing our script along with all its dependencies inside a disk image which we will later provide to the unikernel at run time.\nHere’s how we do this:\n# We \u0026#39;re sill under rumprun-packages/python3. # this will be were we will install the python environment and our script mkdir -p python/lib # Our previous step has fetched all the basic Python environment # under: ./build/pythondist/lib/python3.5 cp -r build/pythondist/lib/python3.5 python/lib/ # We add the script to Python\u0026#39;s site-packages cp myscript.py python/lib/python3.5/site-packages/ # And we prepare our packages dependencies pyvenv-3.5 newpackage-env source newpackage-env/bin/activate pip install a_python_package deactivate cp -r newpackage-env/lib/python3.5/site-packages/* python/lib/python3.5/site-packages/ # Now we have everything we need under \u0026#39;python\u0026#39;, so we create the disk image genisoimage -l -r -o disk.iso python That’s it! disk.iso contains all the necessary environment to run our script.\nsolo5-spt --disk=disk.iso --net=tap0 python.spt \\ \u0026#39;{\u0026#34;cmdline\u0026#34;:\u0026#34;python.spt -m myscript\u0026#34;,\u0026#34;env\u0026#34;:\u0026#34;PYTHONHOME=/python\u0026#34;,\u0026#34;net\u0026#34;:{\u0026#34;if\u0026#34;:\u0026#34;ukvmif0\u0026#34;,\u0026#34;cloner\u0026#34;:\u0026#34;True\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;inet\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;10.0.0.2\u0026#34;,\u0026#34;mask\u0026#34;:\u0026#34;16\u0026#34;},\u0026#34;blk\u0026#34;:{\u0026#34;source\u0026#34;:\u0026#34;etfs\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/dev/ld0a\u0026#34;,\u0026#34;fstype\u0026#34;:\u0026#34;blk\u0026#34;,\u0026#34;mountpoint\u0026#34;:\u0026#34;/python\u0026#34;}} We have created a Docker image in order to automate the previous procedure of building Python as a unikernel and preparing the disk iso with our script and its dependencies, so that instead of running the following steps you can simply do something like:\ndocker run --rm -v $(pwd):/build cloudkernels/python3-build disk.iso myscript.py requirements.txt where requirements.txt includes the dependencies of myscript.py in the form of one package per line (this is, essentially, whatever running pip freeze on your python project directory would produce).\nYou can find the Docker image on Docker hub and on github.\nA working example is found below. Please note that this version includes a hack to hardcode the dns server in the dummy rootfs as we haven’t yet patched the configuration logic of rumprun to include a command line option for dns.\nWe will use a simple requests example. The files needed are the python snippet and requirements.txt.\nrequests_main.py:\nimport requests r = requests.get(\u0026#39;https://www.example.com\u0026#39;) print(r.status_code) print(r.text) requirements.txt:\nrequests Now run the command to bake the necessary python dependencies:\n# docker run --rm -v $(pwd):/build cloudkernels/python3-build:x86_64_dns disk.iso requests_main.py requirements.txt [...] 7.12% done, estimate finish Sat Feb 23 18:56:49 2019 14.25% done, estimate finish Sat Feb 23 18:56:49 2019 21.35% done, estimate finish Sat Feb 23 18:56:49 2019 28.48% done, estimate finish Sat Feb 23 18:56:49 2019 35.59% done, estimate finish Sat Feb 23 18:56:49 2019 42.70% done, estimate finish Sat Feb 23 18:56:49 2019 49.81% done, estimate finish Sat Feb 23 …","date":1550938671,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550938671,"objectID":"de644cceca290267eb3f290dd5685c8b","permalink":"/blog/building-python-snippets-for-nabla/","publishdate":"2019-02-23T18:17:51+02:00","relpermalink":"/blog/building-python-snippets-for-nabla/","section":"blog","summary":"In our previous posts, we saw how to build the toolchain for a Nabla container, and also how we can use this toolchain to run applications as unikernels using Nabla.\n","tags":["unikernels","nabla","containers","rumprun","python","docker"],"title":"How to build a python snippet for running in a Nabla container","type":"blog"},{"authors":null,"categories":null,"content":"In this post, we go through the basic steps for containerizing a unikernel application and running it on nabla runnc. Checkout nabla containers and in particular runnc.\nHow to In order to build a docker image for nabla containers, we have to build:\nthe nabla toolstack the unikernel image the docker image Build the nabla toolstack There’s an informative blog post on how to build the nabla rumprun toolstack here. Now that our aarch64 changes are upstream, the process should be as easy as the following:\ngit clone https://github.com/nabla-containers/rumrpun cd rumprun git submodule update --init make . obj/config-PATH.sh and you should end up with the toolstack (${ARCH}-rumprun-netbsd-) in your PATH variable.\nAlternatively you can use one of the (unofficial) docker images with the toolstack embedded at /usr/local:\nx86_64-rumprun-netbsd- aarch64-rumprun-netbsd- You can run these containers using the following command:\ndocker run --runtime=runc --rm -v ${PWD}:/build -it cloudkernels/debian-rumprun-build:${ARCH} /bin/bash where ${ARCH} could be x86_64 or aarch64.\nBuild the unikernel Image building the image is as easy as running the following:\n${ARCH}-rumprun-netbsd-gcc myprog.c -o myprog-rumprun rumprun-bake solo5-spt myprog.spt myprog-rumprun So we have ourselves an .spt file (a unikernel). To try it out, assuming you have a solo5-spt binary lying around you can do the following:\nsolo5-spt ./myprog.spt Assuming the unikernel image is successful, its time to construct the docker image.\nBuild the docker image Rumprun is a quirky unikernel framework, with a number of assumptions we can’t ignore. Thus, in order to get the unikernel running correctly when in a nabla container environment, we need to be careful and include the correct stubs for a dummy root filesystem. So, clone this repo, which contains the basic rootfs from rumprun’s lib/librumprunfs_base/rootfs, and a template Dockerfile shown below:\nFROM scratch COPY myprog.spt /myprog.nabla COPY rootfs/etc /etc ENTRYPOINT [ \u0026#34;myprog.nabla\u0026#34; ] Copy the spt file in this directory and run:\ndocker build -f Dockerfile -t myprog-nabla:${ARCH} . You should end up with a local docker image named myprog-nabla, tagged with your current architecture variant (x86_64 or aarch64).\nAssuming you have setup runnc correctly, spawning the container is as easy as:\ndocker run --rm --runtime=runnc myprog-nabla:${ARCH} ","date":1550925700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550925700,"objectID":"9c6639a440bca34fef467c0bb9136795","permalink":"/blog/build-a-nabla-docker-image/","publishdate":"2019-02-23T14:41:40+02:00","relpermalink":"/blog/build-a-nabla-docker-image/","section":"blog","summary":"In this post, we go through the basic steps for containerizing a unikernel application and running it on nabla runnc. Checkout nabla containers and in particular runnc.\n","tags":["Unikernels","Nabla","Containers","Rumprun","Docker"],"title":"Build a Nabla Docker Image","type":"blog"},{"authors":null,"categories":null,"content":"[UPDATE: Revise instructions and links to use latest upstream nabla repo with merged aarch64 support.] In previous posts, we covered a bit of background on rumprun, the nabla containers fork and our port on aarch64. In this post, we describe how to build everything from source. In order to build a rumprun unikernel for aarch64, the first step is to build the rumprun toolchain.\nClone the relevant repositories:\ngit clone https://github.com/nabla-containers/rumprun git clone https://github.com/cloudkernels/rumprun-packages cd rumprun git submodule update --init Build rumprun with:\ncd rumprun make The build is tested with gcc-5 but it should work with gcc versions up to 6. If you need to explicitly set the gcc version use:\ncd rumprun CC=\u0026lt;gcc-version\u0026gt; make Solo5 is now built by default as part of the nabla-containers code distribution.\nBuild (\u0026amp; bake) hello (both hvt and spt versions):\ncd rumprun-packages cd hello make Output is the following:\nmkdir -p build cp src/* build CC=aarch64-rumprun-netbsd-gcc make -C build hello.spt make[1]: Entering directory \u0026#39;/build/rumprun-packages/hello/build\u0026#39; aarch64-rumprun-netbsd-gcc hello.c -o hello-rumprun rumprun-bake solo5_spt hello.spt hello-rumprun !!! !!! NOTE: rumprun-bake is experimental. syntax may change in the future !!! make[1]: Leaving directory \u0026#39;/build/rumprun-packages/hello/build\u0026#39; mkdir -p bin cp build/hello.spt bin/hello.spt CC=aarch64-rumprun-netbsd-gcc make -C build hello.hvt make[1]: Entering directory \u0026#39;/build/rumprun-packages/hello/build\u0026#39; rumprun-bake solo5_hvt hello.hvt hello-rumprun !!! !!! NOTE: rumprun-bake is experimental. syntax may change in the future !!! make[1]: Leaving directory \u0026#39;/build/rumprun-packages/hello/build\u0026#39; mkdir -p bin cp build/hello.hvt bin/hello.hvt Try to run both (an existing dummy file and tap0 device are expected):\n../../rumprun/solo5/tenders/hvt/solo5-hvt --disk=dummy --net=tap0 ./bin/hello.hvt Output:\nsolo5-hvt: bin/hello.hvt: Warning: phdr[0] requests WRITE and EXEC permissions solo5-hvt: WARNING: Tender is configured with HVT_DROP_PRIVILEGES=0. Not dropping any privileges. solo5-hvt: WARNING: This is not recommended for production use. | ___| __| _ \\ | _ \\ __ \\ \\__ \\ ( | | ( | ) | ____/\\___/ _|\\___/____/ Solo5: Memory map: 512 MB addressable: Solo5: reserved @ (0x0 - 0xfffff) Solo5: text @ (0x100000 - 0x30efff) Solo5: rodata @ (0x30f000 - 0x35bfff) Solo5: data @ (0x35c000 - 0x3cafff) Solo5: heap \u0026gt;= 0x3cb000 \u0026lt; stack \u0026lt; 0x20000000 rump kernel bare metal bootstrap [ 1.0000000] Copyright (c) 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, [ 1.0000000] 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, [ 1.0000000] 2018 The NetBSD Foundation, Inc. All rights reserved. [ 1.0000000] Copyright (c) 1982, 1986, 1989, 1991, 1993 [ 1.0000000] The Regents of the University of California. All rights reserved. [ 1.0000000] NetBSD 8.99.25 (RUMP-ROAST) [ 1.0000000] total memory = 253 MB [ 1.0000000] timecounter: Timecounters tick every 10.000 msec [ 1.0000080] timecounter: Timecounter \u0026#34;clockinterrupt\u0026#34; frequency 100 Hz quality 0 [ 1.0000090] cpu0 at thinair0: rump virtual cpu [ 1.0000090] root file system type: rumpfs [ 1.0000090] kern.module.path=/stand/evbarm/8.99.25/modules [ 1.0200090] mainbus0 (root) [ 1.0200090] timecounter: Timecounter \u0026#34;bmktc\u0026#34; frequency 1000000000 Hz quality 100 rumprun: could not find start of json. no config? mounted tmpfs on /tmp === calling \u0026#34;rumprun\u0026#34; main() === This is the Rumprun Hello World ... ... using the Solo5 framework ... ... in a Nabla container via runnc! === main() of \u0026#34;rumprun\u0026#34; returned 0 === === _exit(0) called === [ 3.0281712] rump kernel halting... [ 3.0281712] syncing disks... done [ 3.0281712] unmounting file systems... [ 3.0281712] unmounted tmpfs on /tmp type tmpfs [ 3.0281712] unmounted rumpfs on / type rumpfs [ 3.0281712] unmounting done halted Solo5: solo5_exit(0) called and the spt one:\n../../rumprun/solo5/tenders/spt/solo5-spt --disk=dummy --net=tap0 ./bin/hello.spt solo5-spt: bin/hello.spt: Warning: phdr[0] requests WRITE and EXEC permissions | ___| __| _ \\ | _ \\ __ \\ \\__ \\ ( | | ( | ) | ____/\\___/ _|\\___/____/ Solo5: Memory map: 512 MB addressable: Solo5: reserved @ (0x0 - 0xfffff) Solo5: text @ (0x100000 - 0x30bfff) Solo5: rodata @ (0x30c000 - 0x357fff) Solo5: data @ (0x358000 - 0x3c6fff) Solo5: heap \u0026gt;= 0x3c7000 \u0026lt; stack \u0026lt; 0x20000000 rump kernel bare metal bootstrap [ 1.0000000] Copyright (c) 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, [ 1.0000000] 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, [ 1.0000000] 2018 The NetBSD Foundation, Inc. All rights reserved. [ 1.0000000] Copyright (c) 1982, 1986, 1989, 1991, 1993 [ 1.0000000] The Regents of the University of California. All rights reserved. [ 1.0000000] NetBSD 8.99.25 (RUMP-ROAST) [ 1.0000000] total memory = 253 MB [ 1.0000000] timecounter: Timecounters tick every 10.000 msec [ 1.0000080] timecounter: Timecounter \u0026#34;clockinterrupt\u0026#34; frequency 100 Hz quality 0 [ 1.0000090] …","date":1548339471,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548339471,"objectID":"a68f07955c600ebcdba841a57bcd1aa4","permalink":"/blog/building-nabla-aarch64/","publishdate":"2019-01-24T16:17:51+02:00","relpermalink":"/blog/building-nabla-aarch64/","section":"blog","summary":"[UPDATE: Revise instructions and links to use latest upstream nabla repo with merged aarch64 support.] ","tags":["Unikernels","Nabla","Containers","Rumprun","`aarch64`"],"title":"Building the Nabla containers toolchain for `aarch64`","type":"blog"},{"authors":null,"categories":null,"content":"[UPDATE: Revise instructions to reflect upstream nabla changes.] In this post, we will walk through the steps of compiling, baking, and running an application as a rumprun unikernel on a Raspberry Pi 3.\nIn our previous post, we provided some background for Rumprun/rump kernels and Solo5. In short, Rumprun provides the necessary components to run a POSIX compatible application as a unikernel. Solo5 is, essentially, a hardware abstraction layer that provides a very thin interface, or else a minimal attack surface.\nThe Nabla Containers fork of Rumprun provides a solo5 target for the rump kernel unikernel stack. Additionally, they provide a Docker runtime that spawns unikernel images as containers. We will be talking about Nabla Containers in more detail in future posts. Stay tuned :)\nFor this tutorial, we will use our current aarch64 port. We will show you how to build and run a unikernel application on a RPi3. Keep in mind, that our port should work without issues on any aarch64 platform. Feel free to run this tutorial on your favourite ARM board and let us know about your experience!\nHowever, enough with the boring theory, let’s get our hands dirty.\nHands-on First of all, we need the rumprun toolchain to bake our unikernel image. One option is to use our docker image with the toolchain preinstalled.\nAnother option is to build the rumprun toolchain from source. We will provide the necessary information on how to build all components for aarch64 in the coming days.\nSo, log in to your RPi3 and, after you’ve installed docker-ce, try running the following command:\ndocker run -ti --rm -v /build:/build cloudkernels/debian-rumprun-build:aarch64 /bin/bash you should be presented with something like the following:\nUnable to find image \u0026#39;cloudkernels/debian-rumprun-build:aarch64\u0026#39; locally aarch64: Pulling from cloudkernels/debian-rumprun-build e10919c546c2: Pull complete 6b3f0a4d7b10: Pull complete 473e207e8cf0: Pull complete 0deecc1ceca2: Pull complete 628025a81431: Pull complete 25fd95c63d4f: Pull complete Digest: sha256:0221ba1c3a120bde1fd83b6d9c267fb4379c33d8f0012b9a64826afd476faf72 Status: Downloaded newer image for cloudkernels/debian-rumprun-build:aarch64 root@184fa9ecd15d:/# now move to the build directory, and clone the rumprun-packages repo:\nroot@184fa9ecd15d:/# cd build root@184fa9ecd15d:/build# git clone https://github.com/cloudkernels/rumprun-packages Make sure your config.mk file contains the correct toolchain tuple:\nroot@184fa9ecd15d:/build# cd rumprun-packages root@184fa9ecd15d:/build/rumprun-packages# echo RUMPRUN_TOOLCHAIN_TUPLE=aarch64-rumprun-netbsd \u0026gt; config.mk and go to an example package, say hello, and type make\nroot@184fa9ecd15d:/build/rumprun-packages# cd hello root@184fa9ecd15d:/build/rumprun-packages# make The output should be the following:\n# make mkdir -p build cp src/* build make -C build hello.spt make[1]: Entering directory \u0026#39;/build/rumprun-packages/hello/build\u0026#39; aarch64-rumprun-netbsd-gcc hello.c -o hello-rumprun rumprun-bake solo5_spt hello.spt hello-rumprun !!! !!! NOTE: rumprun-bake is experimental. syntax may change in the future !!! make[1]: Leaving directory \u0026#39;/build/rumprun-packages/hello/build\u0026#39; mkdir -p bin cp build/hello.spt bin/hello.spt rumprun-bake solo5_hvt hello.hvt hello-rumprun !!! !!! NOTE: rumprun-bake is experimental. syntax may change in the future !!! make[1]: Leaving directory \u0026#39;/build/rumprun-packages/hello/build\u0026#39; mkdir -p bin cp build/hello.hvt bin/hello.hvt Now, exit the container (Ctrl-D) and cd into this directory:\nroot@rpi3:~# cd /build/rumprun-packages/hello root@rpi3:/build/rumprun-packages/hello# Make sure there’s a dummy file for the disk image, and a tap interface:\nroot@rpi3:/build/rumprun-packages/hello# dd if=/dev/zero of=dummy count=1 bs=512 root@rpi3:/build/rumprun-packages/hello# ip tuntap add tap0 mode tap root@rpi3:/build/rumprun-packages/hello# ip link set dev tap0 up And make sure you’ve got the solo5-hvt/spt binaries (solo5). If not, do the following:\nroot@rpi3:/build# git clone https://github.com/solo5/solo5 root@rpi3:/build# cd solo5 root@rpi3:/build/solo5# apt-get install libseccomp-dev \u0026amp;\u0026amp; make If everything was successful, you should have two binaries: solo5-spt (seccomp tender) and solo5-hvt (KVM tender) at:\n/build/solo5/tenders/spt/solo5-spt /build/solo5/tenders/hvt/solo5-hvt So, returning to the previous dir / environment, you can execute the hello unikernel:\nroot@rpi3:/build/rumprun-packages/hello# /build/solo5/tenders/spt/solo5-spt --mem=32 --net=tap0 --disk=dummy ./bin/hello.spt | ___| __| _ \\ | _ \\ __ \\ \\__ \\ ( | | ( | ) | ____/\\___/ _|\\___/____/ Solo5: Memory map: 32 MB addressable: Solo5: unused @ (0x0 - 0xfffff) Solo5: text @ (0x100000 - 0x30dfff) Solo5: rodata @ (0x30e000 - 0x356fff) Solo5: data @ (0x357000 - 0x3c3fff) Solo5: heap \u0026gt;= 0x3c4000 \u0026lt; stack \u0026lt; 0x2000000 rump kernel bare metal bootstrap [ 1.0000000] Copyright (c) 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, [ 1.0000000] 2006, 2007, 2008, 2009, 2010, 2011, 2012, …","date":1548281367,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548281367,"objectID":"509e30e9a5a8bf32058c7c2b0d175567","permalink":"/blog/example-rumprun-solo5-on-aarch64/","publishdate":"2019-01-24T00:09:27+02:00","relpermalink":"/blog/example-rumprun-solo5-on-aarch64/","section":"blog","summary":"[UPDATE: Revise instructions to reflect upstream nabla changes.] ","tags":["Unikernels","Nabla","Containers","Rumprun","Raspberry Pi","Docker","Solo5","`aarch64`"],"title":"Run a rumprun unikernel on a `RPi3`","type":"blog"},{"authors":null,"categories":null,"content":"[UPDATE: Rumprun aarch64 support has now been merged in upstream nabla.] Nabla containers provide a new type of container designed for strong isolation on a host system. The foundation of nabla containers lies in three main components: rumpkernel, solo5, and runnc. The team that built nabla containers extended the rumprun unikernel framework to support solo5 (instead of hardware/baremetal or xen), so that a rumprun-baked unikernel application can be executed on top of a lightweight monitor such as solo5. In this post, we describe the steps we took in order to port Nabla containers to the ARMv8 architecture.\nA bit of background Rumprun is a unikernel framework based on rumpkernel, a project that provides kernel-quality drivers for various components, e.g. file systems, network devices, POSIX system calls. Rump kernel exposes these drivers through the rump kernel hypercall interface to higher abstraction layers. While initially, rump kernels were designed to provide ‘userspace’ drivers, they evolved to become the base of the Rumprun unikernel.\nAlmost any POSIX-based application can be built into a Rumprun unikernel using the following workflow:\nCompile and link the application against the POSIX-y interface that Rumprun exposes. Bake the application, in order to add the bits and pieces needed to turn it into an image that is bootable on the targets that Rumprun runs on top of. Upstream Rumprun defines the concept of target, i.e. the platforms on which a Rumprun unikernel can run on top of. At the moment the upstream Rumprun provides two targets:\nThe hw target provides support on raw hardware which includes most available hypervisors. The Xen target is optimized for execution on top of the Xen hypervisor The Nabla fork of Rumprun provides a new unikernel target, Solo5. Solo5 is, essentially, a hardware abstraction layer that provides a very thin interface, or else a minimal attach surface. Its purpose is to facilitate the port of libOS/unikernel frameworks on various hardware platforms, i.e. a unikernel that runs on top of Solo5, runs on top of all the hardware targets, or tenders, using the Solo5 terminology, supported by Solo5.\nARMv8 rumprun solo5 To port nabla containers to the ARMv8 architecture, one has to provide support for each of these components: rumprun, solo5 and runnc. We decided to tackle this challenge by separately porting each component and working on integrating as much code as possible from upstream repositories.\nSolo5 For the solo5 port, most of the code was already in upstream solo5, although the seccomp tender (as it is now called) provided by nabla didn’t have support for aarch64 targets. To implement that, apart from adding the compilation target itself, we also had to provide the arch specific lds and add the hypercall-to-syscall mechanism used to aarch64. Finally, for everything to actually work we implemented reading the cpu timer frequency correctly for aarch64 and provided some missing seccomp definitions. As of the time of writing this post, an upstream solo5-seccomp tender for both x86 and aarch64 is under way and should be merged in really soon.\nRumprun For rumprun, things were a bit more complicated. Both upstream and nabla rumprun repos build necessary NetBSD parts from an old snapshot of the official sources. NetBSD has added aarch64 support fairly recently and with much work still being done we decided to base our build on the latest official sources instead of the stripped down version provided by rumprun. This presented a challenge: integrating the changes to a not actively maintained code base is not a walk in the park. First things first, we had to successfully compile the whole project: Rump parts of the NetBSD kernel are not actively tested with rumprun and so changes being made to related components are not always guaranteed to work. After adding the aarch64 platform to the required Makefiles for rump in NetBSD source and creating the relevant directories, we encountered linking errors pertaining to both double symbols between the provided libc and rump and also incorrectly linked for the rump case internal functions of the kernel itself. Thankfully similar problems have already been solved for the arm 32-bit architecture and so we could implement a solution based on existing code.\nHaving successfully compiled NetBSD’s source we then had to implement any missing rumpuser parts either for aarch64 or for functions introduced in upstream kernel’s rump.\nThe most challenging part was to understand how thread-level switching happens on aarch64 and implement that in the context of rumprun. Upon initial creation of “main” threads for the rump components, rumprun stores a “bouncer” function on top of their freshly allocated stacks and then switches between them using its scheduler. When a thread’s turn comes to run, the “bouncer” function is popped from the stack and the actual thread content is executed.\nThe mechanism doing the actual thread switching is implemented in …","date":1548247304,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548247304,"objectID":"046d1fda6015536378a4faa74f1c10c8","permalink":"/blog/nabla-containers-aarch64/","publishdate":"2019-01-23T14:41:44+02:00","relpermalink":"/blog/nabla-containers-aarch64/","section":"blog","summary":"[UPDATE: Rumprun aarch64 support has now been merged in upstream nabla.] ","tags":["Unikernel","Nabla","Containers","ARMv8","Solo5"],"title":"Experiences from porting nabla containers to an ARMv8 board","type":"blog"},{"authors":["Orestis Lagkas Nikolos","Konstantinos Papazafeiropoulos","Stratos Psomadakis","Anastassios Nanos","Nectarios Koziris"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c9faabd8b426fb73472ce502fbbe3a8d","permalink":"/publication/nikolos-2019-extending/","publishdate":"2025-10-15T23:59:22.749207Z","relpermalink":"/publication/nikolos-2019-extending/","section":"publication","summary":"","tags":null,"title":"Extending storage support for unikernel containers","type":"publication"},{"authors":["Dimiter R Avresky","Erik Maehle","Nectarios Koziris","Anastassios Nanos"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"e776165b55552649c4d45d7fdaa607e4","permalink":"/publication/avresky-2015-dpdns/","publishdate":"2025-10-15T23:59:22.742942Z","relpermalink":"/publication/avresky-2015-dpdns/","section":"publication","summary":"","tags":null,"title":"DPDNS Introduction and Committees","type":"publication"},{"authors":["Ioannis Mytilinis","Dimitrios Tsoumakos","Verena Kantere","Anastassios Nanos","Nectarios Koziris"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"40285d7afc09053109dff305f0950516","permalink":"/publication/mytilinis-2015-performance/","publishdate":"2025-10-15T23:59:22.73072Z","relpermalink":"/publication/mytilinis-2015-performance/","section":"publication","summary":"","tags":null,"title":"I/O performance modeling for big data applications over cloud infrastructures","type":"publication"},{"authors":["Anastassios Nanos","Stefanos Gerangelos","Ioanna Alifieraki","Nectarios Koziris"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"782b5aaf384ac1ecbb141eaa7485a551","permalink":"/publication/nanos-2015-v-4-vsockets/","publishdate":"2025-10-15T23:59:22.736822Z","relpermalink":"/publication/nanos-2015-v-4-vsockets/","section":"publication","summary":"","tags":null,"title":"V4vsockets: Low-overhead intra-node communication in xen","type":"publication"},{"authors":["Anastassios Nanos","Nectarios Koziris"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"a9940a8767d7d9fef4320c22532cf79d","permalink":"/publication/nanos-2014-xen-2-mx/","publishdate":"2025-10-15T23:59:22.724588Z","relpermalink":"/publication/nanos-2014-xen-2-mx/","section":"publication","summary":"","tags":null,"title":"Xen2MX: High-performance communication in virtualized environments","type":"publication"},{"authors":["Michael Alexander","Gianluigi Zanetti","Anastassios Nanos"],"categories":null,"content":"","date":1325376e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376e3,"objectID":"76b7bc5315117745a0b286cc518dfe57","permalink":"/publication/alexander-20127-th/","publishdate":"2025-10-15T23:59:22.718516Z","relpermalink":"/publication/alexander-20127-th/","section":"publication","summary":"","tags":null,"title":"7th Workshop on Virtualization in High-Performance Cloud Computing--VHPC2012","type":"publication"},{"authors":["Anastassios Nanos","Nectarios Koziris"],"categories":null,"content":"","date":1325376e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376e3,"objectID":"4555d613c951d108a772391664d20b09","permalink":"/publication/nanos-2012-xen-2-mx/","publishdate":"2025-10-15T23:59:22.712505Z","relpermalink":"/publication/nanos-2012-xen-2-mx/","section":"publication","summary":"","tags":null,"title":"Xen2MX: towards high-performance communication in the cloud","type":"publication"},{"authors":["Anastassios Nanos","Nikos Nikoleris","Stratos Psomadakis","Elisavet Kozyri","Nectarios Koziris"],"categories":null,"content":"","date":129384e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":129384e4,"objectID":"3e221c1b73bf4ef1a6422f4b885b4aca","permalink":"/publication/nanos-2011-smart/","publishdate":"2025-10-15T23:59:22.687537Z","relpermalink":"/publication/nanos-2011-smart/","section":"publication","summary":"","tags":null,"title":"A Smart HPC interconnect for clusters of Virtual Machines","type":"publication"},{"authors":["Dimitris Aragiorgis","Anastassios Nanos","Nectarios Koziris"],"categories":null,"content":"","date":129384e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":129384e4,"objectID":"d764c3248a0168f403af270ec8e6a12f","permalink":"/publication/aragiorgis-2011-coexisting/","publishdate":"2025-10-15T23:59:22.706428Z","relpermalink":"/publication/aragiorgis-2011-coexisting/","section":"publication","summary":"","tags":null,"title":"Coexisting scheduling policies boosting I/O virtual machines","type":"publication"},{"authors":["Anastassios Nanos","Georgios Goumas","Nectarios Koziris"],"categories":null,"content":"","date":1262304e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304e3,"objectID":"0bf2a44abf50201c728d4e79cef09d7b","permalink":"/publication/nanos-2010-exploring/","publishdate":"2025-10-15T23:59:22.681241Z","relpermalink":"/publication/nanos-2010-exploring/","section":"publication","summary":"","tags":null,"title":"Exploring I/O virtualization data paths for MPI applications in a cluster of VMs: a networking perspective","type":"publication"},{"authors":["Evangelos Koukis","Anastassios Nanos","Nectarios Koziris"],"categories":null,"content":"","date":1262304e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304e3,"objectID":"fdea9110b03518f3ae220d37d49f8c3a","permalink":"/publication/koukis-2010-gmblock/","publishdate":"2025-10-15T23:59:22.69385Z","relpermalink":"/publication/koukis-2010-gmblock/","section":"publication","summary":"","tags":null,"title":"GMBlock: Optimizing data movement in a block-level storage sharing system over myrinet","type":"publication"},{"authors":["Anastassios Nanos","Nectarios Koziris"],"categories":null,"content":"","date":1230768e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768e3,"objectID":"3535d72eda23eacd4d1f8c23c48d6a37","permalink":"/publication/nanos-2009-myrixen/","publishdate":"2025-10-15T23:59:22.671799Z","relpermalink":"/publication/nanos-2009-myrixen/","section":"publication","summary":"","tags":null,"title":"MyriXen: message passing in Xen virtual machines over Myrinet and Ethernet","type":"publication"},{"authors":["Evangelos Koukis","Anastassios Nanos","Nectarios Koziris"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"03a3fe695b6f37d0a543cc7ad3daba5c","permalink":"/publication/koukis-2008-synchronized/","publishdate":"2025-10-15T23:59:22.70009Z","relpermalink":"/publication/koukis-2008-synchronized/","section":"publication","summary":"","tags":null,"title":"Synchronized send operations for efficient streaming block I/O over Myrinet","type":"publication"}]